"""
AIæ™ºèƒ½ä½“ç³»ç»Ÿé…ç½®ç®¡ç†
"""

import os
import json
from pathlib import Path
from typing import Dict, Any, Optional
from enum import Enum

class AgentMode(Enum):
    """æ™ºèƒ½ä½“è¿è¡Œæ¨¡å¼"""
    AI_DRIVEN = "ai_driven"

class AIAgentConfig:
    """AIæ™ºèƒ½ä½“é…ç½®ç®¡ç†å™¨"""
    
    def __init__(self):
        # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œé…ç½®æ–‡ä»¶åœ¨åŒä¸€ç›®å½•ä¸‹
        self.config_file = Path(__file__).parent / "ai_agent_config.json"
        self.default_config = {
            "agent_mode": AgentMode.AI_DRIVEN.value,
            "ai_model_settings": {
                "enable_code_quality_ai": True,
                "enable_security_ai": True,
                "enable_performance_ai": True,
                "enable_static_scan_ai": True,
                "enable_user_communication_ai": True,
                "model_timeout_seconds": 30,
                "max_code_length": 10000,
                "ai_confidence_threshold": 0.7
            },
            "user_communication_settings": {
                "hybrid_mode": True,
                "ai_intent_analysis": True,
                "ai_response_generation": True,
                "natural_language_understanding": True,
                "context_memory": True,
                "fallback_to_traditional": True,
                "ai_confidence_threshold": 0.8
            },
            "performance_settings": {
                "parallel_ai_processing": True,
                "cache_ai_results": True,
                "max_concurrent_ai_tasks": 3
            },
            "prompt_engineering": {
                "use_dynamic_prompts": True,
                "context_aware_prompts": True,
                "multi_language_prompts": True,
                "custom_prompt_templates": {
                    "user_intent_analysis": "åˆ†æç”¨æˆ·æ„å›¾å¹¶æå–å…³é”®ä¿¡æ¯: {user_message}",
                    "requirement_extraction": "ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–ä»£ç åˆ†æéœ€æ±‚: {user_message}",
                    "response_generation": "ç”Ÿæˆä¸“ä¸šå‹å¥½çš„å›åº”: {context}"
                }
            }
        }
        self.config = self._load_config()
    
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            if self.config_file.exists():
                with open(self.config_file, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                    # åˆå¹¶é»˜è®¤é…ç½®ä»¥ç¡®ä¿æ‰€æœ‰é”®éƒ½å­˜åœ¨
                    return self._merge_configs(self.default_config, config)
            else:
                # åˆ›å»ºé»˜è®¤é…ç½®æ–‡ä»¶
                self._save_config(self.default_config)
                return self.default_config
        except Exception as e:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨é»˜è®¤é…ç½®: {e}")
            return self.default_config
    
    def _merge_configs(self, default: Dict[str, Any], user: Dict[str, Any]) -> Dict[str, Any]:
        """åˆå¹¶é…ç½®ï¼Œç”¨æˆ·é…ç½®ä¼˜å…ˆ"""
        merged = default.copy()
        for key, value in user.items():
            if isinstance(value, dict) and key in merged:
                merged[key] = self._merge_configs(merged[key], value)
            else:
                merged[key] = value
        return merged
    
    def _save_config(self, config: Dict[str, Any]):
        """ä¿å­˜é…ç½®æ–‡ä»¶"""
        try:
            self.config_file.parent.mkdir(parents=True, exist_ok=True)
            with open(self.config_file, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
        except Exception as e:
            print(f"âš ï¸ é…ç½®æ–‡ä»¶ä¿å­˜å¤±è´¥: {e}")
    
    def get_agent_mode(self) -> AgentMode:
        """è·å–å½“å‰æ™ºèƒ½ä½“è¿è¡Œæ¨¡å¼"""
        mode_str = self.config.get("agent_mode", AgentMode.AI_DRIVEN.value)
        try:
            return AgentMode(mode_str)
        except ValueError:
            return AgentMode.AI_DRIVEN
    
    def set_agent_mode(self, mode: AgentMode):
        """è®¾ç½®æ™ºèƒ½ä½“è¿è¡Œæ¨¡å¼"""
        self.config["agent_mode"] = mode.value
        self._save_config(self.config)
    
    def is_ai_enabled_for_agent(self, agent_type: str) -> bool:
        """æ£€æŸ¥æŒ‡å®šæ™ºèƒ½ä½“æ˜¯å¦å¯ç”¨AIåŠŸèƒ½"""
        ai_settings = self.config.get("ai_model_settings", {})
        setting_key = f"enable_{agent_type}_ai"
        return ai_settings.get(setting_key, True)
    
    def get_ai_model_timeout(self) -> int:
        """è·å–AIæ¨¡å‹è¶…æ—¶æ—¶é—´"""
        return self.config.get("ai_model_settings", {}).get("model_timeout_seconds", 30)
    
    def get_max_code_length(self) -> int:
        """è·å–æœ€å¤§ä»£ç é•¿åº¦é™åˆ¶"""
        return self.config.get("ai_model_settings", {}).get("max_code_length", 10000)
    
    def get_ai_confidence_threshold(self) -> float:
        """è·å–AIç½®ä¿¡åº¦é˜ˆå€¼"""
        return self.config.get("ai_model_settings", {}).get("ai_confidence_threshold", 0.7)
    
    def get_max_concurrent_ai_tasks(self) -> int:
        """è·å–æœ€å¤§å¹¶å‘AIä»»åŠ¡æ•°"""
        return self.config.get("performance_settings", {}).get("max_concurrent_ai_tasks", 3)
    
    def is_parallel_processing_enabled(self) -> bool:
        """æ˜¯å¦å¯ç”¨å¹¶è¡ŒAIå¤„ç†"""
        return self.config.get("performance_settings", {}).get("parallel_ai_processing", True)
    
    def is_ai_caching_enabled(self) -> bool:
        """æ˜¯å¦å¯ç”¨AIç»“æœç¼“å­˜"""
        return self.config.get("performance_settings", {}).get("cache_ai_results", True)
    
    def get_custom_prompt_template(self, agent_type: str) -> Optional[str]:
        """è·å–è‡ªå®šä¹‰promptæ¨¡æ¿"""
        templates = self.config.get("prompt_engineering", {}).get("custom_prompt_templates", {})
        return templates.get(agent_type)
    
    def set_custom_prompt_template(self, agent_type: str, template: str):
        """è®¾ç½®è‡ªå®šä¹‰promptæ¨¡æ¿"""
        if "prompt_engineering" not in self.config:
            self.config["prompt_engineering"] = {}
        if "custom_prompt_templates" not in self.config["prompt_engineering"]:
            self.config["prompt_engineering"]["custom_prompt_templates"] = {}
        
        self.config["prompt_engineering"]["custom_prompt_templates"][agent_type] = template
        self._save_config(self.config)
    
    def get_agent_selection_strategy(self) -> Dict[str, str]:
        """è·å–æ™ºèƒ½ä½“é€‰æ‹©ç­–ç•¥"""
        # å§‹ç»ˆä½¿ç”¨AIé©±åŠ¨æ¨¡å¼
        return {
            "code_quality": "ai_code_quality",
            "security": "ai_security", 
            "performance": "ai_performance",
            "static_scan": "static_scan"
        }
    
    def update_config(self, updates: Dict[str, Any]):
        """æ›´æ–°é…ç½®"""
        self.config = self._merge_configs(self.config, updates)
        self._save_config(self.config)
    
    def reset_to_defaults(self):
        """é‡ç½®ä¸ºé»˜è®¤é…ç½®"""
        self.config = self.default_config.copy()
        self._save_config(self.config)
    
    def validate_config(self) -> bool:
        """éªŒè¯é…ç½®æœ‰æ•ˆæ€§"""
        try:
            # æ£€æŸ¥å¿…è¦çš„é…ç½®é¡¹
            required_keys = ["agent_mode", "ai_model_settings"]
            for key in required_keys:
                if key not in self.config:
                    return False
            
            # éªŒè¯agent_mode
            mode_str = self.config["agent_mode"]
            AgentMode(mode_str)  # å¦‚æœæ— æ•ˆä¼šæŠ›å‡ºå¼‚å¸¸
            
            return True
        except Exception:
            return False
    
    def should_use_traditional_fallback(self) -> bool:
        """æ˜¯å¦åº”è¯¥ä½¿ç”¨ä¼ ç»Ÿæ¨¡å¼ä½œä¸ºé™çº§å¤‡ç”¨"""
        # AIé©±åŠ¨æ¨¡å¼ä¸‹ï¼Œå§‹ç»ˆå°è¯•AIä¼˜å…ˆï¼Œä½†å¯ä»¥é™çº§åˆ°ä¼ ç»Ÿæ¨¡å¼
        return True
    
    def get_config_summary(self) -> Dict[str, Any]:
        """è·å–é…ç½®æ‘˜è¦"""
        return {
            "agent_mode": self.get_agent_mode().value,
            "ai_enabled_agents": {
                agent_type: self.is_ai_enabled_for_agent(agent_type)
                for agent_type in ["code_quality", "security", "performance", "static_scan"]
            },
            "performance_settings": {
                "parallel_processing": self.is_parallel_processing_enabled(),
                "max_concurrent_tasks": self.get_max_concurrent_ai_tasks(),
                "caching_enabled": self.is_ai_caching_enabled()
            },
            "fallback_enabled": self.should_use_traditional_fallback(),
            "config_valid": self.validate_config()
        }
    
    def is_user_communication_ai_enabled(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨ç”¨æˆ·äº¤æµAI"""
        return self.config.get("ai_model_settings", {}).get("enable_user_communication_ai", True)
    
    def is_hybrid_communication_mode(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨æ··åˆäº¤æµæ¨¡å¼"""
        return self.config.get("user_communication_settings", {}).get("hybrid_mode", True)
    
    def get_user_communication_ai_confidence_threshold(self) -> float:
        """è·å–ç”¨æˆ·äº¤æµAIç½®ä¿¡åº¦é˜ˆå€¼"""
        return self.config.get("user_communication_settings", {}).get("ai_confidence_threshold", 0.8)
    
    def is_natural_language_understanding_enabled(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨è‡ªç„¶è¯­è¨€ç†è§£"""
        return self.config.get("user_communication_settings", {}).get("natural_language_understanding", True)
    
    def is_context_memory_enabled(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ç”¨ä¸Šä¸‹æ–‡è®°å¿†"""
        return self.config.get("user_communication_settings", {}).get("context_memory", True)
    
    def should_fallback_to_traditional_communication(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦åº”è¯¥é™çº§åˆ°ä¼ ç»Ÿäº¤æµæ¨¡å¼"""
        return self.config.get("user_communication_settings", {}).get("fallback_to_traditional", True)
    
    def get_user_communication_prompt_templates(self) -> Dict[str, str]:
        """è·å–ç”¨æˆ·äº¤æµç›¸å…³çš„æç¤ºè¯æ¨¡æ¿"""
        all_templates = self.config.get("prompt_engineering", {}).get("custom_prompt_templates", {})
        return {
            "user_intent_analysis": all_templates.get("user_intent_analysis", "åˆ†æç”¨æˆ·æ„å›¾å¹¶æå–å…³é”®ä¿¡æ¯: {user_message}"),
            "requirement_extraction": all_templates.get("requirement_extraction", "ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–ä»£ç åˆ†æéœ€æ±‚: {user_message}"),
            "response_generation": all_templates.get("response_generation", "ç”Ÿæˆä¸“ä¸šå‹å¥½çš„å›åº”: {context}")
        }
        
    def set_user_communication_ai_enabled(self, enabled: bool):
        """è®¾ç½®ç”¨æˆ·äº¤æµAIå¯ç”¨çŠ¶æ€"""
        if "ai_model_settings" not in self.config:
            self.config["ai_model_settings"] = {}
        self.config["ai_model_settings"]["enable_user_communication_ai"] = enabled
        self._save_config(self.config)
    
    def set_hybrid_communication_mode(self, enabled: bool):
        """è®¾ç½®æ··åˆäº¤æµæ¨¡å¼"""
        if "user_communication_settings" not in self.config:
            self.config["user_communication_settings"] = {}
        self.config["user_communication_settings"]["hybrid_mode"] = enabled
        self._save_config(self.config)
    
    # === æ–°å¢ï¼šå„æ™ºèƒ½ä½“ä¸“å±é…ç½®è®¿é—®æ–¹æ³• ===
    
    def get_code_quality_agent_config(self) -> Dict[str, Any]:
        """è·å–ä»£ç è´¨é‡æ™ºèƒ½ä½“é…ç½®"""
        return self.config.get("code_quality_agent", {})
    
    def get_security_agent_config(self) -> Dict[str, Any]:
        """è·å–å®‰å…¨åˆ†ææ™ºèƒ½ä½“é…ç½®"""
        return self.config.get("security_agent", {})
    
    def get_performance_agent_config(self) -> Dict[str, Any]:
        """è·å–æ€§èƒ½åˆ†ææ™ºèƒ½ä½“é…ç½®"""
        return self.config.get("performance_agent", {})
    
    def get_user_communication_agent_config(self) -> Dict[str, Any]:
        """è·å–ç”¨æˆ·äº¤äº’æ™ºèƒ½ä½“é…ç½®"""
        return self.config.get("user_communication_agent", {})
    
    def get_static_scan_agent_config(self) -> Dict[str, Any]:
        """è·å–é™æ€æ‰«ææ™ºèƒ½ä½“é…ç½®"""
        return self.config.get("static_scan_agent", {})
    
    def get_readability_agent_config(self) -> Dict[str, Any]:
        """è·å–å¯è¯»æ€§å¢å¼ºæ™ºèƒ½ä½“é…ç½®"""
        return self.config.get("readability_enhancement_agent", {})
    
    def get_model_cache_dir(self) -> str:
        """è·å–æ¨¡å‹ç¼“å­˜ç›®å½•"""
        return self.config.get("model_configuration", {}).get("cache_dir", "./model_cache/")

# å…¨å±€é…ç½®å®ä¾‹
_ai_agent_config = None

def get_ai_agent_config() -> AIAgentConfig:
    """è·å–AIæ™ºèƒ½ä½“é…ç½®å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰"""
    global _ai_agent_config
    if _ai_agent_config is None:
        _ai_agent_config = AIAgentConfig()
    return _ai_agent_config

def print_config_status():
    """æ‰“å°å½“å‰é…ç½®çŠ¶æ€"""
    config = get_ai_agent_config()
    summary = config.get_config_summary()
    
    print("\nğŸ¤– AIæ™ºèƒ½ä½“ç³»ç»Ÿé…ç½®çŠ¶æ€:")
    print(f"è¿è¡Œæ¨¡å¼: {summary['agent_mode']} (AIé©±åŠ¨)")
    print(f"é…ç½®æœ‰æ•ˆ: {'âœ…' if summary['config_valid'] else 'âŒ'}")
    
    print("\nAIåŠŸèƒ½å¯ç”¨çŠ¶æ€:")
    for agent, enabled in summary['ai_enabled_agents'].items():
        status = "âœ…" if enabled else "âŒ"
        print(f"  {agent}: {status}")
    
    print(f"\næ€§èƒ½è®¾ç½®:")
    print(f"  å¹¶è¡Œå¤„ç†: {'âœ…' if summary['performance_settings']['parallel_processing'] else 'âŒ'}")
    print(f"  æœ€å¤§å¹¶å‘ä»»åŠ¡: {summary['performance_settings']['max_concurrent_tasks']}")
    print(f"  ç»“æœç¼“å­˜: {'âœ…' if summary['performance_settings']['caching_enabled'] else 'âŒ'}")
