#!/usr/bin/env python3
"""
Test Qwen1.5-7B-Chat as Qwen2-7B replacement
"""

import sys
import os
from transformers import AutoModel, AutoTokenizer
import torch

sys.path.insert(0, os.path.join(os.path.dirname(__file__), '../..'))

def test_qwen15_replacement():
    """Test Qwen1.5-7B-Chat functionality"""
    
    model_id = "Qwen/Qwen1.5-7B-Chat"
    print(f"ğŸ§ª Testing {model_id} as Qwen2 replacement")
    
    try:
        # Load tokenizer
        print("ğŸ“ Loading tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)
        
        # Test basic tokenization
        test_text = "ä½ å¥½ï¼Œè¯·å¸®æˆ‘åˆ†æä»£ç è´¨é‡"
        tokens = tokenizer(test_text, return_tensors="pt")
        print(f"âœ… Tokenization successful: {len(tokens['input_ids'][0])} tokens")
        
        # Load model (if GPU available)
        if torch.cuda.is_available():
            print("ğŸš€ Loading model on GPU...")
            model = AutoModel.from_pretrained(
                model_id, 
                trust_remote_code=True,
                torch_dtype=torch.float16,
                device_map="auto"
            )
        else:
            print("ğŸ’» Loading model on CPU...")
            model = AutoModel.from_pretrained(model_id, trust_remote_code=True)
        
        print("âœ… Model loaded successfully")
        
        # Test basic inference
        print("ğŸ”¬ Testing basic inference...")
        with torch.no_grad():
            outputs = model(**tokens)
            print(f"âœ… Inference successful: {outputs.last_hidden_state.shape}")
        
        # Test chat functionality
        print("ğŸ’¬ Testing chat functionality...")
        messages = [
            {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä»£ç åˆ†æåŠ©æ‰‹"},
            {"role": "user", "content": "è¯·åˆ†æè¿™æ®µä»£ç çš„è´¨é‡"}
        ]
        
        if hasattr(tokenizer, 'apply_chat_template'):
            chat_input = tokenizer.apply_chat_template(
                messages, 
                tokenize=False, 
                add_generation_prompt=True
            )
            print("âœ… Chat template applied successfully")
        else:
            print("âš ï¸  Chat template not available, using direct text")
        
        return {
            "model_id": model_id,
            "status": "success",
            "features": {
                "tokenization": True,
                "model_loading": True,
                "inference": True,
                "chat_template": hasattr(tokenizer, 'apply_chat_template')
            },
            "device": "cuda" if torch.cuda.is_available() else "cpu",
            "dtype": str(model.dtype) if 'model' in locals() else "unknown"
        }
        
    except Exception as e:
        print(f"âŒ Error: {e}")
        return {
            "model_id": model_id,
            "status": "failed",
            "error": str(e)
        }

if __name__ == "__main__":
    result = test_qwen15_replacement()
    
    # Save results
    import json
    os.makedirs("tests/model_compatibility/results", exist_ok=True)
    with open("tests/model_compatibility/results/qwen15_test.json", 'w') as f:
        json.dump(result, f, indent=2, ensure_ascii=False)
    
    if result["status"] == "success":
        print("\nğŸ‰ Qwen1.5-7B-Chat ready as Qwen2 replacement!")
    else:
        print("\nğŸ’¥ Qwen1.5-7B-Chat test failed")
