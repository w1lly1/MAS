#!/usr/bin/env python3
"""
ChatGLMç®€å•æµ‹è¯•
é€æ­¥è¯Šæ–­é—®é¢˜
"""

import sys
import traceback

print("ğŸš€ å¼€å§‹ChatGLMç®€å•æµ‹è¯•")

try:
    print("ğŸ“¦ å¯¼å…¥åŸºç¡€åº“...")
    import torch
    print(f"âœ… PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    from transformers import AutoTokenizer, AutoModel, AutoConfig
    print("âœ… Transformerså¯¼å…¥æˆåŠŸ")
    
    print("ğŸ“ æµ‹è¯•åŸºæœ¬é…ç½®åŠ è½½...")
    config = AutoConfig.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
    print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ: {type(config)}")
    print(f"ğŸ“‹ æ¨¡å‹ç±»å‹: {config.model_type}")
    print(f"ğŸ“‹ å±‚æ•°: {config.num_layers}")
    
    print("ğŸ“ æµ‹è¯•tokenizeråŠ è½½...")
    tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
    print(f"âœ… TokenizeråŠ è½½æˆåŠŸ: {type(tokenizer)}")
    
    print("ğŸ§ª æµ‹è¯•åŸºæœ¬ç¼–ç ...")
    test_text = "ä½ å¥½"
    encoded = tokenizer.encode(test_text)
    print(f"âœ… ç¼–ç æˆåŠŸ: {encoded}")
    
    decoded = tokenizer.decode(encoded)
    print(f"âœ… è§£ç æˆåŠŸ: {decoded}")
    
    print("âœ… åŸºç¡€æµ‹è¯•å®Œæˆ")
    
except Exception as e:
    print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
    print(f"ğŸ› è¯¦ç»†é”™è¯¯:")
    traceback.print_exc()
    sys.exit(1)

print("ğŸ‰ æ‰€æœ‰åŸºç¡€æµ‹è¯•é€šè¿‡")
