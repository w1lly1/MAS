#!/usr/bin/env python3
"""
ChatGLMå…¼å®¹æ€§æ¢ç´¢æµ‹è¯•
æ¢ç´¢æ­£ç¡®çš„ChatGLMåŠ è½½å’Œä¿®å¤æ–¹å¼
"""

import os
import sys
import traceback
from typing import Optional, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    from transformers import AutoTokenizer, AutoModel, AutoConfig
    import torch
except ImportError as e:
    print(f"âŒ å¯¼å…¥åº“å¤±è´¥: {e}")
    sys.exit(1)


def explore_chatglm_loading():
    """æ¢ç´¢ChatGLMçš„æ­£ç¡®åŠ è½½æ–¹å¼"""
    print("ğŸ” æ¢ç´¢ChatGLMåŠ è½½æ–¹å¼...")
    
    try:
        # 1. å…ˆåŠ è½½tokenizerï¼Œè¿™æ ·å¯ä»¥è®¿é—®è‡ªå®šä¹‰ç±»
        print("ğŸ“ åŠ è½½tokenizer...")
        tokenizer = AutoTokenizer.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
        print(f"âœ… Tokenizerç±»å‹: {type(tokenizer)}")
        
        # 2. è·å–tokenizerçš„å®é™…ç±»
        tokenizer_class = type(tokenizer)
        print(f"ğŸ“ Tokenizerç±»: {tokenizer_class}")
        
        # 3. æ£€æŸ¥tokenizerçš„æ–¹æ³•
        print("ğŸ“ Tokenizeræ–¹æ³•:")
        for method in dir(tokenizer):
            if method.startswith('_pad'):
                print(f"  - {method}")
        
        # 4. æµ‹è¯•åŸºæœ¬åŠŸèƒ½
        print("ğŸ§ª æµ‹è¯•åŸºæœ¬ç¼–ç ...")
        test_text = "ä½ å¥½"
        encoded = tokenizer.encode(test_text)
        print(f"âœ… ç¼–ç æˆåŠŸ: {encoded}")
        
        decoded = tokenizer.decode(encoded)
        print(f"âœ… è§£ç æˆåŠŸ: {decoded}")
        
        return tokenizer, tokenizer_class
        
    except Exception as e:
        print(f"âŒ æ¢ç´¢å¤±è´¥: {e}")
        print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None, None


def apply_dynamic_fixes(tokenizer_class):
    """åŠ¨æ€åº”ç”¨ä¿®å¤"""
    try:
        print("ğŸ”§ åº”ç”¨åŠ¨æ€ä¿®å¤...")
        
        # ä¿®å¤tokenizerçš„_padæ–¹æ³•
        if hasattr(tokenizer_class, '_pad'):
            original_pad = tokenizer_class._pad
            
            def fixed_pad(self, encoded_inputs, max_length=None, padding_strategy=None, pad_to_multiple_of=None, return_attention_mask=None):
                """ä¿®å¤çš„_padæ–¹æ³•"""
                # ç§»é™¤padding_sideå±æ€§å¦‚æœå­˜åœ¨
                if hasattr(self, 'padding_side'):
                    delattr(self, 'padding_side')
                
                return original_pad(self, encoded_inputs, max_length, padding_strategy, pad_to_multiple_of, return_attention_mask)
            
            tokenizer_class._pad = fixed_pad
            print("âœ… Tokenizer _padæ–¹æ³•å·²ä¿®å¤")
        
        # ä¿®å¤configçš„å±æ€§è®¿é—®
        from transformers.configuration_utils import PretrainedConfig
        
        # è·å–ChatGLMConfigç±»
        config = AutoConfig.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
        config_class = type(config)
        
        if hasattr(config_class, '__getattribute__'):
            original_getattribute = config_class.__getattribute__
            
            def fixed_getattribute(self, name):
                """ä¿®å¤çš„__getattribute__æ–¹æ³•"""
                # å±æ€§æ˜ å°„
                if name == 'num_hidden_layers' and hasattr(self, 'num_layers'):
                    return self.num_layers
                elif name == 'intermediate_size' and hasattr(self, 'ffn_hidden_size'):
                    return self.ffn_hidden_size
                
                return original_getattribute(self, name)
            
            config_class.__getattribute__ = fixed_getattribute
            print("âœ… Config __getattribute__æ–¹æ³•å·²ä¿®å¤")
        
        return True
        
    except Exception as e:
        print(f"âŒ åŠ¨æ€ä¿®å¤å¤±è´¥: {e}")
        return False


def test_model_loading():
    """æµ‹è¯•æ¨¡å‹åŠ è½½"""
    try:
        print("ğŸ“¦ æµ‹è¯•æ¨¡å‹åŠ è½½...")
        
        # åŠ è½½é…ç½®
        config = AutoConfig.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
        print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•å±æ€§è®¿é—®
        print(f"ğŸ“‹ num_layers: {config.num_layers}")
        print(f"ğŸ“‹ hidden_size: {config.hidden_size}")
        
        # æµ‹è¯•ä¿®å¤åçš„å±æ€§
        try:
            print(f"ğŸ“‹ num_hidden_layers: {config.num_hidden_layers}")
        except Exception as e:
            print(f"âŒ num_hidden_layersè®¿é—®å¤±è´¥: {e}")
        
        # åŠ è½½æ¨¡å‹
        model = AutoModel.from_pretrained(
            "THUDM/chatglm2-6b",
            trust_remote_code=True,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto" if torch.cuda.is_available() else None,
            low_cpu_mem_usage=True,
        )
        model.eval()
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        return model
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None


def test_chat_functionality(tokenizer, model):
    """æµ‹è¯•èŠå¤©åŠŸèƒ½"""
    try:
        print("ğŸ§ª æµ‹è¯•èŠå¤©åŠŸèƒ½...")
        
        # ç®€å•å¯¹è¯æµ‹è¯•
        message = "ä½ å¥½"
        print(f"ğŸ“ è¾“å…¥: {message}")
        
        response, history = model.chat(
            tokenizer,
            message,
            history=[],
            max_length=100,
            temperature=0.8,
        )
        
        print(f"âœ… å›åº”: {response}")
        return True
        
    except Exception as e:
        print(f"âŒ èŠå¤©æµ‹è¯•å¤±è´¥: {e}")
        print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ChatGLMå…¼å®¹æ€§æ¢ç´¢æµ‹è¯•")
    print("=" * 50)
    
    # 1. æ¢ç´¢åŠ è½½æ–¹å¼
    tokenizer, tokenizer_class = explore_chatglm_loading()
    if not tokenizer:
        return False
    
    # 2. åº”ç”¨åŠ¨æ€ä¿®å¤
    if not apply_dynamic_fixes(tokenizer_class):
        return False
    
    # 3. æµ‹è¯•æ¨¡å‹åŠ è½½
    model = test_model_loading()
    if not model:
        return False
    
    # 4. æµ‹è¯•èŠå¤©åŠŸèƒ½
    if test_chat_functionality(tokenizer, model):
        print("\nâœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼ŒChatGLMåŠŸèƒ½æ­£å¸¸")
        return True
    else:
        print("\nâŒ èŠå¤©åŠŸèƒ½æµ‹è¯•å¤±è´¥")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
