#!/usr/bin/env python3
"""
ChatGLM2-6B æ¨¡å‹å…¼å®¹æ€§æµ‹è¯•
ä¸“é—¨è§£å†³ padding_side å‚æ•°å…¼å®¹æ€§é—®é¢˜
"""

import os
import sys
import asyncio
import traceback
from typing import Optional

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

def test_basic_imports():
    """æµ‹è¯•åŸºç¡€æ¨¡å—å¯¼å…¥"""
    print("ğŸ”§ æµ‹è¯•åŸºç¡€æ¨¡å—å¯¼å…¥...")
    try:
        import torch
        import transformers
        import sentencepiece
        print(f"âœ… PyTorch: {torch.__version__}")
        print(f"âœ… Transformers: {transformers.__version__}")
        print(f"âœ… SentencePiece: {sentencepiece.__version__}")
        return True
    except ImportError as e:
        print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
        return False

def test_chatglm_tokenizer_direct():
    """ç›´æ¥æµ‹è¯•ChatGLM tokenizer"""
    print("ğŸ§ª ç›´æ¥æµ‹è¯•ChatGLM tokenizer...")
    try:
        from transformers import AutoTokenizer
        
        model_name = "THUDM/chatglm2-6b"
        print(f"ğŸ“¦ åŠ è½½tokenizer: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            trust_remote_code=True
        )
        print("âœ… TokenizeråŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•åŸºæœ¬ç¼–ç åŠŸèƒ½
        test_text = "ä½ å¥½ï¼Œæˆ‘æ˜¯ChatGLMåŠ©æ‰‹"
        tokens = tokenizer.encode(test_text, add_special_tokens=True)
        decoded = tokenizer.decode(tokens)
        
        print(f"ğŸ“ åŸæ–‡: {test_text}")
        print(f"ğŸ”¢ ç¼–ç : {tokens[:10]}...")  # åªæ˜¾ç¤ºå‰10ä¸ªtoken
        print(f"ğŸ“„ è§£ç : {decoded}")
        
        return tokenizer
        
    except Exception as e:
        print(f"âŒ Tokenizeræµ‹è¯•å¤±è´¥: {e}")
        print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None

def test_chatglm_model_direct():
    """ç›´æ¥æµ‹è¯•ChatGLMæ¨¡å‹åŠ è½½"""
    print("ğŸ§ª ç›´æ¥æµ‹è¯•ChatGLMæ¨¡å‹åŠ è½½...")
    try:
        from transformers import AutoModel, AutoTokenizer
        
        model_name = "THUDM/chatglm2-6b"
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_name}")
        
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            trust_remote_code=True
        )
        
        model = AutoModel.from_pretrained(
            model_name,
            trust_remote_code=True,
            device_map="auto"
        )
        
        print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æµ‹è¯•ç›´æ¥å¯¹è¯
        test_input = "ä½ å¥½"
        print(f"ğŸ“ æµ‹è¯•è¾“å…¥: {test_input}")
        
        # ä½¿ç”¨æ¨¡å‹çš„chatæ–¹æ³•
        response, history = model.chat(tokenizer, test_input, history=[])
        print(f"ğŸ‰ æ¨¡å‹å›åº”: {response}")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None, None

def test_alternative_pipeline():
    """æµ‹è¯•æ›¿ä»£çš„pipelineé…ç½®"""
    print("ğŸ§ª æµ‹è¯•æ›¿ä»£pipelineé…ç½®...")
    try:
        from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
        
        model_name = "THUDM/chatglm2-6b"
        print(f"ğŸ“¦ ä½¿ç”¨æ›¿ä»£æ–¹å¼åŠ è½½: {model_name}")
        
        # æ–¹æ³•1: ä¸ä½¿ç”¨pipelineï¼Œç›´æ¥ä½¿ç”¨æ¨¡å‹
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            trust_remote_code=True
        )
        
        model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            device_map="auto"
        )
        
        print("âœ… ç›´æ¥æ¨¡å‹åŠ è½½æˆåŠŸ")
        
        # æ‰‹åŠ¨ç”Ÿæˆ
        test_prompt = "ç”¨æˆ·: ä½ å¥½\nåŠ©æ‰‹:"
        inputs = tokenizer.encode(test_prompt, return_tensors="pt")
        
        print(f"ğŸ“ è¾“å…¥prompt: {test_prompt}")
        print("ğŸ¤– å¼€å§‹ç”Ÿæˆ...")
        
        with torch.no_grad():
            outputs = model.generate(
                inputs,
                max_length=inputs.shape[1] + 50,
                do_sample=True,
                temperature=0.8,
                pad_token_id=tokenizer.eos_token_id
            )
        
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        # æå–å›åº”éƒ¨åˆ†
        if test_prompt in response:
            ai_response = response.replace(test_prompt, "").strip()
        else:
            ai_response = response.strip()
            
        print(f"ğŸ‰ ç”ŸæˆæˆåŠŸ: {ai_response}")
        
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ æ›¿ä»£æ–¹æ³•å¤±è´¥: {e}")
        print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None, None

class ChatGLMWrapper:
    """ChatGLMåŒ…è£…å™¨ - é¿å…pipelineå…¼å®¹æ€§é—®é¢˜"""
    
    def __init__(self, model_name: str = "THUDM/chatglm2-6b"):
        self.model_name = model_name
        self.model = None
        self.tokenizer = None
        self.initialized = False
    
    def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            print(f"ğŸ”§ åˆå§‹åŒ–ChatGLMåŒ…è£…å™¨: {self.model_name}")
            
            from transformers import AutoModel, AutoTokenizer
            import torch
            
            # åŠ è½½tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name, 
                trust_remote_code=True
            )
            print("âœ… TokenizeråŠ è½½æˆåŠŸ")
            
            # åŠ è½½æ¨¡å‹
            self.model = AutoModel.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                device_map="auto"
            )
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            self.initialized = True
            return True
            
        except Exception as e:
            print(f"âŒ åŒ…è£…å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
            return False
    
    def generate_response(self, prompt: str, max_length: int = 100) -> Optional[str]:
        """ç”Ÿæˆå›åº”"""
        if not self.initialized:
            print("âŒ æ¨¡å‹æœªåˆå§‹åŒ–")
            return None
        
        try:
            print(f"ğŸ§  ç”Ÿæˆå›åº”: {prompt[:50]}...")
            
            # ä½¿ç”¨ChatGLMçš„åŸç”Ÿchatæ–¹æ³•
            response, _ = self.model.chat(
                self.tokenizer, 
                prompt, 
                history=[],
                max_length=max_length,
                temperature=0.8
            )
            
            print(f"âœ… ç”ŸæˆæˆåŠŸ: {response}")
            return response
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            return None

def test_chatglm_wrapper():
    """æµ‹è¯•ChatGLMåŒ…è£…å™¨"""
    print("ğŸ§ª æµ‹è¯•ChatGLMåŒ…è£…å™¨...")
    
    wrapper = ChatGLMWrapper()
    
    if wrapper.initialize():
        print("âœ… åŒ…è£…å™¨åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•å¯¹è¯
        test_cases = [
            "ä½ å¥½",
            "ä½ æ˜¯è°ï¼Ÿ",
            "è¯·ä»‹ç»ä¸€ä¸‹ä»£ç åˆ†æçš„é‡è¦æ€§",
            "How are you?"
        ]
        
        for i, test_input in enumerate(test_cases, 1):
            print(f"\nğŸ§ª æµ‹è¯• {i}: {test_input}")
            response = wrapper.generate_response(test_input)
            if response:
                print(f"âœ… å›åº”: {response}")
            else:
                print("âŒ ç”Ÿæˆå¤±è´¥")
        
        return wrapper
    else:
        print("âŒ åŒ…è£…å™¨åˆå§‹åŒ–å¤±è´¥")
        return None

async def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("ğŸš€ ChatGLM2-6B å…¼å®¹æ€§æµ‹è¯•å¼€å§‹")
    print("=" * 60)
    
    # 1. åŸºç¡€å¯¼å…¥æµ‹è¯•
    if not test_basic_imports():
        print("âŒ åŸºç¡€å¯¼å…¥å¤±è´¥ï¼Œç»ˆæ­¢æµ‹è¯•")
        return
    
    print("\n" + "=" * 60)
    
    # 2. Tokenizeræµ‹è¯•
    tokenizer = test_chatglm_tokenizer_direct()
    if not tokenizer:
        print("âŒ Tokenizeræµ‹è¯•å¤±è´¥")
    
    print("\n" + "=" * 60)
    
    # 3. ç›´æ¥æ¨¡å‹æµ‹è¯•
    model, tokenizer = test_chatglm_model_direct()
    if model and tokenizer:
        print("âœ… ç›´æ¥æ¨¡å‹è°ƒç”¨æˆåŠŸ")
    
    print("\n" + "=" * 60)
    
    # 4. æ›¿ä»£æ–¹æ³•æµ‹è¯•
    alt_model, alt_tokenizer = test_alternative_pipeline()
    if alt_model and alt_tokenizer:
        print("âœ… æ›¿ä»£æ–¹æ³•æˆåŠŸ")
    
    print("\n" + "=" * 60)
    
    # 5. åŒ…è£…å™¨æµ‹è¯•
    wrapper = test_chatglm_wrapper()
    if wrapper:
        print("âœ… åŒ…è£…å™¨æµ‹è¯•æˆåŠŸ")
        return wrapper
    
    print("\n" + "=" * 60)
    print("ğŸ æµ‹è¯•å®Œæˆ")
    return None

if __name__ == "__main__":
    import torch
    
    print(f"ğŸ–¥ï¸ è¿è¡Œç¯å¢ƒ:")
    print(f"   Python: {sys.version}")
    print(f"   PyTorch: {torch.__version__}")
    print(f"   CUDAå¯ç”¨: {torch.cuda.is_available()}")
    print(f"   å·¥ä½œç›®å½•: {os.getcwd()}")
    
    # è¿è¡Œæµ‹è¯•
    result = asyncio.run(run_all_tests())
    
    if result:
        print("ğŸ‰ æ‰¾åˆ°å¯ç”¨çš„ChatGLMå®ç°æ–¹æ¡ˆï¼")
    else:
        print("âŒ æ‰€æœ‰æµ‹è¯•æ–¹æ¡ˆéƒ½å¤±è´¥äº†")
