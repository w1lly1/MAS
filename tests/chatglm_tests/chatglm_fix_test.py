#!/usr/bin/env python3
"""
ChatGLM2-6B Tokenizer å…¼å®¹æ€§ä¿®å¤
é€šè¿‡Monkey Patchè§£å†³padding_sideå‚æ•°é—®é¢˜
"""

import os
import sys
import asyncio
import traceback
import torch
from typing import Optional, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

def fix_chatglm_compatibility():
    """ä¿®å¤ChatGLMçš„å…¼å®¹æ€§é—®é¢˜"""
    print("ğŸ”§ åº”ç”¨ChatGLMå…¨é¢å…¼å®¹æ€§ä¿®å¤...")
    
    try:
        from transformers import AutoTokenizer, AutoConfig
        
        model_name = "THUDM/chatglm2-6b"
        
        # 1. ä¿®å¤tokenizerå…¼å®¹æ€§
        print("ğŸ”§ ä¿®å¤tokenizerå…¼å®¹æ€§...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            trust_remote_code=True
        )
        
        # è·å–ChatGLM tokenizerç±»
        ChatGLMTokenizer = type(tokenizer)
        
        # ä¿å­˜åŸå§‹_padæ–¹æ³•
        if hasattr(ChatGLMTokenizer, '_original_pad'):
            print("âš ï¸ Tokenizerå·²ä¿®å¤ï¼Œè·³è¿‡")
        else:
            original_pad = ChatGLMTokenizer._pad
            ChatGLMTokenizer._original_pad = original_pad
            
            # åˆ›å»ºå…¼å®¹çš„_padæ–¹æ³•
            def compatible_pad(self, encoded_inputs, **kwargs):
                # ç§»é™¤ä¸å…¼å®¹çš„å‚æ•°
                filtered_kwargs = {k: v for k, v in kwargs.items() 
                                 if k not in ['padding_side']}
                return ChatGLMTokenizer._original_pad(self, encoded_inputs, **filtered_kwargs)
            
            # åº”ç”¨monkey patch
            ChatGLMTokenizer._pad = compatible_pad
            print("âœ… Tokenizerå…¼å®¹æ€§ä¿®å¤æˆåŠŸ")
        
        # 2. ä¿®å¤é…ç½®å…¼å®¹æ€§
        print("ğŸ”§ ä¿®å¤é…ç½®å…¼å®¹æ€§...")
        config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)
        ChatGLMConfig = type(config)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ä¿®å¤num_hidden_layers
        if not hasattr(config, 'num_hidden_layers') and hasattr(config, 'num_layers'):
            print("ğŸ”§ æ·»åŠ num_hidden_layerså±æ€§æ˜ å°„...")
            
            # ä¿å­˜åŸå§‹__getattribute__æ–¹æ³•
            if not hasattr(ChatGLMConfig, '_original_getattribute'):
                ChatGLMConfig._original_getattribute = ChatGLMConfig.__getattribute__
                
                def patched_getattribute(self, name):
                    if name == 'num_hidden_layers' and hasattr(self, 'num_layers'):
                        return self.num_layers
                    return ChatGLMConfig._original_getattribute(self, name)
                
                ChatGLMConfig.__getattribute__ = patched_getattribute
                print("âœ… é…ç½®å…¼å®¹æ€§ä¿®å¤æˆåŠŸ")
        
        print("âœ… ChatGLMå…¼å®¹æ€§ä¿®å¤æˆåŠŸ")
        return tokenizer, config
        
    except Exception as e:
        print(f"âŒ å…¼å®¹æ€§ä¿®å¤å¤±è´¥: {e}")
        import traceback
        print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None, None

class FixedChatGLMAgent:
    """ä¿®å¤ç‰ˆChatGLMä»£ç†"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.initialized = False
        print("ğŸ¤– åˆå§‹åŒ–ä¿®å¤ç‰ˆChatGLMä»£ç†")
    
    def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            print("ğŸ”§ å¼€å§‹åˆå§‹åŒ–ä¿®å¤ç‰ˆChatGLM...")
            
            # é¦–å…ˆåº”ç”¨å…¨é¢å…¼å®¹æ€§ä¿®å¤
            self.tokenizer, config = fix_chatglm_compatibility()
            if not self.tokenizer:
                return False
            
            # åŠ è½½æ¨¡å‹ - ä½¿ç”¨æ­£ç¡®çš„æ¨¡å‹ç±»
            from transformers import AutoModelForCausalLM
            
            model_name = "THUDM/chatglm2-6b"
            print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_name}")
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                device_map="auto"
            )
            
            # å¯¹å·²åŠ è½½çš„æ¨¡å‹å®ä¾‹åº”ç”¨ä¿®å¤
            print("ğŸ”§ å¯¹æ¨¡å‹å®ä¾‹åº”ç”¨å…¼å®¹æ€§ä¿®å¤...")
            if not hasattr(self.model, '_extract_past_from_model_output'):
                def _extract_past_from_model_output(*args, **kwargs):
                    """æå–past_key_valuesï¼Œå…¼å®¹æ–°ç‰ˆtransformersçš„å‚æ•°"""
                    # ç¬¬ä¸€ä¸ªå‚æ•°åº”è¯¥æ˜¯outputs
                    outputs = args[0] if args else None
                    
                    if outputs is None:
                        return None
                    
                    if hasattr(outputs, 'past_key_values'):
                        return outputs.past_key_values
                    elif isinstance(outputs, dict) and 'past_key_values' in outputs:
                        return outputs['past_key_values']
                    else:
                        return None
                
                # å°†æ–¹æ³•ç»‘å®šåˆ°æ¨¡å‹å®ä¾‹
                import types
                self.model._extract_past_from_model_output = types.MethodType(_extract_past_from_model_output, self.model)
                print("âœ… æ¨¡å‹å®ä¾‹å…¼å®¹æ€§ä¿®å¤æˆåŠŸ")
            
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            self.initialized = True
            return True
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False
    
    def test_basic_functions(self):
        """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
        if not self.initialized:
            print("âŒ æ¨¡å‹æœªåˆå§‹åŒ–")
            return False
        
        try:
            print("ğŸ§ª æµ‹è¯•åŸºæœ¬tokenizeråŠŸèƒ½...")
            
            # æµ‹è¯•ç¼–ç 
            test_text = "ä½ å¥½ï¼Œæˆ‘æ˜¯ChatGLMåŠ©æ‰‹"
            tokens = self.tokenizer.encode(test_text, add_special_tokens=True)
            decoded = self.tokenizer.decode(tokens)
            
            print(f"ğŸ“ åŸæ–‡: {test_text}")
            print(f"ğŸ”¢ ç¼–ç : {tokens[:10]}...")
            print(f"ğŸ“„ è§£ç : {decoded}")
            
            return True
            
        except Exception as e:
            print(f"âŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def generate_response(self, user_input: str) -> Optional[str]:
        """ç”Ÿæˆå›åº”"""
        if not self.initialized:
            print("âŒ æ¨¡å‹æœªåˆå§‹åŒ–")
            return None
        
        try:
            print(f"ğŸ§  ç”Ÿæˆå›åº”: {user_input}")
            
            # ä½¿ç”¨æ›´å®‰å…¨çš„æ–¹å¼è°ƒç”¨ChatGLM
            # ç›´æ¥ä½¿ç”¨tokenizerç¼–ç å’Œæ¨¡å‹ç”Ÿæˆï¼Œé¿å…chatæ–¹æ³•çš„ç¼“å­˜é—®é¢˜
            inputs = self.tokenizer.encode(user_input, return_tensors="pt")
            
            # ä½¿ç”¨generateæ–¹æ³•è€Œä¸æ˜¯chatæ–¹æ³•
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs,
                    max_length=inputs.shape[1] + 100,  # é™åˆ¶ç”Ÿæˆé•¿åº¦
                    temperature=0.8,
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id,
                    use_cache=False  # ç¦ç”¨ç¼“å­˜é¿å…ç¼“å­˜ç›¸å…³é”™è¯¯
                )
            
            # è§£ç å“åº”
            response = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
            
            print(f"âœ… ç”ŸæˆæˆåŠŸ: {response}")
            return response
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            
            # å¦‚æœä¸Šè¿°æ–¹æ³•å¤±è´¥ï¼Œå°è¯•ä½¿ç”¨æœ€åŸºæœ¬çš„æ–¹æ³•
            try:
                print("ğŸ”„ å°è¯•åŸºç¡€ç”Ÿæˆæ–¹æ³•...")
                
                # ç®€å•çš„æ–‡æœ¬ç”Ÿæˆï¼Œä¸ä½¿ç”¨å¯¹è¯æ ¼å¼
                input_text = f"é—®ï¼š{user_input}\nç­”ï¼š"
                inputs = self.tokenizer(input_text, return_tensors="pt")
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        **inputs,
                        max_new_tokens=50,
                        temperature=0.7,
                        do_sample=True,
                        use_cache=False,
                        pad_token_id=self.tokenizer.pad_token_id or self.tokenizer.eos_token_id
                    )
                
                response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
                # æå–ç­”æ¡ˆéƒ¨åˆ†
                if "ç­”ï¼š" in response:
                    response = response.split("ç­”ï¼š", 1)[-1].strip()
                
                print(f"âœ… åŸºç¡€æ–¹æ³•ç”ŸæˆæˆåŠŸ: {response}")
                return response
                
            except Exception as e2:
                print(f"âŒ åŸºç¡€æ–¹æ³•ä¹Ÿå¤±è´¥: {e2}")
                return None

def test_fixed_chatglm():
    """æµ‹è¯•ä¿®å¤ç‰ˆChatGLM"""
    print("ğŸš€ æµ‹è¯•ä¿®å¤ç‰ˆChatGLMä»£ç†")
    print("=" * 50)
    
    agent = FixedChatGLMAgent()
    
    # åˆå§‹åŒ–
    if not agent.initialize():
        print("âŒ åˆå§‹åŒ–å¤±è´¥")
        return None
    
    print("âœ… åˆå§‹åŒ–æˆåŠŸ")
    
    # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
    if not agent.test_basic_functions():
        print("âŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥")
        return None
    
    print("âœ… åŸºæœ¬åŠŸèƒ½æµ‹è¯•æˆåŠŸ")
    
    # æµ‹è¯•å¯¹è¯
    test_cases = [
        "ä½ å¥½",
        "ä½ æ˜¯è°ï¼Ÿ",
        "è¯·ç®€å•ä»‹ç»ä»£ç åˆ†æ",
        "Hello"
    ]
    
    success_count = 0
    for i, test_input in enumerate(test_cases, 1):
        print(f"\nğŸ§ª æµ‹è¯• {i}: {test_input}")
        response = agent.generate_response(test_input)
        if response:
            print(f"âœ… å›åº”: {response}")
            success_count += 1
        else:
            print("âŒ ç”Ÿæˆå¤±è´¥")
    
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {success_count}/{len(test_cases)} æˆåŠŸ")
    
    if success_count > 0:
        print("ğŸ‰ ä¿®å¤æ–¹æ¡ˆæœ‰æ•ˆï¼")
        return agent
    else:
        print("âŒ ä¿®å¤æ–¹æ¡ˆæ— æ•ˆ")
        return None

if __name__ == "__main__":
    print("ğŸš€ å¼€å§‹ChatGLMå…¼å®¹æ€§ä¿®å¤æµ‹è¯•")
    print("=" * 50)
    
    try:
        # è¿è¡Œä¿®å¤æµ‹è¯•
        result = test_fixed_chatglm()
        
        if result:
            print("\nğŸ‰ ChatGLMå…¼å®¹æ€§é—®é¢˜ä¿®å¤æˆåŠŸï¼")
            print("ğŸ’¡ å¯ä»¥ä½¿ç”¨è¿™ä¸ªä¿®å¤æ–¹æ¡ˆæ›´æ–°åŸå§‹ä»£ç†")
        else:
            print("\nâŒ ä¿®å¤æ–¹æ¡ˆä»éœ€æ”¹è¿›")
    except Exception as e:
        print(f"\nğŸ’¥ ç¨‹åºå¼‚å¸¸é€€å‡º: {e}")
        import traceback
        print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
