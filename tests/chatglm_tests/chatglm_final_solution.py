#!/usr/bin/env python3
"""
ChatGLM2-6B æœ€ç»ˆå…¼å®¹æ€§è§£å†³æ–¹æ¡ˆ
æ•´åˆæ‰€æœ‰ä¿®å¤ï¼Œæä¾›ç”Ÿäº§å°±ç»ªçš„ä»£ç†å®ç°
"""

import os
import sys
import torch
from typing import Optional, Dict, Any

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
sys.path.insert(0, project_root)

def apply_chatglm_compatibility_fixes():
    """åº”ç”¨ChatGLM2-6Bçš„æ‰€æœ‰å…¼å®¹æ€§ä¿®å¤"""
    print("ğŸ”§ åº”ç”¨ChatGLM2-6Bå…¼å®¹æ€§ä¿®å¤...")
    
    try:
        from transformers import AutoTokenizer, AutoConfig
        
        model_name = "THUDM/chatglm2-6b"
        
        # 1. ä¿®å¤tokenizerå…¼å®¹æ€§
        print("ğŸ”§ ä¿®å¤tokenizerå…¼å®¹æ€§...")
        tokenizer = AutoTokenizer.from_pretrained(
            model_name, 
            trust_remote_code=True
        )
        
        # è·å–ChatGLM tokenizerç±»å¹¶ä¿®å¤_padæ–¹æ³•
        ChatGLMTokenizer = type(tokenizer)
        if not hasattr(ChatGLMTokenizer, '_original_pad'):
            original_pad = ChatGLMTokenizer._pad
            ChatGLMTokenizer._original_pad = original_pad
            
            def compatible_pad(self, encoded_inputs, **kwargs):
                # ç§»é™¤ä¸å…¼å®¹çš„å‚æ•°
                filtered_kwargs = {k: v for k, v in kwargs.items() 
                                 if k not in ['padding_side']}
                return ChatGLMTokenizer._original_pad(self, encoded_inputs, **filtered_kwargs)
            
            ChatGLMTokenizer._pad = compatible_pad
            print("âœ… Tokenizerå…¼å®¹æ€§ä¿®å¤æˆåŠŸ")
        
        # 2. ä¿®å¤é…ç½®å…¼å®¹æ€§
        print("ğŸ”§ ä¿®å¤é…ç½®å…¼å®¹æ€§...")
        config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)
        ChatGLMConfig = type(config)
        
        if not hasattr(config, 'num_hidden_layers') and hasattr(config, 'num_layers'):
            if not hasattr(ChatGLMConfig, '_original_getattribute'):
                ChatGLMConfig._original_getattribute = ChatGLMConfig.__getattribute__
                
                def patched_getattribute(self, name):
                    if name == 'num_hidden_layers' and hasattr(self, 'num_layers'):
                        return self.num_layers
                    return ChatGLMConfig._original_getattribute(self, name)
                
                ChatGLMConfig.__getattribute__ = patched_getattribute
                print("âœ… é…ç½®å…¼å®¹æ€§ä¿®å¤æˆåŠŸ")
        
        # 3. é¢„ä¿®å¤æ¨¡å‹ç±»çš„get_masksæ–¹æ³• - è¿™æ˜¯å…³é”®ä¿®å¤ï¼
        print("ğŸ”§ é¢„ä¿®å¤ChatGLMæ¨¡å‹ç±»...")
        from transformers import AutoModelForCausalLM
        
        # åŠ è½½ä¸€ä¸ªä¸´æ—¶æ¨¡å‹å®ä¾‹æ¥è·å–æ¨¡å‹ç±»
        temp_model = AutoModelForCausalLM.from_pretrained(
            model_name,
            trust_remote_code=True,
            device_map="cpu",
            torch_dtype=torch.float32
        )
        
        # è·å–transformerç±»å¹¶ä¿®å¤get_masksæ–¹æ³•
        transformer = temp_model.transformer
        transformer_class = type(transformer)
        
        if not hasattr(transformer_class, '_original_get_masks'):
            print("ğŸ”§ ä¿®å¤get_masksæ–¹æ³•...")
            original_get_masks = transformer_class.get_masks
            transformer_class._original_get_masks = original_get_masks
            
            def safe_get_masks(self, input_ids, past_key_values, padding_mask=None):
                """å®‰å…¨çš„get_masksæ–¹æ³•ï¼Œå¤„ç†None past_key_values"""
                seq_length = input_ids.shape[1]
                
                # å®‰å…¨åœ°è·å–past_length
                if past_key_values is None or not past_key_values or past_key_values[0] is None or past_key_values[0][0] is None:
                    past_length = 0
                else:
                    try:
                        past_length = past_key_values[0][0].shape[0]
                    except (AttributeError, IndexError, TypeError):
                        past_length = 0
                
                # åˆ›å»ºattention mask
                import torch
                device = input_ids.device
                dtype = torch.float32
                
                full_attention_mask = torch.ones(
                    input_ids.shape[0], 
                    past_length + seq_length, 
                    past_length + seq_length,
                    device=device,
                    dtype=dtype
                )
                
                # åº”ç”¨å› æœæ©ç  (ä¸‹ä¸‰è§’çŸ©é˜µ)
                full_attention_mask.triu_(diagonal=1)
                full_attention_mask = full_attention_mask < 0.5
                
                if past_length:
                    full_attention_mask = full_attention_mask[..., past_length:, :]
                
                if padding_mask is not None:
                    full_attention_mask = full_attention_mask * padding_mask.unsqueeze(1)
                    if not past_length:
                        full_attention_mask = full_attention_mask * padding_mask.unsqueeze(-1)
                
                full_attention_mask = full_attention_mask.unsqueeze(1)
                return full_attention_mask
            
            transformer_class.get_masks = safe_get_masks
            print("âœ… get_masksæ–¹æ³•ä¿®å¤æˆåŠŸ")
        
        # æ¸…ç†ä¸´æ—¶æ¨¡å‹
        del temp_model
        
        return tokenizer, config
        
    except Exception as e:
        print(f"âŒ å…¼å®¹æ€§ä¿®å¤å¤±è´¥: {e}")
        import traceback
        print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
        return None, None

class ProductionChatGLMAgent:
    """ç”Ÿäº§å°±ç»ªçš„ChatGLMä»£ç†"""
    
    def __init__(self):
        self.model = None
        self.tokenizer = None
        self.initialized = False
        print("ğŸ¤– åˆå§‹åŒ–ç”Ÿäº§ç‰ˆChatGLMä»£ç†")
    
    def initialize(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        try:
            print("ğŸ”§ å¼€å§‹åˆå§‹åŒ–ChatGLM2-6Bæ¨¡å‹...")
            
            # åº”ç”¨å…¼å®¹æ€§ä¿®å¤
            self.tokenizer, config = apply_chatglm_compatibility_fixes()
            if not self.tokenizer:
                return False
            
            # åŠ è½½æ¨¡å‹
            from transformers import AutoModelForCausalLM
            
            model_name = "THUDM/chatglm2-6b"
            print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_name}")
            
            self.model = AutoModelForCausalLM.from_pretrained(
                model_name,
                trust_remote_code=True,
                device_map="auto",
                torch_dtype=torch.float16,  # ä½¿ç”¨åŠç²¾åº¦æé«˜æ•ˆç‡
                low_cpu_mem_usage=True      # å‡å°‘CPUå†…å­˜ä½¿ç”¨
            )
            
            # ä¿®å¤æ¨¡å‹å®ä¾‹çš„_extract_past_from_model_outputæ–¹æ³•
            if not hasattr(self.model, '_extract_past_from_model_output'):
                def _extract_past_from_model_output(*args, **kwargs):
                    outputs = args[0] if args else None
                    if outputs is None:
                        return None
                    if hasattr(outputs, 'past_key_values'):
                        return outputs.past_key_values
                    elif isinstance(outputs, dict) and 'past_key_values' in outputs:
                        return outputs['past_key_values']
                    return None
                
                import types
                self.model._extract_past_from_model_output = types.MethodType(_extract_past_from_model_output, self.model)
                print("âœ… æ¨¡å‹å…¼å®¹æ€§ä¿®å¤å®Œæˆ")
            
            print("âœ… æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ")
            self.initialized = True
            return True
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False
    
    def generate_response(self, user_input: str, max_new_tokens: int = 100) -> Optional[str]:
        """ç”Ÿæˆå›åº”"""
        if not self.initialized:
            print("âŒ æ¨¡å‹æœªåˆå§‹åŒ–")
            return None
        
        try:
            print(f"ğŸ§  ç”Ÿæˆå›åº”: {user_input}")
            
            # ä½¿ç”¨ChatGLMçš„chatæ–¹æ³•ï¼ˆæ¨èæ–¹æ³•ï¼‰
            try:
                response, history = self.model.chat(
                    self.tokenizer,
                    user_input,
                    history=[],
                    max_length=2048,
                    temperature=0.8,
                    top_p=0.8
                )
                print(f"âœ… ä½¿ç”¨chatæ–¹æ³•ç”ŸæˆæˆåŠŸ: {response}")
                return response
            
            except Exception as chat_error:
                print(f"âš ï¸ chatæ–¹æ³•å¤±è´¥ï¼Œå°è¯•generateæ–¹æ³•: {chat_error}")
                
                # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨generate
                inputs = self.tokenizer.encode(user_input, return_tensors="pt")
                if hasattr(self.tokenizer, 'pad_token_id') and self.tokenizer.pad_token_id is not None:
                    pad_token_id = self.tokenizer.pad_token_id
                else:
                    pad_token_id = self.tokenizer.eos_token_id
                
                # åˆ›å»ºæ³¨æ„åŠ›æ©ç 
                attention_mask = torch.ones_like(inputs)
                
                with torch.no_grad():
                    outputs = self.model.generate(
                        inputs,
                        attention_mask=attention_mask,
                        max_new_tokens=max_new_tokens,
                        temperature=0.8,
                        do_sample=True,
                        top_p=0.8,
                        pad_token_id=pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id,
                        use_cache=True  # å¯ä»¥å°è¯•å¯ç”¨ç¼“å­˜
                    )
                
                response = self.tokenizer.decode(outputs[0][inputs.shape[1]:], skip_special_tokens=True)
                print(f"âœ… ä½¿ç”¨generateæ–¹æ³•ç”ŸæˆæˆåŠŸ: {response}")
                return response
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None
    
    def test_functionality(self):
        """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
        if not self.initialized:
            return False
        
        try:
            # æµ‹è¯•tokenizer
            test_text = "ä½ å¥½ï¼Œä¸–ç•Œï¼"
            tokens = self.tokenizer.encode(test_text)
            decoded = self.tokenizer.decode(tokens)
            print(f"âœ… Tokenizeræµ‹è¯•æˆåŠŸ: {test_text} -> {decoded}")
            
            # æµ‹è¯•ç®€å•ç”Ÿæˆ
            response = self.generate_response("ä½ å¥½", max_new_tokens=20)
            if response:
                print(f"âœ… ç®€å•ç”Ÿæˆæµ‹è¯•æˆåŠŸ: {response}")
                return True
            else:
                print("âŒ ç®€å•ç”Ÿæˆæµ‹è¯•å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ åŠŸèƒ½æµ‹è¯•å¤±è´¥: {e}")
            return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ ChatGLM2-6B ç”Ÿäº§è§£å†³æ–¹æ¡ˆæµ‹è¯•")
    print("=" * 60)
    
    # åˆ›å»ºä»£ç†å®ä¾‹
    agent = ProductionChatGLMAgent()
    
    # åˆå§‹åŒ–
    if not agent.initialize():
        print("âŒ åˆå§‹åŒ–å¤±è´¥")
        return
    
    # åŸºæœ¬åŠŸèƒ½æµ‹è¯•
    if not agent.test_functionality():
        print("âŒ åŸºæœ¬åŠŸèƒ½æµ‹è¯•å¤±è´¥")
        return
    
    # å¯¹è¯æµ‹è¯•
    test_questions = [
        "ä½ å¥½",
        "ä½ æ˜¯è°ï¼Ÿ",
        "è¯·ç®€å•ä»‹ç»Pythonç¼–ç¨‹",
        "ä»€ä¹ˆæ˜¯ä»£ç åˆ†æï¼Ÿ"
    ]
    
    print(f"\nğŸ§ª å¼€å§‹å¯¹è¯æµ‹è¯• ({len(test_questions)} ä¸ªé—®é¢˜)")
    print("-" * 40)
    
    successful_responses = 0
    for i, question in enumerate(test_questions, 1):
        print(f"\né—®é¢˜ {i}: {question}")
        response = agent.generate_response(question, max_new_tokens=100)
        
        if response and response.strip():
            print(f"âœ… å›ç­”: {response}")
            successful_responses += 1
        else:
            print("âŒ ç”Ÿæˆå¤±è´¥æˆ–ç©ºå“åº”")
    
    # ç»“æœç»Ÿè®¡
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡")
    print(f"æˆåŠŸå“åº”: {successful_responses}/{len(test_questions)}")
    success_rate = (successful_responses / len(test_questions)) * 100
    print(f"æˆåŠŸç‡: {success_rate:.1f}%")
    
    if success_rate >= 75:
        print("ğŸ‰ ChatGLM2-6Bå…¼å®¹æ€§ä¿®å¤æˆåŠŸï¼")
        print("ğŸ’¡ å¯ä»¥å°†æ­¤è§£å†³æ–¹æ¡ˆåº”ç”¨åˆ°ç”Ÿäº§ç¯å¢ƒ")
    elif success_rate >= 50:
        print("âš ï¸ éƒ¨åˆ†åŠŸèƒ½æ­£å¸¸ï¼Œå»ºè®®è¿›ä¸€æ­¥ä¼˜åŒ–")
    else:
        print("âŒ éœ€è¦è¿›ä¸€æ­¥è°ƒè¯•å’Œä¿®å¤")

if __name__ == "__main__":
    main()
