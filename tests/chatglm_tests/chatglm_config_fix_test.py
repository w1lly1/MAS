#!/usr/bin/env python3
"""
ChatGLMé…ç½®å…¼å®¹æ€§ä¿®å¤æµ‹è¯•
è§£å†³ ChatGLMConfig ä¸æ–°ç‰ˆ transformers åº“çš„å…¼å®¹æ€§é—®é¢˜
"""

import os
import sys
import traceback
from typing import Optional, Tuple

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.join(os.path.dirname(__file__), '..', '..'))

try:
    from transformers import AutoTokenizer, AutoModel, AutoConfig
    import torch
    from transformers.cache_utils import DynamicCache
    from transformers.configuration_utils import PretrainedConfig
except ImportError as e:
    print(f"âŒ å¯¼å…¥åº“å¤±è´¥: {e}")
    sys.exit(1)


class ChatGLMCompatibilityFixer:
    """ChatGLMå…¼å®¹æ€§ä¿®å¤å™¨"""
    
    @staticmethod
    def apply_tokenizer_fix():
        """åº”ç”¨tokenizerå…¼å®¹æ€§ä¿®å¤"""
        from transformers import ChatGLMTokenizer
        
        # ä¿å­˜åŸå§‹_padæ–¹æ³•
        original_pad = ChatGLMTokenizer._pad
        
        def fixed_pad(self, encoded_inputs, max_length=None, padding_strategy=None, pad_to_multiple_of=None, return_attention_mask=None):
            """ä¿®å¤çš„_padæ–¹æ³•ï¼Œè¿‡æ»¤ä¸å…¼å®¹çš„å‚æ•°"""
            # è¿‡æ»¤æ‰padding_sideå‚æ•°
            if hasattr(self, 'padding_side'):
                delattr(self, 'padding_side')
            
            # è°ƒç”¨åŸå§‹æ–¹æ³•
            return original_pad(self, encoded_inputs, max_length, padding_strategy, pad_to_multiple_of, return_attention_mask)
        
        # åº”ç”¨ä¿®å¤
        ChatGLMTokenizer._pad = fixed_pad
        print("âœ… ChatGLM tokenizerå…¼å®¹æ€§ä¿®å¤å·²åº”ç”¨")
    
    @staticmethod
    def apply_config_fix():
        """åº”ç”¨configå…¼å®¹æ€§ä¿®å¤"""
        try:
            # åŠ¨æ€ä¿®è¡¥ChatGLMConfig
            from transformers import ChatGLMConfig
            
            # ä¿å­˜åŸå§‹__getattribute__æ–¹æ³•
            original_getattribute = ChatGLMConfig.__getattribute__
            
            def fixed_getattribute(self, name):
                """ä¿®å¤çš„__getattribute__æ–¹æ³•ï¼Œæ·»åŠ ç¼ºå¤±çš„å±æ€§æ˜ å°„"""
                # å±æ€§æ˜ å°„è¡¨
                attribute_mapping = {
                    'num_hidden_layers': 'num_layers',  # ChatGLMä½¿ç”¨num_layersè€Œä¸æ˜¯num_hidden_layers
                    'hidden_size': 'hidden_size',
                    'num_attention_heads': 'num_attention_heads',
                    'intermediate_size': 'ffn_hidden_size',
                }
                
                # å¦‚æœè¯·æ±‚çš„æ˜¯æ ‡å‡†å±æ€§åï¼Œå°è¯•æ˜ å°„åˆ°ChatGLMçš„å±æ€§å
                if name in attribute_mapping:
                    chatglm_attr = attribute_mapping[name]
                    if hasattr(self, chatglm_attr):
                        return original_getattribute(self, chatglm_attr)
                    else:
                        # å¦‚æœChatGLMå±æ€§ä¹Ÿä¸å­˜åœ¨ï¼Œæä¾›é»˜è®¤å€¼
                        defaults = {
                            'num_hidden_layers': 28,  # ChatGLM2-6Bçš„é»˜è®¤å±‚æ•°
                            'hidden_size': 4096,
                            'num_attention_heads': 32,
                            'intermediate_size': 13696,
                        }
                        if name in defaults:
                            return defaults[name]
                
                # è°ƒç”¨åŸå§‹æ–¹æ³•
                return original_getattribute(self, name)
            
            # åº”ç”¨ä¿®å¤
            ChatGLMConfig.__getattribute__ = fixed_getattribute
            print("âœ… ChatGLM configå…¼å®¹æ€§ä¿®å¤å·²åº”ç”¨")
            
        except Exception as e:
            print(f"âš ï¸ Configä¿®å¤åº”ç”¨å¤±è´¥: {e}")
    
    @staticmethod
    def apply_all_fixes():
        """åº”ç”¨æ‰€æœ‰å…¼å®¹æ€§ä¿®å¤"""
        ChatGLMCompatibilityFixer.apply_tokenizer_fix()
        ChatGLMCompatibilityFixer.apply_config_fix()


class FixedChatGLMAgent:
    """ä¿®å¤ç‰ˆChatGLMä»£ç†"""
    
    def __init__(self, model_name: str = "THUDM/chatglm2-6b"):
        self.model_name = model_name
        self.tokenizer = None
        self.model = None
        self.config = None
        
    def initialize(self) -> bool:
        """åˆå§‹åŒ–ä»£ç†"""
        try:
            print("ğŸ”§ å¼€å§‹åˆå§‹åŒ–ä¿®å¤ç‰ˆChatGLM...")
            
            # åº”ç”¨æ‰€æœ‰å…¼å®¹æ€§ä¿®å¤
            print("ğŸ”§ åº”ç”¨ChatGLMå…¼å®¹æ€§ä¿®å¤...")
            ChatGLMCompatibilityFixer.apply_all_fixes()
            
            # åŠ è½½é…ç½®
            print("ğŸ“‹ åŠ è½½é…ç½®...")
            self.config = AutoConfig.from_pretrained(self.model_name, trust_remote_code=True)
            print(f"âœ… é…ç½®åŠ è½½æˆåŠŸ: num_layers={getattr(self.config, 'num_layers', 'N/A')}")
            
            # åŠ è½½tokenizer
            print("ğŸ”¤ åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                self.model_name, 
                trust_remote_code=True,
                padding=True,
                truncation=True,
            )
            print("âœ… TokenizeråŠ è½½æˆåŠŸ")
            
            # åŠ è½½æ¨¡å‹
            print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.model_name}")
            self.model = AutoModel.from_pretrained(
                self.model_name,
                trust_remote_code=True,
                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
                device_map="auto" if torch.cuda.is_available() else None,
                low_cpu_mem_usage=True,
            )
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            self.model.eval()
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            return True
            
        except Exception as e:
            print(f"âŒ åˆå§‹åŒ–å¤±è´¥: {e}")
            print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False
    
    def test_tokenizer(self) -> bool:
        """æµ‹è¯•tokenizeråŸºæœ¬åŠŸèƒ½"""
        try:
            print("ğŸ§ª æµ‹è¯•åŸºæœ¬tokenizeråŠŸèƒ½...")
            test_text = "ä½ å¥½ï¼Œæˆ‘æ˜¯ChatGLMåŠ©æ‰‹"
            
            # ç¼–ç 
            encoded = self.tokenizer.encode(test_text)
            print(f"ğŸ“ åŸæ–‡: {test_text}")
            print(f"ğŸ”¢ ç¼–ç : {encoded[:10]}..." if len(encoded) > 10 else f"ğŸ”¢ ç¼–ç : {encoded}")
            
            # è§£ç 
            decoded = self.tokenizer.decode(encoded)
            print(f"ğŸ“„ è§£ç : {decoded}")
            
            return True
            
        except Exception as e:
            print(f"âŒ Tokenizeræµ‹è¯•å¤±è´¥: {e}")
            return False
    
    def generate_response(self, message: str, max_length: int = 512) -> Tuple[bool, Optional[str]]:
        """ç”Ÿæˆå›åº”"""
        try:
            print(f"ğŸ§  ç”Ÿæˆå›åº”: {message}")
            
            # ä½¿ç”¨chatæ–¹æ³•ç”Ÿæˆå›åº”
            response, history = self.model.chat(
                self.tokenizer,
                message,
                history=[],
                max_length=max_length,
                temperature=0.8,
                top_p=0.8,
                do_sample=True,
            )
            
            return True, response
            
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤±è´¥: {e}")
            print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return False, None
    
    def test_conversation(self) -> bool:
        """æµ‹è¯•å¯¹è¯åŠŸèƒ½"""
        test_messages = [
            "ä½ å¥½",
            "ä½ æ˜¯è°ï¼Ÿ",
            "è¯·ç®€å•ä»‹ç»ä»£ç åˆ†æ",
            "Hello",
        ]
        
        success_count = 0
        
        for i, message in enumerate(test_messages, 1):
            print(f"\nğŸ§ª æµ‹è¯• {i}: {message}")
            success, response = self.generate_response(message)
            
            if success and response:
                print(f"âœ… å›åº”: {response[:100]}..." if len(response) > 100 else f"âœ… å›åº”: {response}")
                success_count += 1
            else:
                print("âŒ ç”Ÿæˆå¤±è´¥")
        
        print(f"\nğŸ“Š æµ‹è¯•ç»“æœ: {success_count}/{len(test_messages)} æˆåŠŸ")
        return success_count == len(test_messages)


def check_config_attributes():
    """æ£€æŸ¥é…ç½®å±æ€§"""
    try:
        print("\nğŸ” æ£€æŸ¥ChatGLMé…ç½®å±æ€§...")
        config = AutoConfig.from_pretrained("THUDM/chatglm2-6b", trust_remote_code=True)
        
        # æ£€æŸ¥å¸¸è§å±æ€§
        attributes_to_check = [
            'num_layers', 'num_hidden_layers', 'hidden_size', 
            'num_attention_heads', 'ffn_hidden_size', 'intermediate_size'
        ]
        
        print("ğŸ“‹ é…ç½®å±æ€§:")
        for attr in attributes_to_check:
            if hasattr(config, attr):
                value = getattr(config, attr)
                print(f"  âœ… {attr}: {value}")
            else:
                print(f"  âŒ {attr}: ä¸å­˜åœ¨")
        
        # åˆ—å‡ºæ‰€æœ‰å±æ€§
        print("\nğŸ“‹ æ‰€æœ‰é…ç½®å±æ€§:")
        for attr in sorted(dir(config)):
            if not attr.startswith('_') and not callable(getattr(config, attr)):
                try:
                    value = getattr(config, attr)
                    print(f"  {attr}: {value}")
                except:
                    print(f"  {attr}: <æ— æ³•è·å–>")
                    
    except Exception as e:
        print(f"âŒ é…ç½®æ£€æŸ¥å¤±è´¥: {e}")


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ æµ‹è¯•ä¿®å¤ç‰ˆChatGLMä»£ç†ï¼ˆé…ç½®ä¿®å¤ç‰ˆï¼‰")
    print("=" * 50)
    
    # æ£€æŸ¥é…ç½®å±æ€§
    check_config_attributes()
    
    # åˆå§‹åŒ–ä»£ç†
    print("\nğŸ¤– åˆå§‹åŒ–ä¿®å¤ç‰ˆChatGLMä»£ç†")
    agent = FixedChatGLMAgent()
    
    if not agent.initialize():
        print("âŒ åˆå§‹åŒ–å¤±è´¥ï¼Œé€€å‡ºæµ‹è¯•")
        return False
    
    print("âœ… åˆå§‹åŒ–æˆåŠŸ")
    
    # æµ‹è¯•tokenizer
    if not agent.test_tokenizer():
        print("âŒ Tokenizeræµ‹è¯•å¤±è´¥")
        return False
    
    print("âœ… åŸºæœ¬åŠŸèƒ½æµ‹è¯•æˆåŠŸ")
    
    # æµ‹è¯•å¯¹è¯
    if agent.test_conversation():
        print("\nâœ… ä¿®å¤æ–¹æ¡ˆæœ‰æ•ˆ")
        return True
    else:
        print("\nâŒ ä¿®å¤æ–¹æ¡ˆä»éœ€æ”¹è¿›")
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
