"""
ChatGLMå…¼å®¹çš„ç”¨æˆ·æ²Ÿé€šä»£ç† - ä¿®å¤ç‰ˆæœ¬
åŸºäºå…¼å®¹æ€§æµ‹è¯•ç»“æœï¼Œä½¿ç”¨ç›´æ¥æ¨¡å‹è°ƒç”¨é¿å…pipelineé—®é¢˜
"""

import os
import re
import json
import logging
import datetime
import asyncio
from typing import Dict, Any, Optional, List, Tuple

# å¯¼å…¥åŸºç¡€ç»„ä»¶
try:
    import sys
    sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
    from core.agents.base_agent import BaseAgent, Message
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºç®€å•çš„åŸºç±»
    class BaseAgent:
        def __init__(self, agent_id, name):
            self.agent_id = agent_id
            self.name = name
    
    class Message:
        def __init__(self, message_type, content):
            self.message_type = message_type
            self.content = content

logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)

class ChatGLMCompatibleAgent(BaseAgent):
    """ChatGLMå…¼å®¹çš„ç”¨æˆ·æ²Ÿé€šä»£ç†"""
    
    def __init__(self):
        super().__init__("chatglm_comm_agent", "ChatGLMå…¼å®¹ç”¨æˆ·æ²Ÿé€šä»£ç†")
        
        # ChatGLMç»„ä»¶
        self.model = None
        self.tokenizer = None
        
        # ä¼šè¯ç®¡ç†
        self.session_memory = {}
        
        # æ¨¡å‹é…ç½®
        self.model_name = "THUDM/chatglm2-6b"
        self.ai_enabled = False
        
        print(f"ğŸ¤– åˆå§‹åŒ–ChatGLMå…¼å®¹ä»£ç†: {self.model_name}")
    
    async def initialize_chatglm(self):
        """åˆå§‹åŒ–ChatGLMæ¨¡å‹ - ä½¿ç”¨å…¼å®¹æ–¹æ³•"""
        try:
            print("ğŸ”§ å¼€å§‹åˆå§‹åŒ–ChatGLMæ¨¡å‹ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰...")
            
            from transformers import AutoModel, AutoTokenizer
            import torch
            
            model_name = self.model_name
            print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {model_name}")
            
            # åŠ è½½tokenizer - é¿å…pipelineç›¸å…³é—®é¢˜
            print("ğŸ”§ åŠ è½½tokenizer...")
            self.tokenizer = AutoTokenizer.from_pretrained(
                model_name, 
                trust_remote_code=True
            )
            print("âœ… TokenizeråŠ è½½æˆåŠŸ")
            
            # åŠ è½½æ¨¡å‹ - ä½¿ç”¨AutoModelè€Œä¸æ˜¯AutoModelForCausalLM
            print("ğŸ”§ åŠ è½½æ¨¡å‹...")
            self.model = AutoModel.from_pretrained(
                model_name,
                trust_remote_code=True,
                device_map="auto" if self._has_gpu() else None
            )
            print("âœ… æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            # æµ‹è¯•åŸºæœ¬åŠŸèƒ½
            print("ğŸ§ª æµ‹è¯•åŸºæœ¬å¯¹è¯åŠŸèƒ½...")
            test_response = await self._test_basic_chat()
            
            if test_response:
                print(f"âœ… åŸºæœ¬æµ‹è¯•æˆåŠŸ: {test_response[:50]}...")
                self.ai_enabled = True
                print("ğŸ‰ ChatGLMåˆå§‹åŒ–å®Œæˆ")
                return True
            else:
                print("âŒ åŸºæœ¬æµ‹è¯•å¤±è´¥")
                return False
                
        except Exception as e:
            print(f"âŒ ChatGLMåˆå§‹åŒ–å¤±è´¥: {e}")
            import traceback
            print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            self.ai_enabled = False
            return False
    
    async def _test_basic_chat(self) -> Optional[str]:
        """æµ‹è¯•åŸºæœ¬å¯¹è¯åŠŸèƒ½"""
        try:
            test_input = "ä½ å¥½"
            print(f"ğŸ§ª æµ‹è¯•è¾“å…¥: {test_input}")
            
            # ä½¿ç”¨ChatGLMçš„åŸç”Ÿchatæ–¹æ³•
            response, _ = self.model.chat(
                self.tokenizer, 
                test_input, 
                history=[],
                max_length=2048,
                temperature=0.8
            )
            
            print(f"ğŸ‰ æµ‹è¯•å›åº”: {response}")
            return response
            
        except Exception as e:
            print(f"âŒ åŸºæœ¬æµ‹è¯•å¤±è´¥: {e}")
            return None
    
    def _has_gpu(self) -> bool:
        """æ£€æµ‹GPUå¯ç”¨æ€§"""
        try:
            import torch
            return torch.cuda.is_available()
        except:
            return False
    
    async def generate_response(self, user_input: str, session_id: str = "default") -> Optional[str]:
        """ç”ŸæˆAIå›åº”"""
        if not self.ai_enabled or not self.model or not self.tokenizer:
            print("âŒ æ¨¡å‹æœªå°±ç»ª")
            return None
        
        try:
            print(f"ğŸ§  å¤„ç†ç”¨æˆ·è¾“å…¥: {user_input}")
            
            # è·å–ä¼šè¯å†å²
            history = self._get_session_history(session_id)
            
            # ä½¿ç”¨ChatGLMåŸç”Ÿchatæ–¹æ³•
            response, new_history = self.model.chat(
                self.tokenizer,
                user_input,
                history=history,
                max_length=2048,
                temperature=0.8
            )
            
            # æ›´æ–°ä¼šè¯å†å²
            self._update_session_history(session_id, new_history)
            
            print(f"âœ… AIå›åº”ç”Ÿæˆ: {response}")
            return response
            
        except Exception as e:
            print(f"âŒ å›åº”ç”Ÿæˆå¤±è´¥: {e}")
            import traceback
            print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            return None
    
    def _get_session_history(self, session_id: str) -> List:
        """è·å–ä¼šè¯å†å²"""
        if session_id not in self.session_memory:
            self.session_memory[session_id] = {
                "history": [],
                "last_active": datetime.datetime.now().isoformat()
            }
        
        return self.session_memory[session_id]["history"]
    
    def _update_session_history(self, session_id: str, new_history: List):
        """æ›´æ–°ä¼šè¯å†å²"""
        if session_id not in self.session_memory:
            self.session_memory[session_id] = {
                "history": [],
                "last_active": datetime.datetime.now().isoformat()
            }
        
        # é™åˆ¶å†å²é•¿åº¦
        if len(new_history) > 10:  # ä¿æŒæœ€è¿‘5è½®å¯¹è¯
            new_history = new_history[-10:]
        
        self.session_memory[session_id]["history"] = new_history
        self.session_memory[session_id]["last_active"] = datetime.datetime.now().isoformat()
    
    async def handle_user_input(self, user_input: str, session_id: str = "default") -> str:
        """å¤„ç†ç”¨æˆ·è¾“å…¥çš„ä¸»æ¥å£"""
        try:
            # ç”ŸæˆAIå›åº”
            response = await self.generate_response(user_input, session_id)
            
            if response:
                return response
            else:
                return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç”Ÿæˆå›åº”ã€‚è¯·ç¨åå†è¯•ã€‚"
                
        except Exception as e:
            print(f"âŒ å¤„ç†ç”¨æˆ·è¾“å…¥å¤±è´¥: {e}")
            return "ç³»ç»Ÿé”™è¯¯ï¼Œè¯·é‡è¯•ã€‚"

# æµ‹è¯•å‡½æ•°
async def test_chatglm_compatible_agent():
    """æµ‹è¯•ChatGLMå…¼å®¹ä»£ç†"""
    print("ğŸ§ª æµ‹è¯•ChatGLMå…¼å®¹ä»£ç†...")
    
    agent = ChatGLMCompatibleAgent()
    
    # åˆå§‹åŒ–
    if await agent.initialize_chatglm():
        print("âœ… ä»£ç†åˆå§‹åŒ–æˆåŠŸ")
        
        # æµ‹è¯•å¯¹è¯
        test_cases = [
            "ä½ å¥½",
            "ä½ æ˜¯è°ï¼Ÿ",
            "è¯·ä»‹ç»ä»£ç åˆ†æçš„é‡è¦æ€§",
            "How are you?"
        ]
        
        for i, test_input in enumerate(test_cases, 1):
            print(f"\nğŸ§ª æµ‹è¯• {i}: {test_input}")
            response = await agent.handle_user_input(test_input)
            print(f"âœ… å›åº”: {response}")
        
        return agent
    else:
        print("âŒ ä»£ç†åˆå§‹åŒ–å¤±è´¥")
        return None

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    result = asyncio.run(test_chatglm_compatible_agent())
    
    if result:
        print("ğŸ‰ ChatGLMå…¼å®¹ä»£ç†æµ‹è¯•æˆåŠŸï¼")
    else:
        print("âŒ ChatGLMå…¼å®¹ä»£ç†æµ‹è¯•å¤±è´¥ï¼")
