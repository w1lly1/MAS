"""
AIé©±åŠ¨çš„å¯è¯»æ€§å¢å¼ºä»£ç† - å°†å¤æ‚çš„JSONåˆ†ææŠ¥å‘Šè½¬æ¢ä¸ºæ˜“è¯»çš„Markdownæ ¼å¼æ‘˜è¦

è¿™ä¸ªä»£ç†çš„æ ¸å¿ƒåŠŸèƒ½æ˜¯ï¼š
1. æ‰«æanalysis/{run_id}ç›®å½•ä¸‹æ‰€æœ‰JSONæ–‡ä»¶ï¼ˆagentså’Œconsolidatedç›®å½•ï¼‰
2. åˆ†æå’Œåˆ†ç±»é—®é¢˜
3. ç”ŸæˆMarkdownæ ¼å¼çš„æ˜“ç†è§£çš„ä¸­æ–‡æ‘˜è¦
4. åˆ›å»ºanalysis/{run_id}/readability_enhancementç›®å½•ç»“æ„
5. ä¿å­˜å¢å¼ºåçš„Markdownæ–‡ä»¶

CPUå‹å¥½è®¾è®¡ï¼šä½¿ç”¨è½»é‡çº§çš„æ–‡æœ¬å¤„ç†ï¼Œæ— éœ€å¤§å‹ç”Ÿæˆæ¨¡å‹
"""

import os
import json
import logging
import asyncio
from typing import Dict, Any, Optional, List
from pathlib import Path
from dataclasses import dataclass, asdict
from enum import Enum

from .base_agent import BaseAgent, Message
from infrastructure.reports import report_manager

# è®¾ç½®æ—¥å¿—
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)


class AIDrivenReadabilityEnhancementAgent(BaseAgent):
    """
    AIé©±åŠ¨çš„å¯è¯»æ€§å¢å¼ºä»£ç† V2
    
    èŒè´£ï¼š
    1. æ‰«æ analysis/{run_id}/ ç›®å½•ä¸‹æ‰€æœ‰JSONæ–‡ä»¶
    2. åˆ†æå’Œåˆ†ç±»é—®é¢˜
    3. ç”ŸæˆMarkdownæ ¼å¼çš„æ˜“ç†è§£çš„ä¸­æ–‡æ‘˜è¦
    4. åˆ›å»ºanalysis/{run_id}/readability_enhancement/ç›®å½•
    5. ä¿å­˜agentså’Œconsolidatedä¸¤ç±»å¢å¼ºæ–‡ä»¶
    
    ç‰¹ç‚¹ï¼š
    - å®Œå…¨åŸºäºCPUè¿è¡Œï¼Œæ— éœ€GPU
    - ç”ŸæˆMarkdownæ ¼å¼è¾“å‡º
    - è‡ªåŠ¨æ‰«æå’Œå¤„ç†run_idç›®å½•ä¸‹çš„æ‰€æœ‰æŠ¥å‘Š
    """
    
    def __init__(self):
        super().__init__(
            agent_id="ai_readability_enhancement_agent",
            name="AIé©±åŠ¨çš„å¯è¯»æ€§å¢å¼ºä»£ç†"
        )
        
        # ç¼“å­˜
        self.report_cache = {}
        self.reports_base_dir = Path(__file__).parent.parent.parent / "reports" / "analysis"
        
    async def initialize(self):
        """åˆå§‹åŒ–ä»£ç†"""
        try:
            logger.info(f"åˆå§‹åŒ–å¯è¯»æ€§å¢å¼ºä»£ç†")
            self.is_running = True
            logger.info("âœ… å¯è¯»æ€§å¢å¼ºä»£ç†åˆå§‹åŒ–å®Œæˆ")
        except Exception as e:
            logger.error(f"åˆå§‹åŒ–å¤±è´¥: {e}")
            raise
    
    async def handle_message(self, message: Message):
        """å¤„ç†æ¥æ”¶åˆ°çš„æ¶ˆæ¯"""
        if message.message_type == "analyze_consolidated_report":
            # å¤„ç†æ¥è‡ªSummaryAgentçš„è½¬å‘æ¶ˆæ¯
            try:
                run_id = message.content.get("run_id")
                requirement_id = message.content.get("requirement_id")
                
                # æ‰«æè¯¥run_idä¸‹çš„æ‰€æœ‰æŠ¥å‘Šå¹¶è¿›è¡Œå¯è¯»æ€§å¢å¼º
                await self.enhance_run_reports(run_id)
                
                logger.info(f"âœ… å¯è¯»æ€§å¢å¼ºå®Œæˆ: run_id={run_id}")
                
            except Exception as e:
                logger.error(f"âŒ å¤„ç†æ¶ˆæ¯å¤±è´¥: {e}")
    
    async def enhance_run_reports(self, run_id: str) -> bool:
        """
        æ‰«æå¹¶å¢å¼ºæŒ‡å®šrun_idä¸‹çš„æ‰€æœ‰æŠ¥å‘Š
        
        Args:
            run_id: è¿è¡ŒID
        
        Returns:
            æ˜¯å¦æˆåŠŸå¤„ç†
        """
        try:
            run_dir = self.reports_base_dir / run_id
            
            if not run_dir.exists():
                logger.warning(f"âš ï¸  run_idç›®å½•ä¸å­˜åœ¨: {run_dir}")
                return False
            
            logger.info(f"ğŸ” æ‰«ææŠ¥å‘Šç›®å½•: {run_dir}")
            
            # åˆ›å»ºè¾“å‡ºç›®å½•ç»“æ„
            enhancement_dir = run_dir / "readability_enhancement"
            agents_dir = enhancement_dir / "agents"
            consolidated_dir = enhancement_dir / "consolidated"
            
            agents_dir.mkdir(parents=True, exist_ok=True)
            consolidated_dir.mkdir(parents=True, exist_ok=True)
            
            logger.info(f"ğŸ“ å·²åˆ›å»ºè¾“å‡ºç›®å½•: {enhancement_dir}")
            
            # å¤„ç†agentsç›®å½•ä¸‹çš„JSONæ–‡ä»¶
            agents_source_dir = run_dir / "agents"
            if agents_source_dir.exists():
                for json_file in agents_source_dir.glob("*.json"):
                    await self._enhance_single_report(json_file, agents_dir, "agents")
            
            # å¤„ç†consolidatedç›®å½•ä¸‹çš„JSONæ–‡ä»¶
            consolidated_source_dir = run_dir / "consolidated"
            if consolidated_source_dir.exists():
                for json_file in consolidated_source_dir.glob("*.json"):
                    await self._enhance_single_report(json_file, consolidated_dir, "consolidated")
            
            logger.info(f"âœ… run_id {run_id} çš„æ‰€æœ‰æŠ¥å‘Šå·²å®Œæˆå¯è¯»æ€§å¢å¼º")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¢å¼ºæŠ¥å‘Šå¤±è´¥: {e}")
            return False
    
    async def _enhance_single_report(
        self,
        json_file: Path,
        output_dir: Path,
        category: str
    ) -> bool:
        """
        å¢å¼ºå•ä¸ªJSONæŠ¥å‘Š
        
        Args:
            json_file: æºJSONæ–‡ä»¶è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            category: æ–‡ä»¶ç±»åˆ« ("agents" æˆ– "consolidated")
        
        Returns:
            æ˜¯å¦æˆåŠŸå¤„ç†
        """
        try:
            # è¯»å–JSONæ–‡ä»¶
            with open(json_file, 'r', encoding='utf-8') as f:
                report_data = json.load(f)
            
            logger.info(f"ğŸ“„ å¤„ç†æ–‡ä»¶: {json_file.name}")
            
            # ç”ŸæˆMarkdownæ‘˜è¦
            markdown_content = self._generate_markdown_summary(report_data, category)
            
            # ç”Ÿæˆè¾“å‡ºæ–‡ä»¶å
            base_name = json_file.stem  # å»æ‰.json
            output_file = output_dir / f"{base_name}.md"
            
            # ä¿å­˜Markdownæ–‡ä»¶
            with open(output_file, 'w', encoding='utf-8') as f:
                f.write(markdown_content)
            
            logger.info(f"âœ… å·²ä¿å­˜: {output_file}")
            return True
            
        except Exception as e:
            logger.error(f"âŒ å¤„ç†æ–‡ä»¶ {json_file.name} å¤±è´¥: {e}")
            return False
    
    def _generate_markdown_summary(self, report_data: Dict[str, Any], category: str) -> str:
        """
        ç”ŸæˆMarkdownæ ¼å¼çš„æŠ¥å‘Šæ‘˜è¦
        
        Args:
            report_data: æŠ¥å‘Šæ•°æ®
            category: æ–‡ä»¶ç±»åˆ«
        
        Returns:
            Markdownæ ¼å¼çš„æ–‡æœ¬
        """
        lines = []
        
        # æ ‡é¢˜
        file_name = report_data.get("file", report_data.get("readable_file", "Unknown File"))
        lines.append(f"# ä»£ç åˆ†ææŠ¥å‘Š - {file_name}\n")
        
        # åŸºæœ¬ä¿¡æ¯
        lines.append("## ğŸ“‹ åŸºæœ¬ä¿¡æ¯\n")
        
        requirement_id = report_data.get("requirement_id")
        run_id = report_data.get("run_id")
        
        if requirement_id:
            lines.append(f"- **éœ€æ±‚ID**: {requirement_id}")
        if run_id:
            lines.append(f"- **è¿è¡ŒID**: {run_id}")
        
        status = report_data.get("status", "unknown")
        lines.append(f"- **çŠ¶æ€**: {status}")
        
        analysis_types = report_data.get("analysis_types", [])
        if analysis_types:
            types_str = ", ".join(analysis_types)
            lines.append(f"- **åˆ†æç±»å‹**: {types_str}")
        
        lines.append("")
        
        # ç»Ÿè®¡ä¿¡æ¯
        issue_count = report_data.get("issue_count", 0)
        severity_stats = report_data.get("severity_stats", {})
        
        lines.append("## ğŸ“Š é—®é¢˜ç»Ÿè®¡\n")
        lines.append(f"**æ€»é—®é¢˜æ•°**: {issue_count}\n")
        
        if severity_stats:
            lines.append("### ä¸¥é‡ç¨‹åº¦åˆ†å¸ƒ\n")
            for severity in ["critical", "high", "medium", "low"]:
                count = severity_stats.get(severity, 0)
                if count > 0:
                    severity_cn = self._translate_severity(severity)
                    lines.append(f"- {severity_cn}: {count}")
            lines.append("")
        
        # é—®é¢˜è¯¦æƒ…
        issues = report_data.get("issues", [])
        if issues:
            lines.append("## ğŸ” é—®é¢˜è¯¦æƒ…\n")
            
            # æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„
            grouped_issues = self._group_issues_by_severity(issues)
            
            for severity in ["critical", "high", "medium", "low"]:
                severity_issues = grouped_issues.get(severity, [])
                if not severity_issues:
                    continue
                
                severity_cn = self._translate_severity(severity)
                lines.append(f"### {severity_cn}é—®é¢˜ ({len(severity_issues)}ä¸ª)\n")
                
                # æŒ‰sourceåˆ†ç±»
                by_source = self._group_issues_by_source(severity_issues)
                
                for source in sorted(by_source.keys()):
                    source_issues = by_source[source]
                    source_cn = self._translate_source(source)
                    
                    lines.append(f"#### {source_cn}\n")
                    
                    for idx, issue in enumerate(source_issues[:5], 1):  # æ¯ä¸ªæ¥æºæœ€å¤šæ˜¾ç¤º5ä¸ª
                        description = issue.get("description", "No description")
                        line_num = issue.get("line")
                        
                        if line_num:
                            lines.append(f"{idx}. **ç¬¬ {line_num} è¡Œ**: {description}")
                        else:
                            lines.append(f"{idx}. {description}")
                    
                    if len(source_issues) > 5:
                        lines.append(f"   ... è¿˜æœ‰ {len(source_issues) - 5} ä¸ªé—®é¢˜\n")
                    else:
                        lines.append("")
        
        # æ”¹è¿›å»ºè®®
        lines.append("## ğŸ’¡ æ”¹è¿›å»ºè®®\n")
        
        if issue_count > 0:
            critical_count = severity_stats.get("critical", 0)
            high_count = severity_stats.get("high", 0)
            medium_count = severity_stats.get("medium", 0)
            low_count = severity_stats.get("low", 0)
            
            if critical_count > 0:
                lines.append(f"### ğŸš¨ ç«‹å³å¤„ç†\n")
                lines.append(f"- æ£€æµ‹åˆ° {critical_count} ä¸ªä¸¥é‡é—®é¢˜ï¼Œéœ€è¦ä¼˜å…ˆä¿®å¤")
                lines.append(f"- å»ºè®®ç«‹å³è¿›è¡Œå½±å“åˆ†æå’Œä¿®å¤è§„åˆ’\n")
            
            if high_count > 0:
                lines.append(f"### ğŸ”´ é«˜ä¼˜å…ˆçº§\n")
                lines.append(f"- æ£€æµ‹åˆ° {high_count} ä¸ªé«˜çº§é—®é¢˜")
                lines.append(f"- å»ºè®®åœ¨æœ¬è½®è¿­ä»£ä¸­å®Œæˆä¿®å¤\n")
            
            if medium_count > 0:
                lines.append(f"### ğŸŸ¡ ä¸­ä¼˜å…ˆçº§\n")
                lines.append(f"- æ£€æµ‹åˆ° {medium_count} ä¸ªä¸­ç­‰é—®é¢˜")
                lines.append(f"- å»ºè®®åœ¨ä¸‹ä¸€ä¸ªå‘¨æœŸå†…é€æ­¥æ”¹è¿›\n")
            
            if low_count > 0:
                lines.append(f"### ğŸŸ¢ ä½ä¼˜å…ˆçº§\n")
                lines.append(f"- æ£€æµ‹åˆ° {low_count} ä¸ªä½çº§é—®é¢˜")
                lines.append(f"- å»ºè®®åœ¨ä»£ç ç»´æŠ¤ä¸­æŒç»­æ”¹è¿›\n")
        else:
            lines.append("âœ… æœªæ£€æµ‹åˆ°é—®é¢˜ï¼Œä»£ç è´¨é‡è‰¯å¥½ï¼\n")
        
        # å·¥ä½œé‡ä¼°è®¡
        estimated_effort = self._estimate_effort(issue_count)
        lines.append("## â±ï¸ å·¥ä½œé‡ä¼°è®¡\n")
        lines.append(f"**é¢„è®¡ä¿®å¤å·¥ä½œé‡**: {estimated_effort}\n")
        
        # åˆ†æè¯¦æƒ…ï¼ˆä»…consolidatedç±»å‹ï¼‰
        if category == "consolidated" and "analysis_types" in report_data:
            lines.append("## ğŸ“ˆ åˆ†æè¯¦æƒ…\n")
            
            for analysis_type in analysis_types:
                type_cn = self._translate_analysis_type(analysis_type)
                count = self._count_issues_by_type(issues, analysis_type)
                if count > 0:
                    lines.append(f"- **{type_cn}**: {count} ä¸ªé—®é¢˜")
            
            lines.append("")
        
        # é¡µè„š
        lines.append("---\n")
        lines.append(f"*æœ¬æŠ¥å‘Šç”±AIå¯è¯»æ€§å¢å¼ºä»£ç†è‡ªåŠ¨ç”Ÿæˆ | ç”Ÿæˆæ—¶é—´: {self._get_current_time()}*\n")
        
        return "\n".join(lines)
    
    def _group_issues_by_severity(self, issues: List[Dict]) -> Dict[str, List[Dict]]:
        """æŒ‰ä¸¥é‡ç¨‹åº¦åˆ†ç»„é—®é¢˜"""
        grouped = {
            "critical": [],
            "high": [],
            "medium": [],
            "low": []
        }
        
        for issue in issues:
            severity = issue.get("severity", "low")
            if severity in grouped:
                grouped[severity].append(issue)
        
        return grouped
    
    def _group_issues_by_source(self, issues: List[Dict]) -> Dict[str, List[Dict]]:
        """æŒ‰æ¥æºåˆ†ç»„é—®é¢˜"""
        grouped = {}
        
        for issue in issues:
            source = issue.get("source", "unknown")
            if source not in grouped:
                grouped[source] = []
            grouped[source].append(issue)
        
        return grouped
    
    def _count_issues_by_type(self, issues: List[Dict], analysis_type: str) -> int:
        """è®¡ç®—æŸç§åˆ†æç±»å‹çš„é—®é¢˜æ•°é‡"""
        # æ ¹æ®issueçš„sourceå­—æ®µæ¨æ–­åˆ†æç±»å‹
        type_mapping = {
            "security_analysis": ["security_vulnerability", "security_risk"],
            "performance_analysis": ["performance_bottleneck"],
            "static_analysis": ["style", "quality"],
            "ai_analysis": []
        }
        
        sources = type_mapping.get(analysis_type, [])
        count = 0
        
        for issue in issues:
            source = issue.get("source", "")
            if source in sources:
                count += 1
        
        return count
    
    def _translate_severity(self, severity: str) -> str:
        """ç¿»è¯‘ä¸¥é‡ç¨‹åº¦"""
        mapping = {
            "critical": "ğŸš¨ ä¸¥é‡",
            "high": "ğŸ”´ é«˜",
            "medium": "ğŸŸ¡ ä¸­",
            "low": "ğŸŸ¢ ä½"
        }
        return mapping.get(severity, severity)
    
    def _translate_source(self, source: str) -> str:
        """ç¿»è¯‘é—®é¢˜æ¥æº"""
        mapping = {
            "style": "ä»£ç é£æ ¼",
            "quality": "ä»£ç è´¨é‡",
            "security": "å®‰å…¨é—®é¢˜",
            "security_vulnerability": "å®‰å…¨æ¼æ´",
            "security_risk": "å®‰å…¨é£é™©",
            "performance": "æ€§èƒ½é—®é¢˜",
            "performance_bottleneck": "æ€§èƒ½ç“¶é¢ˆ",
            "complexity": "å¤æ‚åº¦é—®é¢˜"
        }
        return mapping.get(source, source)
    
    def _translate_analysis_type(self, analysis_type: str) -> str:
        """ç¿»è¯‘åˆ†æç±»å‹"""
        mapping = {
            "security_analysis": "å®‰å…¨åˆ†æ",
            "performance_analysis": "æ€§èƒ½åˆ†æ",
            "static_analysis": "é™æ€åˆ†æ",
            "ai_analysis": "AIåˆ†æ"
        }
        return mapping.get(analysis_type, analysis_type)
    
    def _estimate_effort(self, issue_count: int) -> str:
        """ä¼°è®¡ä¿®å¤å·¥ä½œé‡"""
        if issue_count == 0:
            return "æ— "
        elif issue_count < 10:
            return "ä½ (~0.5-1å¤©)"
        elif issue_count < 30:
            return "ä¸­ (~2-3å¤©)"
        elif issue_count < 60:
            return "é«˜ (~1å‘¨)"
        else:
            return "éå¸¸é«˜ (>1å‘¨)"
    
    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´"""
        from datetime import datetime
        return datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    
    async def _execute_task_impl(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ‰§è¡Œä»»åŠ¡çš„å®ç°ï¼ˆBaseAgentæŠ½è±¡æ–¹æ³•ï¼‰
        
        å¯¹äºå¯è¯»æ€§å¢å¼ºä»£ç†ï¼Œä»»åŠ¡é€šè¿‡æ¶ˆæ¯å¤„ç†è€Œä¸æ˜¯ç›´æ¥ä»»åŠ¡æ‰§è¡Œ
        æ­¤æ–¹æ³•æä¾›å¤‡ç”¨çš„åŒæ­¥ä»»åŠ¡æ¥å£
        """
        try:
            if isinstance(task_data, dict) and "run_id" in task_data:
                run_id = task_data.get("run_id")
                result = await self.enhance_run_reports(run_id)
                return {
                    "status": "success" if result else "failed",
                    "run_id": run_id,
                    "message": f"å¯è¯»æ€§å¢å¼ºå®Œæˆ" if result else "å¯è¯»æ€§å¢å¼ºå¤±è´¥"
                }
            else:
                return {
                    "status": "error",
                    "message": "ä»»åŠ¡æ•°æ®æ ¼å¼é”™è¯¯ï¼Œéœ€è¦åŒ…å«run_id"
                }
        except Exception as e:
            logger.error(f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {e}")
            return {
                "status": "error",
                "message": f"ä»»åŠ¡æ‰§è¡Œå¤±è´¥: {str(e)}"
            }
