import os
import re
import json
import datetime
import torch
import asyncio
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path

from transformers import AutoModelForCausalLM
from .base_agent import BaseAgent, Message
from infrastructure.config.ai_agents import get_ai_agent_config
from utils import log, LogLevel

class AIDrivenDatabaseManageAgent(BaseAgent):
    """AIé©±åŠ¨æ•°æ®åº“ç®¡ç†æ™ºèƒ½ä½“ - è´Ÿè´£è§£æç”¨æˆ·éœ€æ±‚å¹¶è½¬æ¢ä¸ºæ•°æ®åº“æ“ä½œ
    
    æ ¸å¿ƒåŠŸèƒ½:
    1. ç”¨æˆ·éœ€æ±‚è§£æ - ä½¿ç”¨AIæ¨¡å‹ç†è§£ç”¨æˆ·æ„å›¾
    2. æ•°æ®åº“æ“ä½œè½¬æ¢ - å°†è‡ªç„¶è¯­è¨€éœ€æ±‚è½¬æ¢ä¸ºå…·ä½“æ•°æ®åº“æ“ä½œ
    3. çŸ¥è¯†æ£€ç´¢ - ä¸ºåç»­åˆ†ææ¨¡å‹æä¾›æ•°æ®åº“çŸ¥è¯†æŸ¥è¯¢
    4. ä¸ç”¨æˆ·æ²Ÿé€šä»£ç†æ¨¡å‹ä¿æŒä¸€è‡´
    """
    
    def __init__(self):
        super().__init__("db_manage_agent", "AI Database Management Agent")
        
        # AIæ¨¡å‹ç»„ä»¶ - ä¸ç”¨æˆ·æ²Ÿé€šä»£ç†ä¿æŒä¸€è‡´
        self.conversation_model = None
        self.tokenizer = None
        self.used_device = "gpu"
        self.used_device_map = None
        
        # ä»ç»Ÿä¸€é…ç½®è·å–
        self.agent_config = get_ai_agent_config().get_user_communication_agent_config()
        
        # æ¨¡å‹é…ç½®
        self.model_name = self.agent_config.get("model_name", "Qwen/Qwen1.5-7B-Chat")
        
        # ç¡¬ä»¶è¦æ±‚ï¼šä»é…ç½®è¯»å–
        self.max_memory_mb = self.agent_config.get("max_memory_mb", 14336)
        
        # AIæ¨¡å‹çŠ¶æ€
        self.ai_enabled = False
        
        # æ•°æ®åº“æ“ä½œè®°å½•
        self.database_operations = {}
        
        # ä¼šè¯ç®¡ç†
        self.session_memory = {}
    
    async def initialize_data_manage(self, agent_integration=None):
        """åˆå§‹åŒ–AIæ¨¡å‹å’Œä»£ç†é›†æˆ"""
        try:
            self.agent_integration = agent_integration
            await self._initialize_ai_models()
            return True
        except Exception as e:
            log("db_manage_agent", LogLevel.ERROR, f"AIæ•°æ®åº“ç®¡ç†ä»£ç†åˆå§‹åŒ–é”™è¯¯: {e}")
            return False
    
    async def _initialize_ai_models(self):
        """åˆå§‹åŒ–Qwen1.5-7Bæ¨¡å‹ - ä¸ç”¨æˆ·æ²Ÿé€šä»£ç†ä¿æŒä¸€è‡´"""
        try:
            from transformers import pipeline, AutoTokenizer

            log("db_manage_agent", LogLevel.INFO, "ğŸ”§ å¼€å§‹åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†æ¨¡å‹...")
            log("db_manage_agent", LogLevel.INFO, f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")

            cache_dir = get_ai_agent_config().get_model_cache_dir()
            # ç¡®ä¿ç¼“å­˜ç›®å½•æ˜¯ç»å¯¹è·¯å¾„
            if not os.path.isabs(cache_dir):
                cache_dir = os.path.abspath(cache_dir)
            log("db_manage_agent", LogLevel.INFO, f"ğŸ’¾ ç¼“å­˜ç›®å½•: {cache_dir}")

            # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
            os.makedirs(cache_dir, exist_ok=True)

            local_files_only = False
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            model_path = os.path.join(cache_dir, f"models--{self.model_name.replace('/', '--')}")
            # æ£€æŸ¥å¿«ç…§ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºç©º
            snapshots_path = os.path.join(model_path, "snapshots")
            model_files_exist = (
                os.path.exists(model_path) and 
                os.path.exists(snapshots_path) and 
                os.listdir(snapshots_path)
            )

            if model_files_exist:
                local_files_only = True
                log("db_manage_agent", LogLevel.INFO, "ğŸ” æ£€æµ‹åˆ°æœ¬åœ°ç¼“å­˜æ¨¡å‹æ–‡ä»¶ï¼Œå°†ä½¿ç”¨æœ¬åœ°æ–‡ä»¶åŠ è½½")
            else:
                log("db_manage_agent", LogLevel.INFO, "ğŸŒ æœªæ£€æµ‹åˆ°æœ¬åœ°ç¼“å­˜æ¨¡å‹ï¼Œå°†ä»ç½‘ç»œä¸‹è½½")

            # åˆå§‹åŒ–tokenizer
            log("db_manage_agent", LogLevel.INFO, "ğŸ”§ ä½¿ç”¨Qwené…ç½®åŠ è½½tokenizer...")
            if local_files_only and model_files_exist:
                # ä½¿ç”¨æœ¬åœ°è·¯å¾„åŠ è½½tokenizerï¼Œé¿å…ç½‘ç»œè¯·æ±‚
                snapshot_dirs = os.listdir(snapshots_path)
                if snapshot_dirs:
                    model_local_path = os.path.join(snapshots_path, snapshot_dirs[0])
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        model_local_path,
                        cache_dir=cache_dir,
                        trust_remote_code=True,
                        local_files_only=True
                    )
                else:
                    raise Exception("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹å¿«ç…§ç›®å½•")
            else:
                # åœ¨çº¿æ¨¡å¼æˆ–æœ¬åœ°æ–‡ä»¶ä¸å®Œæ•´æ—¶ä½¿ç”¨æ¨¡å‹åç§°
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name, 
                    cache_dir=cache_dir,
                    trust_remote_code=True,
                    local_files_only=local_files_only
                )
            log("db_manage_agent", LogLevel.INFO, "âœ… TokenizeråŠ è½½æˆåŠŸ")

            # é…ç½®tokenizer
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                log("db_manage_agent", LogLevel.INFO, "âœ… Tokenizeré…ç½®æˆåŠŸ")

            # è®¾ç½®padding_side
            self.tokenizer.padding_side = "left"
            log("db_manage_agent", LogLevel.INFO, "ğŸ”§ å·²è®¾ç½®padding_side")

            # åˆå§‹åŒ–å¯¹è¯ç”Ÿæˆpipeline
            log("db_manage_agent", LogLevel.INFO, f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {self.used_device}")

            log("db_manage_agent", LogLevel.INFO, "ğŸ”§ åŠ è½½æ¨¡å‹...")
            if local_files_only and model_files_exist:
                snapshot_dirs = os.listdir(snapshots_path)
                if snapshot_dirs:
                    model_local_path = os.path.join(snapshots_path, snapshot_dirs[0])
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_local_path,
                        cache_dir=cache_dir,
                        trust_remote_code=True,
                        device_map="auto" if self.used_device == "gpu" else None,
                        torch_dtype=torch.float16 if self.used_device == "gpu" else torch.float32,
                        local_files_only=True
                    )
                else:
                    raise Exception("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹å¿«ç…§ç›®å½•")
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    cache_dir=cache_dir,
                    trust_remote_code=True,
                    device_map="auto" if self.used_device == "gpu" else None,
                    torch_dtype=torch.float16 if self.used_device == "gpu" else torch.float32,
                    local_files_only=local_files_only
                )

            log("db_manage_agent", LogLevel.INFO, "ğŸ”¥ é¢„çƒ­æ•°æ®åº“ç®¡ç†æ¨¡å‹...")
            test_prompt = "ä½ å¥½"
            inputs = self.tokenizer(test_prompt, return_tensors="pt")
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=50,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            self.ai_enabled = True
            log("db_manage_agent", LogLevel.INFO, "ğŸ‰ AIæ•°æ®åº“ç®¡ç†æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        except ImportError:
            error_msg = "transformersåº“æœªå®‰è£…,AIåŠŸèƒ½æ— æ³•ä½¿ç”¨"
            log("db_manage_agent", LogLevel.ERROR, f"âŒ {error_msg}")
            raise ImportError(error_msg)
        except Exception as e:
            error_msg = f"AIæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}"
            log("db_manage_agent", LogLevel.ERROR, f"âŒ {error_msg}")
            raise Exception(error_msg)

    async def handle_message(self, message: Message):
        """å¤„ç†æ¶ˆæ¯ - å®ç°BaseAgentçš„æŠ½è±¡æ–¹æ³•"""
        try:
            if message.message_type == "user_requirement":
                await self._process_user_requirement(message.content)
            elif message.message_type == "knowledge_request":
                await self._process_knowledge_request(message.content)
            else:
                log("db_manage_agent", LogLevel.ERROR, f"âŒ ç³»ç»Ÿé”™è¯¯: æ”¶åˆ°æœªçŸ¥æ¶ˆæ¯ç±»å‹: {message.message_type}")
        except Exception as e:
            log("db_manage_agent", LogLevel.ERROR, f"âŒ ç³»ç»Ÿé”™è¯¯: æ¶ˆæ¯å¤„ç†å¼‚å¸¸ ({str(e)})")
            raise

    async def _execute_task_impl(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå…·ä½“ä»»åŠ¡ - å®ç°BaseAgentçš„æŠ½è±¡æ–¹æ³•"""
        # æš‚æ—¶è¿”å›ç©ºç»“æœ
        return {"status": "success", "message": "Task executed successfully"}

    # === å¯¹å¤–æ¥å£æ–¹æ³• ===

    async def user_requirement_interpret(self, user_requirement: Dict[str, Any], session_id: str) -> Dict[str, Any]:
        """
        ç”¨æˆ·éœ€æ±‚è§£ææ¥å£ - æ¥æ”¶ç”¨æˆ·æ²Ÿé€šæ¨¡å‹çš„è¾“å…¥
        
        å‚æ•°:
            user_requirement: JSONæ ¼å¼çš„ç”¨æˆ·éœ€æ±‚
            session_id: å¯¹è¯ä¼šè¯ID
            
        è¿”å›:
            è§£æåçš„æ•°æ®åº“æ“ä½œè§„åˆ’
        """
        log("db_manage_agent", LogLevel.INFO, f"ğŸ“ å¼€å§‹è§£æç”¨æˆ·éœ€æ±‚ï¼Œä¼šè¯ID: {session_id}")
        
        # æš‚æ—¶è¿”å›é»˜è®¤å€¼
        return {
            "status": "success",
            "session_id": session_id,
            "interpreted_operations": [],
            "message": "ç”¨æˆ·éœ€æ±‚è§£æåŠŸèƒ½å¾…å®ç°"
        }

    async def get_knowledge_from_database(self, scan_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        çŸ¥è¯†æ£€ç´¢æ¥å£ - æä¾›ç»™åç»­å¾…å¼€å‘çš„äºŒæ¬¡åˆ†ææ¨¡å‹
        
        å‚æ•°:
            scan_results: JSONæ ¼å¼çš„ä»£ç æ‰«æç»“æœ
            
        è¿”å›:
            ä»æ•°æ®åº“æ£€ç´¢åˆ°çš„ç›¸å…³çŸ¥è¯†
        """
        log("db_manage_agent", LogLevel.INFO, "ğŸ” å¼€å§‹æ£€ç´¢æ•°æ®åº“çŸ¥è¯†")
        
        # æš‚æ—¶è¿”å›é»˜è®¤å€¼
        return {
            "status": "success",
            "knowledge_data": {},
            "message": "çŸ¥è¯†æ£€ç´¢åŠŸèƒ½å¾…å®ç°"
        }

    # === å†…éƒ¨å¤„ç†æ–¹æ³• ===

    async def _process_user_requirement(self, content: Dict[str, Any]):
        """å¤„ç†ç”¨æˆ·éœ€æ±‚æ¶ˆæ¯"""
        user_requirement = content.get("requirement", {})
        session_id = content.get("session_id", "default")
        
        # è°ƒç”¨ç”¨æˆ·éœ€æ±‚è§£ææ¥å£
        result = await self.user_requirement_interpret(user_requirement, session_id)
        
        # è®°å½•æ“ä½œç»“æœ
        if session_id not in self.database_operations:
            self.database_operations[session_id] = []
        
        self.database_operations[session_id].append({
            "timestamp": self._get_current_time(),
            "operation": "user_requirement_interpret",
            "result": result
        })

    async def _process_knowledge_request(self, content: Dict[str, Any]):
        """å¤„ç†çŸ¥è¯†è¯·æ±‚æ¶ˆæ¯"""
        scan_results = content.get("scan_results", {})
        
        # è°ƒç”¨çŸ¥è¯†æ£€ç´¢æ¥å£
        result = await self.get_knowledge_from_database(scan_results)
        
        # è®°å½•æ“ä½œç»“æœ
        if "knowledge_requests" not in self.database_operations:
            self.database_operations["knowledge_requests"] = []
        
        self.database_operations["knowledge_requests"].append({
            "timestamp": self._get_current_time(),
            "operation": "get_knowledge_from_database",
            "result": result
        })

    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        return datetime.datetime.now().isoformat()