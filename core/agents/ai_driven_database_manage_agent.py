import os
import re
import json
import datetime
import torch
import asyncio
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path

from transformers import AutoModelForCausalLM
from .base_agent import BaseAgent, Message
from infrastructure.config.ai_agents import get_ai_agent_config
from infrastructure.database.sqlite.service import DatabaseService
from infrastructure.database.weaviate.service import WeaviateVectorService
from infrastructure.database.vector_sync import (
    IssuePatternSyncService,
    DefaultKnowledgeEncodingAgent,
)
from utils import log, LogLevel

class AIDrivenDatabaseManageAgent(BaseAgent):
    """AIé©±åŠ¨æ•°æ®åº“ç®¡ç†æ™ºèƒ½ä½“ - è´Ÿè´£è§£æç”¨æˆ·éœ€æ±‚å¹¶è½¬æ¢ä¸ºæ•°æ®åº“æ“ä½œ
    
    æ ¸å¿ƒåŠŸèƒ½:
    1. ç”¨æˆ·éœ€æ±‚è§£æ - ä½¿ç”¨AIæ¨¡å‹ç†è§£ç”¨æˆ·æ„å›¾
    2. æ•°æ®åº“æ“ä½œè½¬æ¢ - å°†è‡ªç„¶è¯­è¨€éœ€æ±‚è½¬æ¢ä¸ºå…·ä½“æ•°æ®åº“æ“ä½œ
    3. çŸ¥è¯†æ£€ç´¢ - ä¸ºåç»­åˆ†ææ¨¡å‹æä¾›æ•°æ®åº“çŸ¥è¯†æŸ¥è¯¢
    4. ä¸ç”¨æˆ·æ²Ÿé€šä»£ç†æ¨¡å‹ä¿æŒä¸€è‡´
    """
    
    def __init__(self):
        super().__init__("db_manage_agent", "AI Database Management Agent")
        
        # AIæ¨¡å‹ç»„ä»¶ - ä¸ç”¨æˆ·æ²Ÿé€šä»£ç†ä¿æŒä¸€è‡´
        self.conversation_model = None
        self.tokenizer = None
        self.model = None
        self.used_device = "gpu"
        self.used_device_map = None
        
        # ä»ç»Ÿä¸€é…ç½®è·å–
        self.agent_config = get_ai_agent_config().get_user_communication_agent_config()
        
        # æ¨¡å‹é…ç½®
        self.model_name = self.agent_config.get("model_name", "Qwen/Qwen1.5-7B-Chat")
        
        # ç¡¬ä»¶è¦æ±‚ï¼šä»é…ç½®è¯»å–
        self.max_memory_mb = self.agent_config.get("max_memory_mb", 14336)
        
        # AIæ¨¡å‹çŠ¶æ€
        self.ai_enabled = False
        
        # æ•°æ®åº“æ“ä½œè®°å½•
        self.database_operations = {}
        
        # ä¼šè¯ç®¡ç†
        self.session_memory = {}

        # æ•°æ®å±‚ç»„ä»¶
        self.db_service = DatabaseService()
        self.vector_service = WeaviateVectorService(embed_fn=self._default_embed)
        self.encoding_agent = DefaultKnowledgeEncodingAgent(embed_fn=self._default_embed)
        self.sync_service = IssuePatternSyncService(
            db_service=self.db_service,
            vector_service=self.vector_service,
            agent=self.encoding_agent,
        )

        # å°è¯•è‡ªåŠ¨è¿æ¥ Weaviateï¼ˆä¼˜é›…é™çº§ï¼šè¿æ¥å¤±è´¥æ—¶ä»…ä½¿ç”¨ SQLiteï¼‰
        self._init_weaviate_connection()

    async def initialize_data_manage(self, agent_integration=None):
        """åˆå§‹åŒ–AIæ¨¡å‹å’Œä»£ç†é›†æˆ"""
        try:
            self.agent_integration = agent_integration
            if self.model and self.tokenizer:
                self.ai_enabled = True
                log("db_manage_agent", LogLevel.INFO, "ğŸ”— ä½¿ç”¨å…±äº«æ¨¡å‹å®Œæˆåˆå§‹åŒ–")
                return True
            await self._initialize_ai_models()
            return True
        except Exception as e:
            log("db_manage_agent", LogLevel.ERROR, f"AIæ•°æ®åº“ç®¡ç†ä»£ç†åˆå§‹åŒ–é”™è¯¯: {e}")
            return False

    def _init_weaviate_connection(self) -> None:
        """
        å°è¯•è¿æ¥ Weaviate å‘é‡æ•°æ®åº“ã€‚
        
        è¿æ¥å¤±è´¥æ—¶ä¼˜é›…é™çº§ï¼Œä»…ä½¿ç”¨ SQLite å­˜å‚¨ã€‚
        """
        try:
            connected = self.vector_service.connect(auto_create_schema=True)
            if connected:
                log(
                    "db_manage_agent",
                    LogLevel.INFO,
                    "âœ… Weaviate å‘é‡æ•°æ®åº“å·²è¿æ¥ï¼Œæ”¯æŒè¯­ä¹‰æœç´¢å’Œå»é‡",
                )
            else:
                status = self.vector_service.get_connection_status()
                log(
                    "db_manage_agent",
                    LogLevel.WARNING,
                    f"âš ï¸ Weaviate è¿æ¥å¤±è´¥ï¼Œå°†ä»…ä½¿ç”¨ SQLite å­˜å‚¨: {status.get('error', 'æœªçŸ¥é”™è¯¯')}",
                )
        except Exception as e:
            log(
                "db_manage_agent",
                LogLevel.WARNING,
                f"âš ï¸ Weaviate åˆå§‹åŒ–å¼‚å¸¸ï¼Œå°†ä»…ä½¿ç”¨ SQLite å­˜å‚¨: {e}",
            )

    def set_shared_model(self, model, tokenizer, model_name: Optional[str] = None):
        """æ³¨å…¥å…±äº«æ¨¡å‹ä¸Tokenizerï¼Œé¿å…é‡å¤åŠ è½½"""
        if model is None or tokenizer is None:
            return
        self.model = model
        self.tokenizer = tokenizer
        if model_name:
            self.model_name = model_name
        if self.tokenizer.pad_token is None:
            self.tokenizer.pad_token = self.tokenizer.eos_token
        self.tokenizer.padding_side = "left"
        self.ai_enabled = True
        log("db_manage_agent", LogLevel.INFO, "âœ… å·²æ³¨å…¥å…±äº«æ¨¡å‹/Tokenizer")

    async def stop(self):
        """åœæ­¢æ™ºèƒ½ä½“å¹¶æ¸…ç†èµ„æº"""
        # å…³é—­ Weaviate è¿æ¥
        if self.vector_service and self.vector_service.is_connected():
            try:
                self.vector_service.disconnect()
                log("db_manage_agent", LogLevel.INFO, "ğŸ”Œ Weaviate è¿æ¥å·²å…³é—­")
            except Exception as e:
                log("db_manage_agent", LogLevel.WARNING, f"âš ï¸ å…³é—­ Weaviate è¿æ¥æ—¶å‡ºé”™: {e}")
        
        # è°ƒç”¨çˆ¶ç±»çš„ stop æ–¹æ³•
        await super().stop()
    
    async def _initialize_ai_models(self):
        """åˆå§‹åŒ–Qwen1.5-7Bæ¨¡å‹ - ä¸ç”¨æˆ·æ²Ÿé€šä»£ç†ä¿æŒä¸€è‡´"""
        try:
            from transformers import pipeline, AutoTokenizer

            log("db_manage_agent", LogLevel.INFO, "ğŸ”§ å¼€å§‹åˆå§‹åŒ–æ•°æ®åº“ç®¡ç†æ¨¡å‹...")
            log("db_manage_agent", LogLevel.INFO, f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")

            cache_dir = get_ai_agent_config().get_model_cache_dir()
            # ç¡®ä¿ç¼“å­˜ç›®å½•æ˜¯ç»å¯¹è·¯å¾„
            if not os.path.isabs(cache_dir):
                cache_dir = os.path.abspath(cache_dir)
            log("db_manage_agent", LogLevel.INFO, f"ğŸ’¾ ç¼“å­˜ç›®å½•: {cache_dir}")

            # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
            os.makedirs(cache_dir, exist_ok=True)

            local_files_only = False
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            model_path = os.path.join(cache_dir, f"models--{self.model_name.replace('/', '--')}")
            # æ£€æŸ¥å¿«ç…§ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºç©º
            snapshots_path = os.path.join(model_path, "snapshots")
            model_files_exist = (
                os.path.exists(model_path) and 
                os.path.exists(snapshots_path) and 
                os.listdir(snapshots_path)
            )

            if model_files_exist:
                local_files_only = True
                log("db_manage_agent", LogLevel.INFO, "ğŸ” æ£€æµ‹åˆ°æœ¬åœ°ç¼“å­˜æ¨¡å‹æ–‡ä»¶ï¼Œå°†ä½¿ç”¨æœ¬åœ°æ–‡ä»¶åŠ è½½")
            else:
                log("db_manage_agent", LogLevel.INFO, "ğŸŒ æœªæ£€æµ‹åˆ°æœ¬åœ°ç¼“å­˜æ¨¡å‹ï¼Œå°†ä»ç½‘ç»œä¸‹è½½")

            # åˆå§‹åŒ–tokenizer
            log("db_manage_agent", LogLevel.INFO, "ğŸ”§ ä½¿ç”¨Qwené…ç½®åŠ è½½tokenizer...")
            if local_files_only and model_files_exist:
                # ä½¿ç”¨æœ¬åœ°è·¯å¾„åŠ è½½tokenizerï¼Œé¿å…ç½‘ç»œè¯·æ±‚
                snapshot_dirs = os.listdir(snapshots_path)
                if snapshot_dirs:
                    model_local_path = os.path.join(snapshots_path, snapshot_dirs[0])
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        model_local_path,
                        cache_dir=cache_dir,
                        trust_remote_code=True,
                        local_files_only=True
                    )
                else:
                    raise Exception("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹å¿«ç…§ç›®å½•")
            else:
                # åœ¨çº¿æ¨¡å¼æˆ–æœ¬åœ°æ–‡ä»¶ä¸å®Œæ•´æ—¶ä½¿ç”¨æ¨¡å‹åç§°
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name, 
                    cache_dir=cache_dir,
                    trust_remote_code=True,
                    local_files_only=local_files_only
                )
            log("db_manage_agent", LogLevel.INFO, "âœ… TokenizeråŠ è½½æˆåŠŸ")

            # é…ç½®tokenizer
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                log("db_manage_agent", LogLevel.INFO, "âœ… Tokenizeré…ç½®æˆåŠŸ")

            # è®¾ç½®padding_side
            self.tokenizer.padding_side = "left"
            log("db_manage_agent", LogLevel.INFO, "ğŸ”§ å·²è®¾ç½®padding_side")

            # åˆå§‹åŒ–å¯¹è¯ç”Ÿæˆpipeline
            log("db_manage_agent", LogLevel.INFO, f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {self.used_device}")

            log("db_manage_agent", LogLevel.INFO, "ğŸ”§ åŠ è½½æ¨¡å‹...")
            if local_files_only and model_files_exist:
                snapshot_dirs = os.listdir(snapshots_path)
                if snapshot_dirs:
                    model_local_path = os.path.join(snapshots_path, snapshot_dirs[0])
                    self.model = AutoModelForCausalLM.from_pretrained(
                        model_local_path,
                        cache_dir=cache_dir,
                        trust_remote_code=True,
                        device_map="auto" if self.used_device == "gpu" else None,
                        torch_dtype=torch.float16 if self.used_device == "gpu" else torch.float32,
                        local_files_only=True
                    )
                else:
                    raise Exception("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹å¿«ç…§ç›®å½•")
            else:
                self.model = AutoModelForCausalLM.from_pretrained(
                    self.model_name,
                    cache_dir=cache_dir,
                    trust_remote_code=True,
                    device_map="auto" if self.used_device == "gpu" else None,
                    torch_dtype=torch.float16 if self.used_device == "gpu" else torch.float32,
                    local_files_only=local_files_only
                )

            log("db_manage_agent", LogLevel.INFO, "ğŸ”¥ é¢„çƒ­æ•°æ®åº“ç®¡ç†æ¨¡å‹...")
            test_prompt = "ä½ å¥½"
            inputs = self.tokenizer(test_prompt, return_tensors="pt")
            with torch.no_grad():
                outputs = self.model.generate(
                    inputs.input_ids,
                    max_new_tokens=50,
                    do_sample=False,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            self.ai_enabled = True
            log("db_manage_agent", LogLevel.INFO, "ğŸ‰ AIæ•°æ®åº“ç®¡ç†æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        except ImportError:
            error_msg = "transformersåº“æœªå®‰è£…,AIåŠŸèƒ½æ— æ³•ä½¿ç”¨"
            log("db_manage_agent", LogLevel.ERROR, f"âŒ {error_msg}")
            raise ImportError(error_msg)
        except Exception as e:
            error_msg = f"AIæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}"
            log("db_manage_agent", LogLevel.ERROR, f"âŒ {error_msg}")
            raise Exception(error_msg)

    async def handle_message(self, message: Message):
        """å¤„ç†æ¶ˆæ¯ - å®ç°BaseAgentçš„æŠ½è±¡æ–¹æ³•"""
        try:
            if message.message_type == "user_requirement":
                await self._process_user_requirement(message.content)
            elif message.message_type == "knowledge_request":
                await self._process_knowledge_request(message.content)
            else:
                log("db_manage_agent", LogLevel.ERROR, f"âŒ ç³»ç»Ÿé”™è¯¯: æ”¶åˆ°æœªçŸ¥æ¶ˆæ¯ç±»å‹: {message.message_type}")
        except Exception as e:
            log("db_manage_agent", LogLevel.ERROR, f"âŒ ç³»ç»Ÿé”™è¯¯: æ¶ˆæ¯å¤„ç†å¼‚å¸¸ ({str(e)})")
            raise

    async def receive_message(self, message: Message):
        """
        è¦†å†™æ¥æ”¶å…¥å£ï¼šåœ¨å…¥é˜Ÿå‰åšä¸€æ¬¡â€œåœºæ™¯åˆ†æµ + å…¥å‚è§„èŒƒåŒ–â€ã€‚

        åœºæ™¯ï¼š
        - user_requirement: æœŸæœ› content = {"requirement": {...}, "session_id": "..."}
          ä¸ºäº†ä¾¿äºè°ƒç”¨æ–¹ï¼Œå…è®¸ç›´æ¥ä¼  {"db_tasks":[...], "session_id":"..."}ï¼Œè¿™é‡Œä¼šè‡ªåŠ¨åŒ…è£…åˆ° requirementã€‚
        - knowledge_request: æœŸæœ› content = {"scan_results": {...}}
        """
        try:
            if message.message_type == "user_requirement":
                if isinstance(message.content, dict) and "requirement" not in message.content:
                    # å…è®¸ç›´æ¥æŠŠ requirement ä½œä¸º content ä¼ å…¥
                    session_id = message.content.get("session_id", "default")
                    message.content = {
                        "requirement": {k: v for k, v in message.content.items() if k != "session_id"},
                        "session_id": session_id,
                    }
            elif message.message_type == "knowledge_request":
                # å…è®¸ content ç›´æ¥å°±æ˜¯ scan_results
                if isinstance(message.content, dict) and "scan_results" not in message.content:
                    message.content = {"scan_results": message.content}
            else:
                log(
                    "db_manage_agent",
                    LogLevel.WARNING,
                    f"âš ï¸ æ”¶åˆ°æœªæ”¯æŒçš„æ¶ˆæ¯ç±»å‹ {message.message_type}ï¼Œå°†å¿½ç•¥è¯¥æ¶ˆæ¯",
                )
                return
        except Exception as e:
            log("db_manage_agent", LogLevel.ERROR, f"âŒ è§„èŒƒåŒ–æ¶ˆæ¯å¤±è´¥: {e}")
            return

        await super().receive_message(message)

    async def _execute_task_impl(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œå…·ä½“ä»»åŠ¡ - å®ç°BaseAgentçš„æŠ½è±¡æ–¹æ³•"""
        # æš‚æ—¶è¿”å›ç©ºç»“æœ
        return {"status": "success", "message": "Task executed successfully"}

    # === å¯¹å¤–æ¥å£æ–¹æ³• ===

    async def user_requirement_interpret(
        self,
        user_requirement: Optional[Dict[str, Any]] = None,
        session_id: str = "default",
    ) -> Dict[str, Any]:
        """
        ç”¨æˆ·éœ€æ±‚è§£ææ¥å£
        - å…¥å‚ user_requirement ä¸­åŒ…å«è‡ªç„¶è¯­è¨€ db_tasks
        - é€šè¿‡ Qwen + ç‰¹å®š Prompt ç¿»è¯‘ä¸ºç»“æ„åŒ– DB æ“ä½œåæ‰§è¡Œ
        """
        log("db_manage_agent", LogLevel.INFO, f"ğŸ“ å¼€å§‹è§£æç”¨æˆ·éœ€æ±‚ï¼Œä¼šè¯ID: {session_id}")
        
        raw_text = self._extract_raw_text(user_requirement)
        tasks = self._normalize_db_tasks(user_requirement)
        forced_tasks = self._extract_forced_tasks(user_requirement)
        mode = "write"
        if forced_tasks:
            tasks = forced_tasks
            mode = self._infer_mode_from_tasks(tasks, session_id) or "delete"
            log("db_manage_agent", LogLevel.INFO, f"ğŸ§­ ä½¿ç”¨å¼ºåˆ¶ä»»åŠ¡ mode={mode}")
        else:
            if raw_text:
                mode = await self._classify_db_mode(raw_text)
            log("db_manage_agent", LogLevel.INFO, f"ğŸ§­ è¯†åˆ«æ•°æ®åº“æ„å›¾ mode={mode}")
        # ç¨³å®šæ€§ä¼˜å…ˆï¼šä¸¤é˜¶æ®µç”Ÿæˆï¼ˆå…ˆ issue_patternï¼Œå† session+issueï¼‰
        if tasks and mode == "write" and not forced_tasks:
            issue_pattern_tasks = await self._translate_tasks_with_llm(
                tasks, session_id=session_id, variant="issue_pattern", raw_text=raw_text
            )
            issue_pattern_tasks = self._filter_tasks_by_targets(
                issue_pattern_tasks, {"issue_pattern", "issue_patterns"}
            )
            issue_pattern_task = issue_pattern_tasks[0] if issue_pattern_tasks else None
            if issue_pattern_tasks:
                self._log_llm_tasks("é˜¶æ®µ1(issue_pattern)", issue_pattern_tasks)

            session_issue_tasks = await self._translate_tasks_with_llm(
                tasks,
                session_id=session_id,
                variant="session_issue",
                raw_text=raw_text,
                issue_pattern=issue_pattern_task,
            )
            session_issue_tasks = self._filter_tasks_by_targets(
                session_issue_tasks, {"review_session", "review_sessions", "curated_issue", "curated_issues"}
            )
            if session_issue_tasks:
                self._log_llm_tasks("é˜¶æ®µ2(session_issue)", session_issue_tasks)

            combined = []
            if issue_pattern_tasks:
                combined.extend(issue_pattern_tasks)
            if session_issue_tasks:
                combined.extend(session_issue_tasks)
            combined = self._dedupe_tasks(combined)

            if combined:
                tasks = combined
                try:
                    pretty_tasks = json.dumps(tasks, ensure_ascii=False, indent=2)
                except Exception:
                    pretty_tasks = str(tasks)
                log("db_manage_agent", LogLevel.INFO, f"ğŸ“œ LLM è§£æåçš„ç»“æ„åŒ–ä»»åŠ¡:\n{pretty_tasks}")
            else:
                log("db_manage_agent", LogLevel.WARNING, "âš ï¸ LLM æœªè¿”å›å¯è§£æçš„ db_tasks ç»“æ„ï¼Œæ”¾å¼ƒæ‰§è¡Œ")
        elif tasks and mode in ("query", "delete") and not forced_tasks:
            tasks = await self._translate_tasks_with_llm(
                tasks, session_id=session_id, variant="read_delete", raw_text=raw_text
            )
            tasks = self._dedupe_tasks(tasks)
            if tasks:
                try:
                    pretty_tasks = json.dumps(tasks, ensure_ascii=False, indent=2)
                except Exception:
                    pretty_tasks = str(tasks)
                log("db_manage_agent", LogLevel.INFO, f"ğŸ“œ LLM è§£æåçš„ç»“æ„åŒ–ä»»åŠ¡:\n{pretty_tasks}")
            else:
                log("db_manage_agent", LogLevel.WARNING, "âš ï¸ LLM æœªè¿”å›å¯è§£æçš„ db_tasks ç»“æ„ï¼Œæ”¾å¼ƒæ‰§è¡Œ")
            if mode == "query" and not tasks:
                tasks = [
                    {"target": "review_session", "action": "query", "data": {}},
                    {"target": "curated_issue", "action": "query", "data": {}},
                    {"target": "issue_pattern", "action": "query", "data": {}},
                ]

        inferred_mode = self._infer_mode_from_tasks(tasks, session_id)
        if inferred_mode in ("query", "delete") and inferred_mode != mode:
            log(
                "db_manage_agent",
                LogLevel.INFO,
                f"ğŸ§­ åŸºäºç»“æ„åŒ–ä»»åŠ¡æ¨æ–­ mode={inferred_mode}ï¼Œè¦†ç›–åŸå§‹ mode={mode}",
            )
            mode = inferred_mode
            if mode == "query":
                tasks = self._filter_tasks_by_actions(tasks, {"query"}, session_id)
                if not tasks:
                    tasks = [
                        {"target": "review_session", "action": "query", "data": {}},
                        {"target": "curated_issue", "action": "query", "data": {}},
                        {"target": "issue_pattern", "action": "query", "data": {}},
                    ]
            else:
                tasks = self._filter_tasks_by_actions(tasks, {"delete", "delete_all"}, session_id)

        if mode == "write":
            tasks = self._ensure_three_table_tasks(tasks, raw_text, session_id)
            if not forced_tasks:
                tasks = self._apply_non_llm_defaults(tasks, raw_text, session_id)

        if self._requires_delete_all_confirm(tasks, session_id):
            log(
                "db_manage_agent",
                LogLevel.WARNING,
                "âš ï¸ æ£€æµ‹åˆ° delete_all ä½†ç¼ºå°‘ confirm=trueï¼Œå·²é˜»æ­¢æ‰§è¡Œ",
            )
            return {
                "status": "need_confirm",
                "session_id": session_id,
                "results": [],
                "pending_action": "delete_all",
                "pending_tasks": tasks,
                "message": "åˆ é™¤å…¨éƒ¨æ•°æ®éœ€è¦æ˜ç¡®ç¡®è®¤ï¼Œè¯·ç¡®è®¤åå†æ‰§è¡Œè¯¥æ“ä½œã€‚",
            }

        if not tasks:
            return {
                "status": "noop",
                "session_id": session_id,
                "results": [],
                "message": "æœªè¯†åˆ«åˆ°å¯æ‰§è¡Œçš„æ•°æ®åº“ä»»åŠ¡",
            }

        # å±•å¼€ target="all" çš„ä»»åŠ¡ä¸ºä¸‰å¼ è¡¨çš„ä»»åŠ¡
        tasks = self._expand_all_target_tasks(tasks)

        # æŒ‰ç‰¹å®šé¡ºåºæ‰§è¡Œï¼šå…ˆ issue_patternï¼Œå† review_sessionï¼Œæœ€å curated_issue
        # è¿™æ ·å¯ä»¥ç¡®ä¿ curated_issue å†™å…¥æ—¶èƒ½å…³è”åˆ°åˆšåˆ›å»ºçš„ pattern_id å’Œ session_db_id
        ordered_tasks = self._order_tasks_for_execution(tasks)
        created_pattern_id = None
        created_session_db_id = None

        results: List[Dict[str, Any]] = []
        for task in ordered_tasks:
            try:
                # åœ¨æ‰§è¡Œ curated_issue å‰ï¼Œè‡ªåŠ¨å›å¡«å·²åˆ›å»ºçš„ pattern_id å’Œ session_db_id
                target = str(task.get("target") or task.get("table") or "").lower()
                target_normalized = {
                    "curated_issues": "curated_issue", "curated_issue": "curated_issue",
                    "issue": "curated_issue",
                }.get(target, target)
                if target_normalized == "curated_issue" and isinstance(task.get("data"), dict):
                    if created_pattern_id and not task["data"].get("pattern_id"):
                        task["data"]["pattern_id"] = created_pattern_id
                    if created_session_db_id and not task["data"].get("session_id"):
                        task["data"]["session_id"] = created_session_db_id

                result = await self._handle_single_db_task(task, session_id)
                results.append({"task": task, "status": "success", "result": result})

                # æ”¶é›†å·²åˆ›å»ºçš„ pattern_id å’Œ session_db_idï¼Œä¾›åç»­ curated_issue å…³è”
                if isinstance(result, dict):
                    if result.get("pattern_id") and not created_pattern_id:
                        created_pattern_id = result["pattern_id"]
                    if result.get("session_db_id") and not created_session_db_id:
                        created_session_db_id = result["session_db_id"]
            except Exception as e:
                log("db_manage_agent", LogLevel.ERROR, f"âŒ å¤„ç†æ•°æ®åº“ä»»åŠ¡å¤±è´¥: {e}")
                results.append({"task": task, "status": "failed", "error": str(e)})

        overall_status = (
            "success"
            if all(item["status"] == "success" for item in results)
            else "partial"
        )
        summary_table = ""
        if any(
            self._normalize_task(
                str(item.get("task", {}).get("action", "")),
                str(item.get("task", {}).get("target", "")),
                item.get("task", {}).get("data", {}) if isinstance(item.get("task", {}).get("data"), dict) else {},
                session_id,
            )[0]
            != "query"
            for item in results
        ):
            summary_table = self._build_db_summary_table(results)
            if summary_table:
                log("db_manage_agent", LogLevel.INFO, f"ğŸ“‹ æ•°æ®åº“å†™å…¥æ‘˜è¦:\n{summary_table}")

        query_tables = self._build_db_query_tables(results)
        if query_tables:
            for table_name, table_text in query_tables.items():
                log("db_manage_agent", LogLevel.INFO, f"ğŸ” æ•°æ®åº“æŸ¥è¯¢ç»“æœ ({table_name}):\n{table_text}")

        return {
            "status": overall_status,
            "session_id": session_id,
            "results": results,
            "message": "æ•°æ®åº“ä»»åŠ¡æ‰§è¡Œå®Œæˆ",
            "summary_table": summary_table,
            "query_tables": query_tables,
        }

    async def get_knowledge_from_database(self, scan_results: Dict[str, Any]) -> Dict[str, Any]:
        """
        çŸ¥è¯†æ£€ç´¢æ¥å£ - æä¾›ç»™åç»­å¾…å¼€å‘çš„äºŒæ¬¡åˆ†ææ¨¡å‹
        
        å‚æ•°:
            scan_results: JSONæ ¼å¼çš„ä»£ç æ‰«æç»“æœ
            
        è¿”å›:
            ä»æ•°æ®åº“æ£€ç´¢åˆ°çš„ç›¸å…³çŸ¥è¯†
        """
        log("db_manage_agent", LogLevel.INFO, "ğŸ” å¼€å§‹æ£€ç´¢æ•°æ®åº“çŸ¥è¯†")
        
        # æš‚æ—¶è¿”å›é»˜è®¤å€¼
        return {
            "status": "success",
            "knowledge_data": {},
            "message": "çŸ¥è¯†æ£€ç´¢åŠŸèƒ½å¾…å®ç°"
        }

    # === å†…éƒ¨å¤„ç†æ–¹æ³• ===

    async def _process_user_requirement(self, content: Dict[str, Any]):
        """å¤„ç†ç”¨æˆ·éœ€æ±‚æ¶ˆæ¯"""
        user_requirement = content.get("requirement", {})
        session_id = content.get("session_id", "default")
        
        # è°ƒç”¨ç”¨æˆ·éœ€æ±‚è§£ææ¥å£
        result = await self.user_requirement_interpret(user_requirement, session_id)
        
        # è®°å½•æ“ä½œç»“æœ
        if session_id not in self.database_operations:
            self.database_operations[session_id] = []
        
        self.database_operations[session_id].append({
            "timestamp": self._get_current_time(),
            "operation": "user_requirement_interpret",
            "result": result
        })

    async def _process_knowledge_request(self, content: Dict[str, Any]):
        """å¤„ç†çŸ¥è¯†è¯·æ±‚æ¶ˆæ¯"""
        scan_results = content.get("scan_results", {})
        
        # è°ƒç”¨çŸ¥è¯†æ£€ç´¢æ¥å£
        result = await self.get_knowledge_from_database(scan_results)
        
        # è®°å½•æ“ä½œç»“æœ
        if "knowledge_requests" not in self.database_operations:
            self.database_operations["knowledge_requests"] = []
        
        self.database_operations["knowledge_requests"].append({
            "timestamp": self._get_current_time(),
            "operation": "get_knowledge_from_database",
            "result": result
        })

    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´å­—ç¬¦ä¸²"""
        return datetime.datetime.now().isoformat()

    # ================== å†…éƒ¨è¾…åŠ©æ–¹æ³• ================== #
    def _normalize_db_tasks(
        self,
        user_requirement: Optional[Dict[str, Any]],
    ) -> List[Dict[str, Any]]:
        """
        ä»…ä» user_requirement æå–è‡ªç„¶è¯­è¨€ db_tasksã€‚
        """
        if isinstance(user_requirement, dict):
            raw_text = user_requirement.get("raw_text")
            if isinstance(raw_text, str) and raw_text.strip():
                return [{"description": raw_text.strip()}]
        return []

    def _extract_raw_text(self, user_requirement: Optional[Dict[str, Any]]) -> str:
        if isinstance(user_requirement, dict):
            raw_text = user_requirement.get("raw_text")
            if isinstance(raw_text, str) and raw_text.strip():
                return raw_text.strip()
            tasks = user_requirement.get("db_tasks") or []
            if isinstance(tasks, list) and tasks:
                desc = tasks[0].get("description") if isinstance(tasks[0], dict) else ""
                if isinstance(desc, str) and desc.strip():
                    return desc.strip()
        return ""

    def _extract_forced_tasks(self, user_requirement: Optional[Dict[str, Any]]) -> List[Dict[str, Any]]:
        if not isinstance(user_requirement, dict):
            return []
        forced = user_requirement.get("forced_tasks")
        if not forced:
            return []
        if not isinstance(forced, list):
            forced = [forced]
        return [task for task in forced if isinstance(task, dict)]

    def _filter_tasks_by_targets(
        self, tasks: List[Dict[str, Any]], targets: set
    ) -> List[Dict[str, Any]]:
        filtered = []
        for task in tasks or []:
            if not isinstance(task, dict):
                continue
            target = str(task.get("target") or task.get("table") or "").lower()
            if target in targets:
                filtered.append(task)
        return filtered

    def _dedupe_tasks(self, tasks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        seen = set()
        deduped = []
        for task in tasks or []:
            if not isinstance(task, dict):
                continue
            key = json.dumps(task, ensure_ascii=False, sort_keys=True)
            if key in seen:
                continue
            seen.add(key)
            deduped.append(task)
        return deduped

    def _requires_delete_all_confirm(
        self, tasks: List[Dict[str, Any]], session_id: str
    ) -> bool:
        for task in tasks or []:
            if not isinstance(task, dict):
                continue
            action = str(task.get("action") or task.get("op") or task.get("type") or "").lower()
            target = str(task.get("target") or task.get("table") or task.get("object") or "").lower()
            data = task.get("data") if isinstance(task.get("data"), dict) else {}
            normalized_action, _, normalized_data = self._normalize_task(
                action, target, data, session_id
            )
            if normalized_action == "delete_all" and normalized_data.get("confirm") is not True:
                return True
            if normalized_action == "delete" and self._is_delete_all_candidate(
                normalized_data, target
            ):
                return True
        return False

    def _is_delete_all_candidate(self, data: Dict[str, Any], target: str) -> bool:
        if not data:
            return True
        id_keys = {"id", "pattern_id", "issue_id", "session_db_id"}
        if any(data.get(k) for k in id_keys):
            return False
        # delete æ“ä½œç›®å‰ä¸æ”¯æŒæŒ‰å­—æ®µè¿‡æ»¤ï¼Œç¼ºå°‘ id ç­‰åŒäºå…¨åˆ æ„å›¾
        return True

    def _infer_mode_from_tasks(self, tasks: List[Dict[str, Any]], session_id: str) -> str:
        if not tasks:
            return "write"
        has_query = False
        has_delete = False
        for task in tasks:
            if not isinstance(task, dict):
                continue
            action = str(task.get("action") or task.get("op") or task.get("type") or "").lower()
            target = str(task.get("target") or task.get("table") or task.get("object") or "").lower()
            data = task.get("data") if isinstance(task.get("data"), dict) else {}
            normalized_action, _, _ = self._normalize_task(action, target, data, session_id)
            if normalized_action in ("delete", "delete_all"):
                has_delete = True
            elif normalized_action == "query":
                has_query = True
        if has_delete:
            return "delete"
        if has_query:
            return "query"
        return "write"

    def _filter_tasks_by_actions(
        self,
        tasks: List[Dict[str, Any]],
        allowed_actions: set,
        session_id: str,
    ) -> List[Dict[str, Any]]:
        filtered: List[Dict[str, Any]] = []
        for task in tasks or []:
            if not isinstance(task, dict):
                continue
            action = str(task.get("action") or task.get("op") or task.get("type") or "").lower()
            target = str(task.get("target") or task.get("table") or task.get("object") or "").lower()
            data = task.get("data") if isinstance(task.get("data"), dict) else {}
            normalized_action, _, _ = self._normalize_task(action, target, data, session_id)
            if normalized_action in allowed_actions:
                filtered.append(task)
        return filtered

    def _ensure_three_table_tasks(
        self, tasks: List[Dict[str, Any]], raw_text: str, session_id: str
    ) -> List[Dict[str, Any]]:
        """ç¡®ä¿æ¯æ¬¡éƒ½åŒ…å« review_session / curated_issue / issue_pattern ä¸‰ç±»ä»»åŠ¡ã€‚"""
        normalized = []
        seen = set()
        target_map = {
            "review_sessions": "review_session",
            "review_session": "review_session",
            "curated_issues": "curated_issue",
            "curated_issue": "curated_issue",
            "issue_patterns": "issue_pattern",
            "issue_pattern": "issue_pattern",
        }
        for task in tasks or []:
            if not isinstance(task, dict):
                continue
            target = str(task.get("target") or task.get("table") or "").lower()
            target = target_map.get(target, target)
            if target:
                seen.add(target)
            normalized.append(task)

        def _default_review_session():
            return {
                "target": "review_session",
                "action": "upsert",
                "data": {
                    "session_id": session_id,
                    "user_message": raw_text or "",
                    "code_directory": "",
                    "status": "open",
                    "code_patch": "",
                    "git_commit": "",
                },
            }

        def _default_curated_issue():
            return {
                "target": "curated_issue",
                "action": "upsert",
                "data": {
                    "session_id": session_id,
                    "pattern_id": "",
                    "project_path": "",
                    "file_path": "",
                    "start_line": 0,
                    "end_line": 0,
                    "code_snippet": "",
                    "problem_phenomenon": raw_text or "",
                    "root_cause": "",
                    "solution": "",
                    "severity": "medium",
                    "status": "open",
                },
            }

        def _default_issue_pattern():
            return {
                "target": "issue_pattern",
                "action": "upsert",
                "data": {
                    "error_type": "general",
                    "severity": "medium",
                    "language": "",
                    "framework": "",
                    "error_description": raw_text or "",
                    "problematic_pattern": raw_text or "",
                    "solution": "",
                    "file_pattern": "",
                    "class_pattern": "",
                    "tags": "",
                    "status": "active",
                },
            }

        if "review_session" not in seen:
            normalized.append(_default_review_session())
        if "curated_issue" not in seen:
            normalized.append(_default_curated_issue())
        if "issue_pattern" not in seen:
            normalized.append(_default_issue_pattern())
        return normalized

    def _apply_non_llm_defaults(
        self, tasks: List[Dict[str, Any]], raw_text: str, session_id: str
    ) -> List[Dict[str, Any]]:
        """å¡«å……ç³»ç»Ÿè‡ªç®¡ç†å­—æ®µï¼ˆä¸ä¾èµ–å¤§æ¨¡å‹æ¨æ–­ï¼‰ï¼Œè¦†ç›–/è¡¥é½å†™å…¥æ•°æ®ã€‚

        ç³»ç»Ÿè‡ªç®¡ç†å­—æ®µè§„åˆ™ï¼ˆå‚è€ƒ todoList.txt å®šä¹‰ï¼‰ï¼š
        - ReviewSession: id(è‡ªå¢), session_id(ç³»ç»Ÿ), user_message(ç³»ç»Ÿ), status(ç³»ç»Ÿ,é»˜è®¤open), created_at, updated_at
        - CuratedIssue: id(è‡ªå¢), session_id(ç³»ç»Ÿ), pattern_id(ç³»ç»Ÿ), severity(ç³»ç»Ÿ,é»˜è®¤medium), status(ç³»ç»Ÿ,é»˜è®¤open), created_at, updated_at
        - IssuePattern: id(è‡ªå¢), severity(ç³»ç»Ÿ,é»˜è®¤medium), status(ç³»ç»Ÿ,é»˜è®¤active), created_at, updated_at
        """
        if not tasks:
            return tasks
        for task in tasks:
            if not isinstance(task, dict):
                continue
            action = str(task.get("action") or task.get("op") or task.get("type") or "").lower()
            if action in ("query", "delete", "delete_all"):
                continue
            target = str(task.get("target") or task.get("table") or "").lower()
            data = task.get("data") if isinstance(task.get("data"), dict) else {}

            # --- ç§»é™¤ LLM ä¸åº”è¾“å‡ºçš„ç³»ç»Ÿè‡ªç®¡ç†å­—æ®µï¼ˆé˜²æ­¢ LLM é”™è¯¯è¦†ç›–ï¼‰ ---
            for sys_field in ("id", "created_at", "updated_at"):
                data.pop(sys_field, None)

            if target in ("review_session", "review_sessions", "session"):
                # ç³»ç»Ÿè‡ªç®¡ç†å­—æ®µï¼šsession_id, user_message, status
                data["session_id"] = session_id
                data["user_message"] = raw_text or ""
                data["status"] = data.get("status", "open")
                # LLM æ¨æ–­å­—æ®µå…œåº•
                data.setdefault("code_directory", "")
                data.setdefault("code_patch", "")
                data.setdefault("git_commit", "")
                task["data"] = data
                continue

            if target in ("curated_issue", "curated_issues", "issue"):
                # ç³»ç»Ÿè‡ªç®¡ç†å­—æ®µï¼šsession_id, pattern_id, severity, status
                data["session_id"] = session_id
                # pattern_id å°†åœ¨æ‰§è¡Œé˜¶æ®µè‡ªåŠ¨å…³è”ï¼ˆè§ _link_pattern_id_to_curated_issuesï¼‰
                data.setdefault("pattern_id", "")
                data["severity"] = data.get("severity", "medium")
                data["status"] = data.get("status", "open")
                # LLM æ¨æ–­å­—æ®µå…œåº•
                data.setdefault("project_path", "")
                data.setdefault("file_path", "")
                data.setdefault("start_line", 0)
                data.setdefault("end_line", 0)
                data.setdefault("code_snippet", "")
                data.setdefault("problem_phenomenon", raw_text or "")
                data.setdefault("root_cause", "")
                data.setdefault("solution", "")
                task["data"] = data
                continue

            if target in ("issue_pattern", "issue_patterns", "issuepattern", "pattern"):
                # ç³»ç»Ÿè‡ªç®¡ç†å­—æ®µï¼šseverity, status
                data["severity"] = data.get("severity", "medium")
                data["status"] = data.get("status", "active")
                # LLM æ¨æ–­å­—æ®µå…œåº•
                data.setdefault("error_type", "general")
                data.setdefault("error_description", raw_text or "")
                data.setdefault("problematic_pattern", raw_text or "")
                data.setdefault("solution", "")
                data.setdefault("title", "")
                data.setdefault("language", "")
                data.setdefault("framework", "")
                data.setdefault("file_pattern", "")
                data.setdefault("class_pattern", "")
                data.setdefault("tags", "")
                task["data"] = data
                continue
        return tasks

    def _expand_all_target_tasks(
        self, tasks: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """å±•å¼€ target="all" çš„ä»»åŠ¡ä¸ºä¸‰å¼ è¡¨çš„ä»»åŠ¡ã€‚
        
        å½“ LLM è¾“å‡º {"target": "all", "action": "delete_all", ...} æ—¶ï¼Œ
        è‡ªåŠ¨å±•å¼€ä¸ºä¸‰æ¡ä»»åŠ¡ï¼Œåˆ†åˆ«é’ˆå¯¹ issue_patternã€review_sessionã€curated_issueã€‚
        """
        expanded: List[Dict[str, Any]] = []
        all_tables = ["issue_pattern", "review_session", "curated_issue"]
        
        for task in tasks or []:
            if not isinstance(task, dict):
                continue
            target = str(task.get("target") or task.get("table") or "").lower()
            
            if target in ("all", "*", "all_tables"):
                # å±•å¼€ä¸ºä¸‰å¼ è¡¨çš„ä»»åŠ¡
                action = task.get("action", "")
                data = task.get("data", {}) if isinstance(task.get("data"), dict) else {}
                
                log(
                    "db_manage_agent",
                    LogLevel.INFO,
                    f"ğŸ”„ å±•å¼€ target=all ä¸ºä¸‰å¼ è¡¨çš„ä»»åŠ¡ action={action}",
                )
                
                for table in all_tables:
                    expanded.append({
                        "target": table,
                        "action": action,
                        "data": dict(data),  # å¤åˆ¶ dataï¼Œé¿å…å…±äº«å¼•ç”¨
                    })
            else:
                expanded.append(task)
        
        return expanded

    def _order_tasks_for_execution(
        self, tasks: List[Dict[str, Any]]
    ) -> List[Dict[str, Any]]:
        """æŒ‰æ‰§è¡Œä¼˜å…ˆçº§æ’åºä»»åŠ¡ï¼šissue_pattern > review_session > curated_issueã€‚
        
        è¿™æ ·å¯ä»¥ç¡®ä¿ï¼š
        1. issue_pattern å…ˆåˆ›å»ºï¼Œè·å¾— pattern_id
        2. review_session åˆ›å»ºï¼Œè·å¾— session_db_id
        3. curated_issue æœ€ååˆ›å»ºï¼Œå¯ä»¥å…³è” pattern_id å’Œ session_db_id
        """
        target_priority = {
            "issue_pattern": 0, "issue_patterns": 0, "issuepattern": 0, "pattern": 0,
            "review_session": 1, "review_sessions": 1, "session": 1,
            "curated_issue": 2, "curated_issues": 2, "issue": 2,
        }

        def _sort_key(task: Dict[str, Any]) -> int:
            if not isinstance(task, dict):
                return 99
            target = str(task.get("target") or task.get("table") or "").lower()
            return target_priority.get(target, 50)

        return sorted(tasks, key=_sort_key)

    def _sanitize_json_like_text(self, text: str) -> str:
        """æ¸…ç† LLM è¾“å‡ºä¸­çš„å™ªå£°ï¼Œå°½é‡æå– JSON."""
        if not isinstance(text, str):
            return ""
        cleaned = text.strip()
        # å»é™¤ ```json / ``` åŒ…è£¹
        cleaned = re.sub(r"```json", "", cleaned, flags=re.IGNORECASE)
        cleaned = cleaned.replace("```", "")
        # ç§»é™¤æ³¨é‡Š
        cleaned = re.sub(r"//.*?$", "", cleaned, flags=re.MULTILINE)
        cleaned = re.sub(r"/\*.*?\*/", "", cleaned, flags=re.DOTALL)
        # ç§»é™¤å°¾éšé€—å·
        cleaned = re.sub(r",\s*([\]}])", r"\1", cleaned)
        return cleaned.strip()

    async def _translate_tasks_with_llm(
        self,
        raw_tasks: List[Dict[str, Any]],
        session_id: str,
        variant: Optional[str] = None,
        raw_text: Optional[str] = None,
        issue_pattern: Optional[Dict[str, Any]] = None,
    ) -> List[Dict[str, Any]]:
        """
        ä½¿ç”¨ Qwen å°†è‡ªç„¶è¯­è¨€ db_tasks ç¿»è¯‘ä¸ºç»“æ„åŒ– DB æ“ä½œã€‚
        è¾“å…¥ç¤ºä¾‹ï¼ˆprompts.py 106-112ï¼‰ï¼š
        {
          "db_tasks": [
            {"project": "...", "description": "æ•°æ®åº“æ“ä½œçš„è‡ªç„¶è¯­è¨€æè¿°"}
          ]
        }

        æœŸæœ›è¾“å‡º JSON æ•°ç»„å…ƒç´ ï¼š
        {
          "target": "issue_pattern|curated_issue|review_session",
          "action": "create|update|delete|sync",
          "data": { ... ä¸ models.py å­—æ®µå¯¹é½ ... }
        }
        """
        if not self.ai_enabled or not self.tokenizer or not hasattr(self, "model"):
            log("db_manage_agent", LogLevel.WARNING, "âš ï¸ AI æœªå¯ç”¨ï¼Œè·³è¿‡ LLM ç¿»è¯‘")
            return []

        # ä½¿ç”¨ç‹¬ç«‹ Prompt æŒ‡ä»¤åŒ–ç¿»è¯‘è¡Œä¸º
        try:
            from infrastructure.config.prompts import get_prompt
            system_prompt = get_prompt(
                "db_task_translation",
                model_name=self.model_name,
                variant=variant,
            )
            log("db_manage_agent", LogLevel.INFO, f"âœ… æˆåŠŸåŠ è½½ prompt variant={variant}, é•¿åº¦={len(system_prompt)}")
        except Exception as e:
            log("db_manage_agent", LogLevel.WARNING, f"âš ï¸ Prompt åŠ è½½å¤±è´¥ï¼Œä½¿ç”¨å…œåº• prompt: {e}")
            system_prompt = (
                "ä½ æ˜¯æ•°æ®åº“ç®¡ç†ä»£ç†ã€‚æ ¹æ® db_tasks çš„ project å’Œ descriptionï¼Œ"
                "å°†éœ€æ±‚ç¿»è¯‘ä¸º SQLite çš„ç»“æ„åŒ–æ“ä½œï¼Œå­—æ®µå¯¹é½ review_sessions/curated_issues/issue_patternsã€‚"
                "è¾“å‡º JSON æ•°ç»„ï¼Œä»…åŒ…å« target, action, dataï¼Œç¦æ­¢é™„åŠ è¯´æ˜ã€‚"
            )

        payload = {"db_tasks": raw_tasks}
        if raw_text:
            payload["raw_text"] = raw_text
        if issue_pattern:
            payload["issue_pattern"] = issue_pattern
        try:
            user_content = json.dumps(payload, ensure_ascii=False)
        except Exception:
            user_content = str(payload)

        # å¢å¼ºæ—¥å¿—ï¼šæ‰“å°å®é™…æ¥æ”¶çš„å†…å®¹å’Œå‘é€ç»™æ¨¡å‹çš„å†…å®¹
        log("db_manage_agent", LogLevel.INFO, f"ğŸ“¥ DB Agent æ¥æ”¶åˆ°çš„ db_tasks: {user_content[:500]}")
        log("db_manage_agent", LogLevel.DEBUG, f"ğŸ§  System Prompt (å‰300å­—): {system_prompt[:300]}")


        # ä½¿ç”¨ chat template æ˜ç¡®è§’è‰²ï¼ˆä¸ user_comm_agent ä¿æŒä¸€è‡´ï¼‰
        if hasattr(self.tokenizer, "apply_chat_template") and self.model_name.startswith("Qwen/"):
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content},
            ]
            input_ids = self.tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                return_tensors="pt",
            )
            if self.used_device == "gpu":
                input_ids = input_ids.to("cuda")

            log("db_manage_agent", LogLevel.INFO, "â³ æ­£åœ¨è°ƒç”¨æ¨¡å‹ç”Ÿæˆç¿»è¯‘ç»“æœï¼Œè¯·ç¨å€™...")
            
            # æŠŠåŒæ­¥é˜»å¡çš„ model.generate æ”¾åˆ°çº¿ç¨‹æ± æ‰§è¡Œï¼Œé¿å…é˜»å¡äº‹ä»¶å¾ªç¯
            def _generate_sync():
                with torch.no_grad():
                    return self.model.generate(
                        input_ids,
                        max_new_tokens=512,
                        temperature=0.7,
                        do_sample=True,
                        pad_token_id=self.tokenizer.eos_token_id,
                    )
            
            try:
                outputs = await asyncio.to_thread(_generate_sync)
                generated = self.tokenizer.decode(
                    outputs[0][input_ids.shape[1]:], skip_special_tokens=True
                )
            except Exception as e:
                log("db_manage_agent", LogLevel.ERROR, f"âŒ æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")
                return []
        else:
            # å…œåº•ï¼šæ—§å¼æ‹¼æ¥ï¼ˆä¿ç•™å…¼å®¹æ€§ï¼‰
            prompt = f"{system_prompt}\n\nç”¨æˆ·è¾“å…¥ï¼š{user_content}\n\nè¯·è¾“å‡º JSON æ•°ç»„ï¼š"
            inputs = self.tokenizer(prompt, return_tensors="pt")
            if self.used_device == "gpu":
                inputs = {k: v.to("cuda") for k, v in inputs.items()}

            log("db_manage_agent", LogLevel.WARNING, "â³ æ­£åœ¨è°ƒç”¨æ¨¡å‹ç”Ÿæˆç¿»è¯‘ç»“æœï¼ˆå…œåº•æ¨¡å¼ï¼‰ï¼Œè¯·ç¨å€™...")
            
            def _generate_sync():
                with torch.no_grad():
                    return self.model.generate(
                        inputs["input_ids"],
                        max_new_tokens=512,
                        do_sample=False,
                        pad_token_id=self.tokenizer.eos_token_id,
                    )
            
            try:
                outputs = await asyncio.to_thread(_generate_sync)
                generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            except Exception as e:
                log("db_manage_agent", LogLevel.ERROR, f"âŒ æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")
                return []

        parsed = self._extract_structured_tasks_from_text(generated)
        if parsed:
            variant_text = f" variant={variant}" if variant else ""
            log("db_manage_agent", LogLevel.INFO, f"âœ… LLM ç¿»è¯‘å¾—åˆ° {len(parsed)} æ¡ä»»åŠ¡{variant_text}")
        else:
            # è¾“å‡ºåŸå§‹ LLM å“åº”ä»¥ä¾¿è°ƒè¯•
            log("db_manage_agent", LogLevel.WARNING, f"âš ï¸ æœªèƒ½è§£æ LLM è¾“å‡ºï¼Œè¿”å›ç©ºä»»åŠ¡")
            # åŒæ—¶ç”¨ INFO çº§åˆ«è¾“å‡ºï¼Œç¡®ä¿èƒ½çœ‹åˆ°
            log("db_manage_agent", LogLevel.INFO, f"ğŸ“ LLM åŸå§‹å“åº” (è§£æå¤±è´¥):\n{generated[:500]}..." if len(generated) > 500 else f"ğŸ“ LLM åŸå§‹å“åº” (è§£æå¤±è´¥):\n{generated}")
        return parsed

    async def _classify_db_mode(self, raw_text: str) -> str:
        """ä½¿ç”¨ LLM åˆ¤æ–­ç”¨æˆ·è¯·æ±‚å±äº write/query/deleteã€‚"""
        if not self.ai_enabled or not self.tokenizer or not hasattr(self, "model"):
            return "write"
        try:
            from infrastructure.config.prompts import get_prompt
            system_prompt = get_prompt(
                "db_task_intent",
                model_name=self.model_name,
                variant="default",
            )
        except Exception:
            system_prompt = "åˆ¤æ–­ç”¨æˆ·è¯·æ±‚å±äº write/query/deleteï¼Œä»…è¾“å‡º JSON: {\"mode\":\"write\"}"

        payload = {"raw_text": raw_text}
        try:
            user_content = json.dumps(payload, ensure_ascii=False)
        except Exception:
            user_content = str(payload)

        if hasattr(self.tokenizer, "apply_chat_template") and self.model_name.startswith("Qwen/"):
            messages = [
                {"role": "system", "content": system_prompt},
                {"role": "user", "content": user_content},
            ]
            input_ids = self.tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True,
                return_tensors="pt",
            )
            if self.used_device == "gpu":
                input_ids = input_ids.to("cuda")

            def _generate_sync():
                with torch.no_grad():
                    return self.model.generate(
                        input_ids,
                        max_new_tokens=64,
                        temperature=0.2,
                        do_sample=False,
                        pad_token_id=self.tokenizer.eos_token_id,
                    )

            try:
                outputs = await asyncio.to_thread(_generate_sync)
                generated = self.tokenizer.decode(
                    outputs[0][input_ids.shape[1]:], skip_special_tokens=True
                )
            except Exception as e:
                log("db_manage_agent", LogLevel.WARNING, f"âš ï¸ æ¨¡å¼åˆ¤æ–­å¤±è´¥ï¼Œå›é€€ write: {e}")
                return "write"
        else:
            prompt = f"{system_prompt}\n\nç”¨æˆ·è¾“å…¥ï¼š{user_content}\n\nåªè¾“å‡º JSONï¼š"
            inputs = self.tokenizer(prompt, return_tensors="pt")
            if self.used_device == "gpu":
                inputs = {k: v.to("cuda") for k, v in inputs.items()}

            def _generate_sync():
                with torch.no_grad():
                    return self.model.generate(
                        inputs["input_ids"],
                        max_new_tokens=64,
                        do_sample=False,
                        pad_token_id=self.tokenizer.eos_token_id,
                    )

            try:
                outputs = await asyncio.to_thread(_generate_sync)
                generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            except Exception as e:
                log("db_manage_agent", LogLevel.WARNING, f"âš ï¸ æ¨¡å¼åˆ¤æ–­å¤±è´¥ï¼Œå›é€€ write: {e}")
                return "write"

        cleaned = self._sanitize_json_like_text(generated)
        try:
            data = json.loads(cleaned)
            mode = str(data.get("mode", "")).lower()
            if mode in ("write", "query", "delete"):
                return mode
        except Exception:
            pass
        for key in ("write", "query", "delete"):
            if key in cleaned.lower():
                return key
        return "write"

    def _extract_structured_tasks_from_text(self, text: str) -> List[Dict[str, Any]]:
        """
        ä»æ¨¡å‹è¾“å‡ºä¸­æå– JSON æ•°ç»„ï¼›å®¹é”™ï¼šæˆªå–é¦–å°¾çš„ JSON ç‰‡æ®µã€‚
        """
        cleaned = self._sanitize_json_like_text(text)
        try:
            data = json.loads(cleaned)
            return data if isinstance(data, list) else []
        except Exception:
            pass
        # é€å­—æ‰«æç¬¬ä¸€ä¸ª JSON æ•°ç»„ç‰‡æ®µ
        try:
            start = cleaned.find("[")
            if start == -1:
                return []
            depth = 0
            for idx in range(start, len(cleaned)):
                ch = cleaned[idx]
                if ch == "[":
                    depth += 1
                elif ch == "]":
                    depth -= 1
                    if depth == 0:
                        snippet = cleaned[start : idx + 1]
                        data = json.loads(snippet)
                        return data if isinstance(data, list) else []
        except Exception:
            return []
        return []

    async def _handle_single_db_task(
        self, task: Dict[str, Any], session_id: str
    ) -> Dict[str, Any]:
        """
        æ‰§è¡Œå•æ¡æ•°æ®åº“ä»»åŠ¡ã€‚
        é¢„æœŸå­—æ®µï¼š
            - action: create/update/delete/sync
            - target/table: review_session/curated_issue/issue_pattern
            - data: å…·ä½“å­—æ®µ
        """
        action = str(task.get("action") or task.get("op") or task.get("type") or "").lower()
        target = str(task.get("target") or task.get("table") or task.get("object") or "").lower()
        data = task.get("data") if isinstance(task.get("data"), dict) else {}
        action, target, data = self._normalize_task(action, target, data, session_id)
        if action or target:
            log(
                "db_manage_agent",
                LogLevel.INFO,
                f"ğŸ§© æ‰§è¡Œæ•°æ®åº“ä»»åŠ¡ target={target or 'æœªæä¾›'} action={action or 'æœªæä¾›'} keys={list(data.keys())}",
            )

        # å¦‚æœç¼ºå°‘ action/targetï¼Œå°è¯•æ ¹æ®å­—æ®µæ¨æ–­ï¼ˆå¸¸è§åœºæ™¯ï¼šæ–°å¢ IssuePatternï¼‰
        if not action and task:
            if "error_type" in task:
                action = "create"
                target = target or "issue_pattern"
                data = {**task}
        if not target and "table" in task:
            target = str(task["table"]).lower()

        if target in ("issue_pattern", "issuepattern", "pattern"):
            return await self._handle_issue_pattern_task(action, data)
        if target in ("curated_issue", "curatedissue", "curated"):
            return await self._handle_curated_issue_task(action, data)
        if target in ("review_session", "reviewsession", "session"):
            return await self._handle_review_session_task(action, data, session_id)

        raise ValueError(
            f"æœªçŸ¥çš„æ•°æ®åº“ä»»åŠ¡ç›®æ ‡: {target or 'æœªæä¾›'} (å…è®¸: issue_pattern/curated_issue/review_session)"
        )

    def _normalize_task(
        self,
        action: str,
        target: str,
        data: Dict[str, Any],
        session_id: str,
    ) -> Tuple[str, str, Dict[str, Any]]:
        """è§„èŒƒåŒ– LLM è¾“å‡ºçš„ target/action/dataï¼Œé¿å…è¡¨å/åŠ¨ä½œ/å­—æ®µä¸åŒ¹é…ã€‚"""
        if action:
            action = action.strip().lower()
            # å…¼å®¹ SQL/è‡ªç„¶è¯­ä¹‰åŠ¨ä½œ
            if "select" in action or "query" in action or "read" in action:
                action = "query"
            elif "show" in action or "list" in action or "print" in action:
                action = "query"
            elif "delete" in action and "all" in action:
                action = "delete_all"
            elif "truncate" in action or "clear" in action:
                action = "delete_all"
            elif action.startswith("delete"):
                action = "delete"
        target_map = {
            "review_sessions": "review_session",
            "review_session": "review_session",
            "curated_issues": "curated_issue",
            "curated_issue": "curated_issue",
            "issue_patterns": "issue_pattern",
            "issue_pattern": "issue_pattern",
            "issuepattern": "issue_pattern",
            "pattern": "issue_pattern",
            "session": "review_session",
            "issue": "curated_issue",
        }
        action_map = {
            "insert": "create",
            "create": "create",
            "add": "create",
            "update": "update",
            "modify": "update",
            "upsert": "upsert",
            "create_or_update": "upsert",
            "delete": "delete",
            "remove": "delete",
            "delete_all": "delete_all",
            "truncate": "delete_all",
            "clear": "delete_all",
            "sync": "sync",
            "select": "query",
            "query": "query",
            "read": "query",
            "fetch": "query",
            "list": "query",
            "show": "query",
        }
        target = target_map.get(target, target)
        action = action_map.get(action, action)
        if action == "delete" and data.get("confirm") is True and data.get("scope") in ("all", "*"):
            action = "delete_all"
        if action not in ("create", "update", "delete", "sync", "upsert", "query", "delete_all"):
            action = "upsert"

        # å†™å…¥åœºæ™¯ä¸‹ï¼ŒLLM ç”Ÿæˆçš„ update å¦‚æœç¼ºå°‘ idï¼Œè‡ªåŠ¨å›é€€ä¸º upsert
        # å› ä¸º LLM ä¸è´Ÿè´£ç®¡ç†ä¸»é”® idï¼Œupdate æ— æ³•æ‰§è¡Œï¼Œåº”å›é€€ä¸º create/upsert
        if action == "update":
            # æ ¹æ® target ç±»å‹åˆ¤æ–­ä¸»é”®å­—æ®µï¼ˆåŒºåˆ†ä¸»é”®å’Œå¤–é”®ï¼‰
            if target == "issue_pattern":
                has_id = bool(data.get("id") or data.get("pattern_id"))
            elif target == "curated_issue":
                # æ³¨æ„ï¼špattern_id å’Œ session_id æ˜¯å¤–é”®ï¼Œä¸èƒ½ä½œä¸ºæ›´æ–°çš„æ ‡è¯†
                has_id = bool(data.get("id") or data.get("issue_id"))
            elif target == "review_session":
                has_id = bool(data.get("id") or data.get("session_db_id"))
            else:
                has_id = False
            
            if not has_id:
                log(
                    "db_manage_agent",
                    LogLevel.INFO,
                    f"ğŸ”„ LLM è¾“å‡º update ä½†ç¼ºå°‘ idï¼Œè‡ªåŠ¨å›é€€ä¸º upsert (target={target})",
                )
                action = "upsert"

        # å­—æ®µç™½åå•ï¼Œè¿‡æ»¤ LLM è‡ªé€ å­—æ®µ
        if target == "issue_pattern":
            allowed = {
                # LLM æ¨æ–­å­—æ®µ
                "title",
                "error_type",
                "language",
                "framework",
                "error_description",
                "problematic_pattern",
                "solution",
                "file_pattern",
                "class_pattern",
                "tags",
                # ç³»ç»Ÿè‡ªç®¡ç†å­—æ®µï¼ˆç”± _apply_non_llm_defaults å¡«å……ï¼‰
                "severity",
                "status",
                # æ“ä½œæ§åˆ¶å­—æ®µ
                "layers",
                "id",
                "pattern_id",
                "limit",
                "offset",
                "confirm",
                "scope",
            }
        elif target == "curated_issue":
            allowed = {
                "session_id",
                "pattern_id",
                "project_path",
                "file_path",
                "start_line",
                "end_line",
                "code_snippet",
                "problem_phenomenon",
                "root_cause",
                "solution",
                "severity",
                "status",
                "id",
                "issue_id",
                "session_db_id",
                "limit",
                "offset",
                "confirm",
                "scope",
            }
        elif target == "review_session":
            allowed = {
                "session_id",
                "user_message",
                "code_directory",
                "status",
                "code_patch",
                "git_commit",
                "id",
                "session_db_id",
                "limit",
                "offset",
                "confirm",
                "scope",
            }
        else:
            return action, target, data

        filtered = {k: v for k, v in data.items() if k in allowed}
        # åŸºæœ¬å…œåº•ï¼šreview_session å†™å…¥æ—¶éœ€è¦ session_id
        if target == "review_session" and action in ("create", "update", "upsert"):
            if not filtered.get("session_id"):
                filtered["session_id"] = session_id
        return action, target, filtered

    async def _handle_issue_pattern_task(
        self, action: str, data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        å¤„ç† IssuePattern ç›¸å…³ä»»åŠ¡ï¼š
        - create / update / delete / sync
        """
        action = action or "create"
        action = action.lower()

        if action == "query":
            pattern_id = data.get("id") or data.get("pattern_id")
            if pattern_id:
                item = await self.db_service.get_issue_pattern_by_id(pattern_id)
                return {"items": [item] if item else [], "count": 1 if item else 0}
            status = data.get("status")
            limit = data.get("limit")
            items = await self.db_service.get_issue_patterns(status=status)
            if isinstance(limit, int) and limit > 0:
                items = items[:limit]
            return {"items": items, "count": len(items)}

        if action in ("create", "add", "insert", "upsert"):
            payload = self._fill_issue_pattern_defaults(data)
            
            # å»é‡é€»è¾‘ï¼šåœ¨ Weaviate ä¸­æŸ¥æ‰¾ç›¸ä¼¼çš„ IssuePattern
            similar = self._find_similar_issue_pattern(payload)
            if similar:
                existing_id = similar["sqlite_id"]
                similarity = similar["similarity"]
                log(
                    "db_manage_agent",
                    LogLevel.INFO,
                    f"ğŸ”„ å‘ç°ç›¸ä¼¼è®°å½• id={existing_id} (ç›¸ä¼¼åº¦={similarity:.2%})ï¼Œæ‰§è¡Œæ›´æ–°è€Œéæ–°å¢",
                )
                # æ›´æ–°å·²æœ‰è®°å½•
                updated = await self.db_service.update_issue_pattern(
                    pattern_id=existing_id,
                    error_type=payload.get("error_type"),
                    error_description=payload.get("error_description"),
                    problematic_pattern=payload.get("problematic_pattern"),
                    file_pattern=payload.get("file_pattern"),
                    class_pattern=payload.get("class_pattern"),
                    solution=payload.get("solution"),
                    severity=payload.get("severity"),
                    status=payload.get("status"),
                )
                layers = data.get("layers") if isinstance(data.get("layers"), list) else ["full"]
                sync_info = await self._sync_issue_pattern_if_possible(existing_id, layers)
                return {
                    "pattern_id": existing_id,
                    "action": "updated_existing",
                    "similarity": similarity,
                    "updated": updated,
                    "weaviate_sync": sync_info,
                }
            
            # æ— ç›¸ä¼¼è®°å½•ï¼Œæ–°å¢
            log(
                "db_manage_agent",
                LogLevel.INFO,
                f"ğŸ—„ï¸ å†™å…¥ issue_patterns: keys={list(payload.keys())}",
            )
            pattern_id = await self.db_service.create_issue_pattern(**payload)
            layers = data.get("layers") if isinstance(data.get("layers"), list) else ["full"]
            sync_info = await self._sync_issue_pattern_if_possible(pattern_id, layers)
            return {"pattern_id": pattern_id, "action": "created_new", "weaviate_sync": sync_info}

        if action in ("update", "modify"):
            pattern_id = data.get("id") or data.get("pattern_id")
            if not pattern_id:
                if action == "update":
                    raise ValueError("æ›´æ–° IssuePattern éœ€è¦æä¾› id")
                payload = self._fill_issue_pattern_defaults(data)
                pattern_id = await self.db_service.create_issue_pattern(**payload)
                layers = data.get("layers") if isinstance(data.get("layers"), list) else ["full"]
                sync_info = await self._sync_issue_pattern_if_possible(pattern_id, layers)
                return {"pattern_id": pattern_id, "weaviate_sync": sync_info}
            updated = await self.db_service.update_issue_pattern(
                pattern_id=pattern_id,
                error_type=data.get("error_type"),
                error_description=data.get("error_description"),
                problematic_pattern=data.get("problematic_pattern"),
                file_pattern=data.get("file_pattern"),
                class_pattern=data.get("class_pattern"),
                solution=data.get("solution"),
                severity=data.get("severity"),
                status=data.get("status"),
            )
            log("db_manage_agent", LogLevel.INFO, f"ğŸ—„ï¸ æ›´æ–° issue_patterns id={pattern_id}")
            layers = data.get("layers") if isinstance(data.get("layers"), list) else ["full"]
            sync_info = await self._sync_issue_pattern_if_possible(pattern_id, layers)
            return {"pattern_id": pattern_id, "updated": updated, "weaviate_sync": sync_info}

        if action in ("delete", "remove"):
            pattern_id = data.get("id") or data.get("pattern_id")
            if not pattern_id:
                raise ValueError("åˆ é™¤ IssuePattern éœ€è¦æä¾› id")
            deleted = await self.db_service.delete_issue_pattern(pattern_id)
            log("db_manage_agent", LogLevel.INFO, f"ğŸ—„ï¸ åˆ é™¤ issue_patterns id={pattern_id}")
            weaviate_deleted = self._delete_weaviate_items(pattern_id)
            return {"pattern_id": pattern_id, "deleted": deleted, "weaviate_deleted": weaviate_deleted}

        if action == "delete_all":
            if data.get("confirm") is not True:
                raise ValueError("åˆ é™¤å…¨éƒ¨ IssuePattern éœ€è¦ confirm=true")
            patterns = await self.db_service.get_issue_patterns()
            pattern_ids = [p.get("id") for p in patterns if p.get("id") is not None]
            deleted_count = await self.db_service.delete_all_issue_patterns()
            weaviate_deleted = 0
            for pid in pattern_ids:
                weaviate_deleted += self._delete_weaviate_items(pid)
            log("db_manage_agent", LogLevel.INFO, f"ğŸ—„ï¸ æ‰¹é‡åˆ é™¤ issue_patterns count={deleted_count}")
            return {"deleted_count": deleted_count, "weaviate_deleted": weaviate_deleted}

        if action == "sync":
            pattern_id = data.get("id") or data.get("pattern_id")
            if not pattern_id:
                raise ValueError("åŒæ­¥ IssuePattern éœ€è¦æä¾› id")
            layers = data.get("layers") if isinstance(data.get("layers"), list) else ["full"]
            sync_info = await self._sync_issue_pattern_if_possible(pattern_id, layers)
            return {"pattern_id": pattern_id, "weaviate_sync": sync_info}

        raise ValueError(f"ä¸æ”¯æŒçš„ IssuePattern æ“ä½œ: {action}")

    async def _handle_curated_issue_task(
        self, action: str, data: Dict[str, Any]
    ) -> Dict[str, Any]:
        action = action.lower()
        if action == "query":
            issue_id = data.get("id") or data.get("issue_id")
            if issue_id:
                item = await self.db_service.get_curated_issue_by_id(issue_id)
                return {"items": [item] if item else [], "count": 1 if item else 0}
            session_db_id = data.get("session_db_id") or data.get("session_id")
            file_path = data.get("file_path")
            status = data.get("status")
            limit = data.get("limit")
            items = await self.db_service.get_curated_issues(
                session_db_id=session_db_id,
                file_path=file_path,
                status=status,
            )
            if isinstance(limit, int) and limit > 0:
                items = items[:limit]
            return {"items": items, "count": len(items)}
        if action in ("create", "add", "insert", "upsert"):
            data = self._fill_curated_issue_defaults(data)
            
            # å»é‡é€»è¾‘ï¼šæ£€æŸ¥åŒä¸€ session_id + pattern_id æ˜¯å¦å·²æœ‰è®°å½•
            pattern_id = data.get("pattern_id")
            target_session_id = data.get("session_id")
            if pattern_id and target_session_id:
                existing = await self.db_service.get_curated_issue_by_session_and_pattern(
                    session_id=target_session_id,
                    pattern_id=pattern_id,
                )
                if existing:
                    log(
                        "db_manage_agent",
                        LogLevel.INFO,
                        f"ğŸ”„ å‘ç°å·²æœ‰ curated_issue (session_id={target_session_id}, pattern_id={pattern_id})ï¼Œæ›´æ–°è€Œéæ–°å¢",
                    )
                    # æ›´æ–°ç°æœ‰è®°å½•çš„çŠ¶æ€
                    await self.db_service.update_curated_issue_status(
                        issue_id=existing["id"],
                        status=data.get("status", existing.get("status", "open")),
                    )
                    return {"issue_id": existing["id"], "action": "updated_existing"}
            
            log("db_manage_agent", LogLevel.INFO, "ğŸ—„ï¸ å†™å…¥ curated_issues")
            issue_id = await self.db_service.create_curated_issue(
                session_id=data["session_id"],
                file_path=data["file_path"],
                start_line=data["start_line"],
                end_line=data["end_line"],
                code_snippet=data["code_snippet"],
                problem_phenomenon=data["problem_phenomenon"],
                root_cause=data["root_cause"],
                solution=data["solution"],
                severity=data.get("severity", "medium"),
                status=data.get("status", "open"),
                project_path=data.get("project_path"),
                pattern_id=data.get("pattern_id"),
            )
            return {"issue_id": issue_id}

        if action in ("update", "modify"):
            issue_id = data.get("id") or data.get("issue_id")
            if not issue_id:
                if action == "update":
                    raise ValueError("æ›´æ–° CuratedIssue éœ€è¦æä¾› id")
                data = self._fill_curated_issue_defaults(data)
                issue_id = await self.db_service.create_curated_issue(
                    session_id=data["session_id"],
                    file_path=data["file_path"],
                    start_line=data["start_line"],
                    end_line=data["end_line"],
                    code_snippet=data["code_snippet"],
                    problem_phenomenon=data["problem_phenomenon"],
                    root_cause=data["root_cause"],
                    solution=data["solution"],
                    severity=data.get("severity", "medium"),
                    status=data.get("status", "open"),
                    project_path=data.get("project_path"),
                    pattern_id=data.get("pattern_id"),
                )
                return {"issue_id": issue_id}
            updated = await self.db_service.update_curated_issue_status(
                issue_id=issue_id,
                status=data.get("status", "open"),
            )
            log("db_manage_agent", LogLevel.INFO, f"ğŸ—„ï¸ æ›´æ–° curated_issues id={issue_id}")
            return {"issue_id": issue_id, "updated": updated}

        if action in ("delete", "remove"):
            issue_id = data.get("id") or data.get("issue_id")
            if not issue_id:
                raise ValueError("åˆ é™¤ CuratedIssue éœ€è¦æä¾› id")
            deleted = await self.db_service.delete_curated_issue(issue_id)
            log("db_manage_agent", LogLevel.INFO, f"ğŸ—„ï¸ åˆ é™¤ curated_issues id={issue_id}")
            return {"issue_id": issue_id, "deleted": deleted}

        if action == "delete_all":
            if data.get("confirm") is not True:
                raise ValueError("åˆ é™¤å…¨éƒ¨ CuratedIssue éœ€è¦ confirm=true")
            deleted_count = await self.db_service.delete_all_curated_issues()
            log("db_manage_agent", LogLevel.INFO, f"ğŸ—„ï¸ æ‰¹é‡åˆ é™¤ curated_issues count={deleted_count}")
            return {"deleted_count": deleted_count}

        raise ValueError(f"ä¸æ”¯æŒçš„ CuratedIssue æ“ä½œ: {action}")

    async def _handle_review_session_task(
        self, action: str, data: Dict[str, Any], session_id: str
    ) -> Dict[str, Any]:
        action = action.lower()
        if action == "query":
            db_id = data.get("id") or data.get("session_db_id")
            if db_id:
                item = await self.db_service.get_review_session_by_id(db_id)
                return {"items": [item] if item else [], "count": 1 if item else 0}
            status = data.get("status")
            limit = data.get("limit")
            offset = data.get("offset", 0)
            items = await self.db_service.get_review_sessions(
                status=status,
                limit=limit if isinstance(limit, int) and limit > 0 else None,
                offset=offset if isinstance(offset, int) and offset > 0 else 0,
            )
            return {"items": items, "count": len(items)}
        if action in ("create", "add", "insert", "upsert"):
            # å»é‡é€»è¾‘ï¼šæ£€æŸ¥åŒä¸€ session_id æ˜¯å¦å·²æœ‰è®°å½•
            target_session_id = data.get("session_id", session_id)
            existing = await self.db_service.get_review_session_by_session_id(target_session_id)
            if existing:
                log(
                    "db_manage_agent",
                    LogLevel.INFO,
                    f"ğŸ”„ å‘ç°å·²æœ‰ review_session (session_id={target_session_id})ï¼Œæ›´æ–°è€Œéæ–°å¢",
                )
                # æ›´æ–°ç°æœ‰è®°å½•
                await self.db_service.update_review_session_status(
                    db_id=existing["id"],
                    status=data.get("status", existing.get("status", "open")),
                )
                return {"session_db_id": existing["id"], "action": "updated_existing"}
            
            log("db_manage_agent", LogLevel.INFO, "ğŸ—„ï¸ å†™å…¥ review_sessions")
            session_db_id = await self.db_service.create_review_session(
                session_id=target_session_id,
                user_message=data.get("user_message", ""),
                code_directory=data.get("code_directory", ""),
                code_patch=data.get("code_patch"),
                git_commit=data.get("git_commit"),
                status=data.get("status", "open"),
            )
            return {"session_db_id": session_db_id}

        if action in ("update", "modify"):
            db_id = data.get("id") or data.get("session_db_id")
            if not db_id:
                if action == "update":
                    raise ValueError("æ›´æ–° ReviewSession éœ€è¦æä¾› id")
                session_db_id = await self.db_service.create_review_session(
                    session_id=data.get("session_id", session_id),
                    user_message=data.get("user_message", ""),
                    code_directory=data.get("code_directory", ""),
                    code_patch=data.get("code_patch"),
                    git_commit=data.get("git_commit"),
                    status=data.get("status", "open"),
                )
                return {"session_db_id": session_db_id}
            updated = await self.db_service.update_review_session_status(
                db_id=db_id, status=data.get("status", "open")
            )
            log("db_manage_agent", LogLevel.INFO, f"ğŸ—„ï¸ æ›´æ–° review_sessions id={db_id}")
            return {"session_db_id": db_id, "updated": updated}

        if action in ("delete", "remove"):
            db_id = data.get("id") or data.get("session_db_id")
            if not db_id:
                raise ValueError("åˆ é™¤ ReviewSession éœ€è¦æä¾› id")
            deleted = await self.db_service.delete_review_session(db_id)
            log("db_manage_agent", LogLevel.INFO, f"ğŸ—„ï¸ åˆ é™¤ review_sessions id={db_id}")
            return {"session_db_id": db_id, "deleted": deleted}

        if action == "delete_all":
            if data.get("confirm") is not True:
                raise ValueError("åˆ é™¤å…¨éƒ¨ ReviewSession éœ€è¦ confirm=true")
            deleted_count = await self.db_service.delete_all_review_sessions()
            log("db_manage_agent", LogLevel.INFO, f"ğŸ—„ï¸ æ‰¹é‡åˆ é™¤ review_sessions count={deleted_count}")
            return {"deleted_count": deleted_count}

        raise ValueError(f"ä¸æ”¯æŒçš„ ReviewSession æ“ä½œ: {action}")

    async def _sync_issue_pattern_if_possible(
        self, pattern_id: int, layers: List[str]
    ) -> Dict[str, Any]:
        if not self.vector_service.client:
            log(
                "db_manage_agent",
                LogLevel.WARNING,
                "âš ï¸ æœªé…ç½® Weaviate clientï¼Œè·³è¿‡å‘é‡åŒæ­¥",
            )
            return {"skipped": True, "reason": "weaviate_client_not_configured"}
        sync_info = await self.sync_service.sync_issue_pattern(pattern_id, layers)
        log("db_manage_agent", LogLevel.INFO, f"ğŸ”„ åŒæ­¥ issue_patterns id={pattern_id} layers={layers} sync_info={sync_info}")
        return sync_info

    def _delete_weaviate_items(self, pattern_id: int) -> int:
        if not self.vector_service.client:
            return 0
        deleted_count = self.vector_service.delete_knowledge_items_by_sqlite_id(pattern_id)
        log("db_manage_agent", LogLevel.INFO, f"ğŸ”„ åŒæ­¥åˆ é™¤ weaviate items id={pattern_id} deleted_count={deleted_count}")
        return deleted_count

    def _fill_issue_pattern_defaults(self, data: Dict[str, Any]) -> Dict[str, Any]:
        raw_text = data.get("error_description") or data.get("problematic_pattern") or ""
        return {
            "error_type": data.get("error_type", "general"),
            "error_description": data.get("error_description", raw_text or ""),
            "problematic_pattern": data.get("problematic_pattern", raw_text or ""),
            "solution": data.get("solution", ""),
            "severity": data.get("severity", "medium"),
            "title": data.get("title"),
            "language": data.get("language"),
            "framework": data.get("framework"),
            "file_pattern": data.get("file_pattern", ""),
            "class_pattern": data.get("class_pattern", ""),
            "tags": data.get("tags"),
            "status": data.get("status", "active"),
        }

    def _fill_curated_issue_defaults(self, data: Dict[str, Any]) -> Dict[str, Any]:
        raw_text = data.get("problem_phenomenon") or ""
        return {
            "session_id": data.get("session_id", ""),
            "pattern_id": data.get("pattern_id"),
            "project_path": data.get("project_path"),
            "file_path": data.get("file_path", ""),
            "start_line": data.get("start_line", 0),
            "end_line": data.get("end_line", 0),
            "code_snippet": data.get("code_snippet", ""),
            "problem_phenomenon": data.get("problem_phenomenon", raw_text or ""),
            "root_cause": data.get("root_cause", ""),
            "solution": data.get("solution", ""),
            "severity": data.get("severity", "medium"),
            "status": data.get("status", "open"),
        }

    def _default_embed(self, text: str) -> List[float]:
        """
        è½»é‡çº§åµŒå…¥å‡½æ•°ï¼Œç”¨äºåœ¨ç¼ºå°‘çœŸå®æ¨¡å‹æ—¶æä¾›ç¨³å®šå‘é‡ã€‚
        """
        if text is None:
            text = ""
        total = float(sum(ord(c) for c in text))
        length = float(len(text) or 1)
        return [
            length,
            (total % 991) / 991.0,
            (total % 313) / 313.0,
        ]

    def _build_semantic_text(self, data: Dict[str, Any]) -> str:
        """
        æ„å»ºç”¨äºè¯­ä¹‰åŒ¹é…çš„æ–‡æœ¬ï¼Œä¸ Weaviate çš„ semantic å±‚ä¿æŒä¸€è‡´ã€‚
        
        åŒ…å«å­—æ®µï¼šerror_type, severity, language, framework, error_description
        """
        parts = [
            f"[error_type] {data.get('error_type') or ''}",
            f"[severity] {data.get('severity') or ''}",
            f"[language] {data.get('language') or ''}",
            f"[framework] {data.get('framework') or ''}",
            f"[description] {data.get('error_description') or ''}",
        ]
        return "\n".join(parts)

    def _find_similar_issue_pattern(
        self, data: Dict[str, Any], similarity_threshold: float = 0.85
    ) -> Optional[Dict[str, Any]]:
        """
        åœ¨ Weaviate ä¸­æŸ¥æ‰¾ä¸ç»™å®šæ•°æ®ç›¸ä¼¼çš„ IssuePatternã€‚
        
        Args:
            data: å¾…åŒ¹é…çš„ IssuePattern æ•°æ®
            similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼ï¼ˆ0-1ï¼‰ï¼Œè¶Šé«˜è¶Šä¸¥æ ¼
            
        Returns:
            å¦‚æœæ‰¾åˆ°ç›¸ä¼¼è®°å½•ï¼Œè¿”å›åŒ…å« sqlite_id å’Œ distance çš„å­—å…¸ï¼›å¦åˆ™è¿”å› None
        """
        if not self.vector_service.client:
            log(
                "db_manage_agent",
                LogLevel.DEBUG,
                "âš ï¸ Weaviate æœªé…ç½®ï¼Œè·³è¿‡ç›¸ä¼¼åº¦æŸ¥è¯¢",
            )
            return None
        
        # æ„å»ºè¯­ä¹‰æ–‡æœ¬å¹¶ç”Ÿæˆå‘é‡
        semantic_text = self._build_semantic_text(data)
        query_vector = self._default_embed(semantic_text)
        
        # åœ¨ Weaviate ä¸­æœç´¢ç›¸ä¼¼é¡¹
        try:
            results = self.vector_service.search_knowledge_items(
                query_vector=query_vector,
                limit=1,
                layer="semantic",
            )
            
            if not results:
                return None
            
            top_result = results[0]
            # Weaviate è¿”å›çš„æ˜¯ distanceï¼ˆè·ç¦»ï¼‰ï¼Œè¶Šå°è¶Šç›¸ä¼¼
            # distance = 0 è¡¨ç¤ºå®Œå…¨ç›¸åŒï¼Œdistance = 2 è¡¨ç¤ºå®Œå…¨ä¸åŒï¼ˆä½™å¼¦è·ç¦»èŒƒå›´ 0-2ï¼‰
            distance = top_result.get("_additional", {}).get("distance", 2.0)
            
            # å°† distance è½¬æ¢ä¸º similarity (1 - distance/2)
            similarity = 1.0 - (distance / 2.0)
            
            log(
                "db_manage_agent",
                LogLevel.INFO,
                f"ğŸ” ç›¸ä¼¼åº¦æŸ¥è¯¢ç»“æœ: sqlite_id={top_result.get('sqlite_id')}, "
                f"distance={distance:.4f}, similarity={similarity:.4f}",
            )
            
            if similarity >= similarity_threshold:
                return {
                    "sqlite_id": top_result.get("sqlite_id"),
                    "distance": distance,
                    "similarity": similarity,
                }
            
            return None
            
        except Exception as e:
            log(
                "db_manage_agent",
                LogLevel.WARNING,
                f"âš ï¸ ç›¸ä¼¼åº¦æŸ¥è¯¢å¤±è´¥: {e}",
            )
            return None

    def _format_table(self, headers: List[str], rows: List[List[str]]) -> str:
        if not headers or not rows:
            return ""

        def _col_width(idx: int) -> int:
            values = [headers[idx]] + [r[idx] for r in rows]
            return max(len(v) for v in values)

        widths = [_col_width(i) for i in range(len(headers))]

        def _fmt_row(cols: List[str]) -> str:
            return "| " + " | ".join(col.ljust(widths[i]) for i, col in enumerate(cols)) + " |"

        sep = "+-" + "-+-".join("-" * w for w in widths) + "-+"
        lines = [sep, _fmt_row(headers), sep]
        for row in rows:
            lines.append(_fmt_row(row))
        lines.append(sep)
        return "\n".join(lines)

    def _summarize_tasks_for_log(self, tasks: List[Dict[str, Any]]) -> str:
        """æŒ‰è¡¨/åŠ¨ä½œæ±‡æ€» LLM ç»“æ„åŒ–ä»»åŠ¡ï¼Œä¾¿äºæ—¥å¿—å±•ç¤ºã€‚"""
        if not tasks:
            return ""
        grouped: Dict[str, Dict[str, int]] = {}
        for task in tasks:
            raw_target = str(task.get("target") or task.get("table") or "")
            raw_action = str(task.get("action") or task.get("op") or "")
            data = task.get("data") if isinstance(task.get("data"), dict) else {}
            action, target, _ = self._normalize_task(raw_action, raw_target, data, "summary")
            target_key = target or "unknown"
            action_key = action or "unknown"
            grouped.setdefault(target_key, {})
            grouped[target_key][action_key] = grouped[target_key].get(action_key, 0) + 1

        headers = ["target", "action", "count"]
        rows: List[List[str]] = []
        for target, actions in grouped.items():
            for action, count in actions.items():
                rows.append([target, action, str(count)])
        return self._format_table(headers, rows)

    def _log_llm_tasks(self, stage: str, tasks: List[Dict[str, Any]]) -> None:
        """æ‰“å°å•æ¬¡ LLM è¾“å‡ºçš„ç»“æ„åŒ–ä»»åŠ¡åŠæ‘˜è¦ã€‚"""
        if not tasks:
            return
        try:
            pretty_tasks = json.dumps(tasks, ensure_ascii=False, indent=2)
        except Exception:
            pretty_tasks = str(tasks)
        log("db_manage_agent", LogLevel.INFO, f"ğŸ“œ LLM ç»“æ„åŒ–ä»»åŠ¡({stage}):\n{pretty_tasks}")
        summary_table = self._summarize_tasks_for_log(tasks)
        if summary_table:
            log("db_manage_agent", LogLevel.INFO, f"ğŸ“Œ LLM ä»»åŠ¡æ‘˜è¦({stage}):\n{summary_table}")

    def _build_db_summary_table(self, results: List[Dict[str, Any]]) -> str:
        """ç”Ÿæˆæ•°æ®åº“å†™å…¥æ‘˜è¦è¡¨æ ¼ï¼ˆASCIIï¼‰ã€‚"""
        if not results:
            return ""
        grouped: Dict[str, List[List[str]]] = {}
        for item in results:
            task = item.get("task") if isinstance(item.get("task"), dict) else {}
            result = item.get("result") if isinstance(item.get("result"), dict) else {}
            raw_target = str(task.get("target") or task.get("table") or "")
            raw_action = str(task.get("action") or task.get("op") or "")
            data = task.get("data") if isinstance(task.get("data"), dict) else {}
            action, target, norm_data = self._normalize_task(raw_action, raw_target, data, "summary")
            status = str(item.get("status", ""))
            row_id = (
                result.get("pattern_id")
                or result.get("issue_id")
                or result.get("session_db_id")
                or ""
            )
            rows = grouped.setdefault(target or "unknown", [])
            if norm_data:
                for key, value in norm_data.items():
                    try:
                        value_text = json.dumps(value, ensure_ascii=False)
                    except Exception:
                        value_text = str(value)
                    rows.append([action, status, str(key), value_text, str(row_id)])
            else:
                rows.append([action, status, "", "", str(row_id)])

        tables: List[str] = []
        headers = ["action", "status", "field", "value", "id"]
        for target, rows in grouped.items():
            table_text = self._format_table(headers, rows)
            if table_text:
                tables.append(f"[{target}]\n{table_text}")
        return "\n\n".join(tables)

    def _build_db_query_tables(self, results: List[Dict[str, Any]]) -> Dict[str, str]:
        """ç”ŸæˆæŸ¥è¯¢ç»“æœè¡¨æ ¼ï¼ˆæŒ‰è¡¨ååˆ†ç»„ï¼‰ã€‚"""
        grouped: Dict[str, List[Dict[str, Any]]] = {}
        for item in results:
            if item.get("status") != "success":
                continue
            task = item.get("task") if isinstance(item.get("task"), dict) else {}
            result = item.get("result") if isinstance(item.get("result"), dict) else {}
            raw_target = str(task.get("target") or task.get("table") or "")
            raw_action = str(task.get("action") or task.get("op") or "")
            data = task.get("data") if isinstance(task.get("data"), dict) else {}
            action, target, _ = self._normalize_task(raw_action, raw_target, data, "summary")
            if action != "query":
                continue
            items = result.get("items") if isinstance(result.get("items"), list) else []
            if items:
                grouped.setdefault(target, []).extend(items)

        tables: Dict[str, str] = {}
        for target, items in grouped.items():
            if not items:
                continue
            headers = sorted({k for item in items for k in item.keys()})
            rows = []
            for item in items:
                rows.append([str(item.get(h, "")) for h in headers])
            table_text = self._format_table(headers, rows)
            if table_text:
                tables[target] = table_text
        return tables