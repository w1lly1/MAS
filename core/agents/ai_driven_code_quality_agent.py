import torch
import asyncio
from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM, AutoModelForSequenceClassification
from typing import Dict, Any, List
from .base_agent import BaseAgent, Message
from infrastructure.database.service import DatabaseService
from infrastructure.config.settings import HUGGINGFACE_CONFIG

class AIDrivenCodeQualityAgent(BaseAgent):
    """AIé©±åŠ¨çš„ä»£ç è´¨é‡åˆ†ææ™ºèƒ½ä½“ - çœŸæ­£åˆ©ç”¨AIæ¨¡å‹èƒ½åŠ›"""
    
    def __init__(self):
        super().__init__("ai_code_quality_agent", "AIé©±åŠ¨ä»£ç è´¨é‡åˆ†ææ™ºèƒ½ä½“")
        self.db_service = DatabaseService()
        self.model_config = HUGGINGFACE_CONFIG["models"]["code_quality"]
        
        # AIæ¨¡å‹ç»„ä»¶
        self.code_understanding_model = None
        self.text_generation_model = None
        self.classification_model = None
        
        # ä¸“ä¸špromptæ¨¡æ¿
        self.quality_analysis_prompt = """
ä½œä¸ºä¸€ä¸ªèµ„æ·±çš„ä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œè¯·åˆ†æä»¥ä¸‹ä»£ç çš„è´¨é‡:

**åˆ†æç»´åº¦:**
1. ä»£ç å¯è¯»æ€§å’Œå‘½åè§„èŒƒ
2. å‡½æ•°å’Œç±»çš„è®¾è®¡æ˜¯å¦åˆç†
3. ä»£ç å¤æ‚åº¦å’Œç»´æŠ¤æ€§
4. é”™è¯¯å¤„ç†å’Œè¾¹ç•Œæ¡ä»¶
5. æ€§èƒ½è€ƒè™‘
6. æœ€ä½³å®è·µéµå¾ªæƒ…å†µ

**ä»£ç å†…å®¹:**
```
{code_content}
```

**è¯·æä¾›:**
1. æ€»ä½“è´¨é‡è¯„åˆ† (1-10åˆ†)
2. å…·ä½“é—®é¢˜åˆ—è¡¨
3. æ”¹è¿›å»ºè®®
4. é‡æ„å»ºè®®

**åˆ†æç»“æœ:**
"""

        self.refactoring_prompt = """
ä½œä¸ºä¸€ä¸ªä»£ç é‡æ„ä¸“å®¶ï¼Œè¯·ä¸ºä»¥ä¸‹ä»£ç æä¾›é‡æ„å»ºè®®:

**å½“å‰ä»£ç :**
```
{code_content}
```

**é‡æ„ç›®æ ‡:**
- æé«˜ä»£ç å¯è¯»æ€§
- å‡å°‘å¤æ‚åº¦
- å¢å¼ºå¯ç»´æŠ¤æ€§
- éµå¾ªè®¾è®¡æ¨¡å¼

**è¯·æä¾›å…·ä½“çš„é‡æ„æ–¹æ¡ˆ:**
"""

    async def _initialize_models(self):
        """åˆå§‹åŒ–AIæ¨¡å‹ - é’ˆå¯¹CPUç¯å¢ƒä¼˜åŒ–"""
        try:
            model_name = self.model_config["name"]
            cache_dir = HUGGINGFACE_CONFIG["cache_dir"]
            
            # å¼ºåˆ¶ä½¿ç”¨CPUï¼Œé¿å…GPUç›¸å…³é”™è¯¯
            device = -1  # CPU only
            torch.set_num_threads(4)  # é™åˆ¶CPUçº¿ç¨‹æ•°
            
            print(f"ğŸ¤– æ­£åœ¨åŠ è½½ä»£ç ç†è§£æ¨¡å‹ (CPUæ¨¡å¼): {model_name}")
            print(f"ğŸ’¾ ç¼“å­˜ç›®å½•: {cache_dir}")
            
            # 1. ä¼˜å…ˆåŠ è½½è½»é‡çº§æ¨¡å‹
            try:
                self.tokenizer = AutoTokenizer.from_pretrained(
                    model_name, 
                    cache_dir=cache_dir,
                    local_files_only=False,  # å…è®¸ä¸‹è½½
                    trust_remote_code=False
                )
                print("âœ… TokenizeråŠ è½½æˆåŠŸ")
                
                # ä½¿ç”¨æ›´è½»é‡çš„pipelineè€Œä¸æ˜¯ç›´æ¥åŠ è½½æ¨¡å‹
                self.classification_model = pipeline(
                    "text-classification",
                    model=model_name,
                    tokenizer=self.tokenizer,
                    device=device,
                    cache_dir=cache_dir,
                    model_kwargs={
                        "torch_dtype": torch.float32,  # ä½¿ç”¨float32å‡å°‘å†…å­˜
                        "low_cpu_mem_usage": True
                    }
                )
                print("âœ… åˆ†ç±»æ¨¡å‹åŠ è½½æˆåŠŸ")
                
            except Exception as model_error:
                print(f"âš ï¸ ä¸»æ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°è¯•å¤‡ç”¨æ¨¡å‹: {model_error}")
                # é™çº§åˆ°DistilBERT (æ›´è½»é‡)
                fallback_model = "distilbert-base-uncased"
                self.tokenizer = AutoTokenizer.from_pretrained(fallback_model, cache_dir=cache_dir)
                self.classification_model = pipeline(
                    "text-classification",
                    model=fallback_model,
                    device=device,
                    cache_dir=cache_dir
                )
                print(f"âœ… å¤‡ç”¨æ¨¡å‹åŠ è½½æˆåŠŸ: {fallback_model}")
            
            # 2. ä½¿ç”¨CPUå‹å¥½çš„æ–‡æœ¬ç”Ÿæˆ (å¯é€‰ï¼Œæ€§èƒ½è¦æ±‚é«˜æ—¶å¯ç¦ç”¨)
            try:
                # ä½¿ç”¨æ›´å°çš„æ¨¡å‹ç”¨äºæ–‡æœ¬ç”Ÿæˆ
                self.text_generation_model = pipeline(
                    "text-generation",
                    model="gpt2",  # æ”¹ä¸ºæ›´è½»é‡çš„GPT-2
                    device=device,
                    cache_dir=cache_dir,
                    model_kwargs={"low_cpu_mem_usage": True}
                )
                print("âœ… æ–‡æœ¬ç”Ÿæˆæ¨¡å‹åŠ è½½æˆåŠŸ")
            except Exception as gen_error:
                print(f"âš ï¸ æ–‡æœ¬ç”Ÿæˆæ¨¡å‹åŠ è½½å¤±è´¥ï¼Œå°†ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ: {gen_error}")
                self.text_generation_model = None
            
            # 3. è®¾ç½®ä»£ç ç†è§£æ¨¡å‹å¼•ç”¨
            self.code_understanding_model = self.classification_model
            
            print(f"âœ… AIæ¨¡å‹åˆå§‹åŒ–å®Œæˆ (CPUæ¨¡å¼)")
            
        except Exception as e:
            print(f"âŒ AIæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            print("ğŸ”„ åˆ‡æ¢åˆ°æ— AIæ¨¡å¼ï¼Œä½¿ç”¨åŸºç¡€åˆ†æ")
            self.code_understanding_model = None
            self.classification_model = None
            self.text_generation_model = None
            
    async def handle_message(self, message: Message):
        """å¤„ç†ä»£ç è´¨é‡åˆ†æè¯·æ±‚"""
        if message.message_type == "quality_analysis_request":
            requirement_id = message.content.get("requirement_id")
            code_content = message.content.get("code_content", "")
            code_directory = message.content.get("code_directory", "")
            
            print(f"ğŸ¤– AIä»£ç è´¨é‡åˆ†æå¼€å§‹ - éœ€æ±‚ID: {requirement_id}")
            
            if not self.code_understanding_model:
                await self._initialize_models()
            
            # å…ˆè¯·æ±‚é™æ€æ‰«æç»“æœ
            await self.send_message(
                receiver="static_scan_agent",
                content={
                    "requirement_id": requirement_id,
                    "code_content": code_content,
                    "code_directory": code_directory
                },
                message_type="static_scan_request"
            )
            
            print(f"ğŸ“‹ å·²è¯·æ±‚é™æ€æ‰«æç»“æœ - éœ€æ±‚ID: {requirement_id}")
            
        elif message.message_type == "static_scan_complete":
            # æ¥æ”¶é™æ€æ‰«æç»“æœå¹¶è¿›è¡ŒAIç»¼åˆåˆ†æ
            requirement_id = message.content.get("requirement_id")
            static_scan_results = message.content.get("static_scan_results", {})
            code_content = message.content.get("code_content", "")
            code_directory = message.content.get("code_directory", "")
            
            print(f"ğŸ“Š æ”¶åˆ°é™æ€æ‰«æç»“æœï¼Œå¼€å§‹AIç»¼åˆåˆ†æ - éœ€æ±‚ID: {requirement_id}")
            
            # æ‰§è¡ŒAIé©±åŠ¨çš„ç»¼åˆåˆ†æ
            result = await self._ai_comprehensive_analysis(
                code_content, code_directory, static_scan_results
            )
            
            # å‘é€æœ€ç»ˆç»“æœ
            await self.send_message(
                receiver="ai_user_comm_agent",
                content={
                    "requirement_id": requirement_id,
                    "agent_type": "ai_code_quality",
                    "results": result,
                    "analysis_complete": True
                },
                message_type="analysis_result"
            )
            
            print(f"âœ… AIç»¼åˆä»£ç è´¨é‡åˆ†æå®Œæˆ - éœ€æ±‚ID: {requirement_id}")

    async def _ai_comprehensive_analysis(self, code_content: str, code_directory: str, 
                                        static_scan_results: Dict[str, Any]) -> Dict[str, Any]:
        """AIé©±åŠ¨çš„ç»¼åˆä»£ç è´¨é‡åˆ†æï¼ˆç»“åˆé™æ€æ‰«æç»“æœï¼‰"""
        
        try:
            print("ğŸ§  AIæ­£åœ¨ç»¼åˆåˆ†æä»£ç è´¨é‡å’Œé™æ€æ‰«æç»“æœ...")
            
            # 1. æ‰§è¡ŒåŸæœ‰çš„AIåˆ†æ
            ai_analysis = await self._ai_driven_quality_analysis(code_content, code_directory)
            
            # 2. è§£æå’Œç†è§£é™æ€æ‰«æç»“æœ
            static_analysis_insights = await self._analyze_static_scan_results(static_scan_results)
            
            # 3. AIç»¼åˆè¯„ä¼°ï¼šç»“åˆé™æ€åˆ†æå’ŒAIç†è§£
            comprehensive_assessment = await self._ai_comprehensive_assessment(
                ai_analysis, static_scan_results, static_analysis_insights
            )
            
            # 4. AIç”Ÿæˆæ•´åˆå»ºè®®
            integrated_suggestions = await self._generate_integrated_suggestions(
                ai_analysis, static_scan_results, comprehensive_assessment
            )
            
            # 5. ç”Ÿæˆæœ€ç»ˆè´¨é‡æŠ¥å‘Š
            final_report = await self._generate_final_quality_report(
                ai_analysis, static_scan_results, comprehensive_assessment, integrated_suggestions
            )
            
            print("âœ… AIç»¼åˆåˆ†æå®Œæˆï¼Œç”Ÿæˆæœ€ç»ˆè´¨é‡æŠ¥å‘Š")
            
            return {
                "analysis_type": "comprehensive_ai_static_analysis",
                "ai_analysis": ai_analysis,
                "static_scan_results": static_scan_results,
                "static_analysis_insights": static_analysis_insights,
                "comprehensive_assessment": comprehensive_assessment,
                "integrated_suggestions": integrated_suggestions,
                "final_report": final_report,
                "analysis_timestamp": self._get_current_time(),
                "analysis_status": "completed"
            }
            
        except Exception as e:
            print(f"âŒ AIç»¼åˆåˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return {
                "analysis_type": "comprehensive_analysis_error",
                "error_message": str(e),
                "analysis_status": "failed"
            }

    async def _analyze_static_scan_results(self, static_results: Dict[str, Any]) -> Dict[str, Any]:
        """åˆ†æé™æ€æ‰«æç»“æœï¼Œæå–å…³é”®æ´å¯Ÿ"""
        insights = {
            "critical_issues_summary": [],
            "pattern_analysis": {},
            "quality_trends": {},
            "improvement_priority": [],
            "tool_effectiveness": {}
        }
        
        try:
            # åˆ†æè´¨é‡é—®é¢˜
            quality_issues = static_results.get("quality_issues", [])
            security_issues = static_results.get("security_issues", [])
            type_issues = static_results.get("type_issues", [])
            
            # æå–ä¸¥é‡é—®é¢˜
            critical_issues = [
                issue for issue in quality_issues + security_issues + type_issues 
                if issue.get("severity") in ["critical", "high"]
            ]
            
            insights["critical_issues_summary"] = [
                {
                    "type": issue.get("type"),
                    "message": issue.get("message"),
                    "tool": issue.get("tool"),
                    "severity": issue.get("severity")
                }
                for issue in critical_issues[:10]  # å‰10ä¸ªä¸¥é‡é—®é¢˜
            ]
            
            # åˆ†æå¤æ‚åº¦æ•°æ®
            complexity_analysis = static_results.get("complexity_analysis", {})
            insights["complexity_insights"] = {
                "maintainability_index": complexity_analysis.get("maintainability_index", 0),
                "average_complexity": complexity_analysis.get("average_complexity", 0),
                "complexity_distribution": complexity_analysis.get("cyclomatic_complexity", {})
            }
            
            # å·¥å…·æ•ˆæœè¯„ä¼°
            tools_used = static_results.get("tools_used", [])
            summary = static_results.get("summary", {})
            
            insights["tool_effectiveness"] = {
                "tools_used": tools_used,
                "issues_found": summary.get("total_issues", 0),
                "quality_score": summary.get("quality_score", 0),
                "coverage": len(tools_used) / 5.0 * 100  # å‡è®¾æœ€å¤š5ä¸ªå·¥å…·
            }
            
        except Exception as e:
            insights["analysis_error"] = str(e)
            
        return insights

    async def _ai_comprehensive_assessment(self, ai_analysis: Dict[str, Any], 
                                         static_results: Dict[str, Any],
                                         static_insights: Dict[str, Any]) -> Dict[str, Any]:
        """AIé©±åŠ¨çš„ç»¼åˆè¯„ä¼°"""
        
        assessment = {
            "overall_quality_score": 0.0,
            "confidence_level": 0.0,
            "assessment_dimensions": {},
            "consistency_analysis": {},
            "risk_assessment": {}
        }
        
        try:
            # æå–AIåˆ†æç»“æœ
            ai_quality = ai_analysis.get("quality_classification", {}).get("confidence", 0.5)
            ai_complexity = ai_analysis.get("code_embeddings_summary", {}).get("semantic_complexity", 0.5)
            
            # æå–é™æ€åˆ†æç»“æœ
            static_quality = static_results.get("summary", {}).get("quality_score", 5.0) / 10.0
            static_issues = static_results.get("summary", {}).get("total_issues", 0)
            
            # ç»¼åˆè¯„åˆ†è®¡ç®—
            # AIç†è§£æƒé‡40%ï¼Œé™æ€åˆ†ææƒé‡60%
            overall_score = (ai_quality * 0.4 + static_quality * 0.6) * 10.0
            assessment["overall_quality_score"] = round(overall_score, 2)
            
            # ä¸€è‡´æ€§åˆ†æ
            score_difference = abs(ai_quality * 10 - static_quality * 10)
            assessment["consistency_analysis"] = {
                "ai_static_alignment": "high" if score_difference < 2.0 else "medium" if score_difference < 4.0 else "low",
                "score_difference": round(score_difference, 2),
                "assessment_reliability": "high" if score_difference < 2.0 else "medium"
            }
            
            # é£é™©è¯„ä¼°
            critical_issues = len([
                issue for issue in static_insights.get("critical_issues_summary", [])
                if issue.get("severity") == "critical"
            ])
            
            assessment["risk_assessment"] = {
                "critical_risk_level": "high" if critical_issues > 3 else "medium" if critical_issues > 0 else "low",
                "security_risk": "high" if any("security" in issue.get("type", "") 
                                             for issue in static_insights.get("critical_issues_summary", [])) else "low",
                "maintainability_risk": "high" if static_insights.get("complexity_insights", {}).get("maintainability_index", 50) < 30 else "low"
            }
            
            # ç½®ä¿¡åº¦è¯„ä¼°
            tool_coverage = static_insights.get("tool_effectiveness", {}).get("coverage", 0)
            ai_confidence = ai_analysis.get("ai_confidence", 0.8)
            
            assessment["confidence_level"] = round((tool_coverage / 100.0 * 0.3 + ai_confidence * 0.7), 2)
            
        except Exception as e:
            assessment["assessment_error"] = str(e)
            
        return assessment

    async def _generate_integrated_suggestions(self, ai_analysis: Dict[str, Any],
                                             static_results: Dict[str, Any],
                                             assessment: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæ•´åˆçš„æ”¹è¿›å»ºè®®"""
        
        suggestions = {
            "immediate_actions": [],
            "quality_improvements": [],
            "ai_enhanced_recommendations": [],
            "tool_based_fixes": [],
            "strategic_improvements": []
        }
        
        try:
            # åŸºäºé™æ€åˆ†æçš„å³æ—¶ä¿®å¤å»ºè®®
            static_issues = static_results.get("quality_issues", []) + static_results.get("security_issues", [])
            critical_static_issues = [issue for issue in static_issues if issue.get("severity") in ["critical", "high"]]
            
            for issue in critical_static_issues[:5]:
                suggestions["immediate_actions"].append({
                    "type": "static_fix",
                    "description": f"ä¿®å¤ {issue.get('tool')} æ£€æµ‹åˆ°çš„é—®é¢˜: {issue.get('message')}",
                    "line": issue.get("line", 0),
                    "severity": issue.get("severity"),
                    "tool": issue.get("tool")
                })
            
            # åŸºäºAIåˆ†æçš„è´¨é‡æ”¹è¿›å»ºè®®
            ai_suggestions = ai_analysis.get("improvement_suggestions", [])
            for suggestion in ai_suggestions[:3]:
                suggestions["ai_enhanced_recommendations"].append({
                    "type": "ai_insight",
                    "description": suggestion.get("description", ""),
                    "priority": suggestion.get("priority", "medium"),
                    "category": suggestion.get("category", "general")
                })
            
            # æˆ˜ç•¥æ€§æ”¹è¿›å»ºè®®
            overall_score = assessment.get("overall_quality_score", 5.0)
            if overall_score < 6.0:
                suggestions["strategic_improvements"].append({
                    "type": "architecture_review",
                    "description": "ä»£ç è´¨é‡åˆ†æ•°åä½ï¼Œå»ºè®®è¿›è¡Œæ¶æ„å®¡æŸ¥å’Œé‡æ„è§„åˆ’",
                    "priority": "high"
                })
            
            risk_level = assessment.get("risk_assessment", {}).get("critical_risk_level", "low")
            if risk_level == "high":
                suggestions["strategic_improvements"].append({
                    "type": "risk_mitigation",
                    "description": "å­˜åœ¨é«˜é£é™©é—®é¢˜ï¼Œå»ºè®®ç«‹å³åˆ¶å®šé£é™©ç¼“è§£è®¡åˆ’",
                    "priority": "critical"
                })
                
        except Exception as e:
            suggestions["generation_error"] = str(e)
            
        return suggestions

    async def _generate_final_quality_report(self, ai_analysis: Dict[str, Any],
                                           static_results: Dict[str, Any],
                                           assessment: Dict[str, Any],
                                           suggestions: Dict[str, Any]) -> Dict[str, Any]:
        """ç”Ÿæˆæœ€ç»ˆçš„è´¨é‡æŠ¥å‘Š"""
        
        report = {
            "executive_summary": {},
            "detailed_findings": {},
            "recommendations": {},
            "next_steps": [],
            "quality_metrics": {}
        }
        
        try:
            # æ‰§è¡Œæ‘˜è¦
            overall_score = assessment.get("overall_quality_score", 5.0)
            total_issues = static_results.get("summary", {}).get("total_issues", 0)
            
            report["executive_summary"] = {
                "overall_quality_score": overall_score,
                "quality_grade": self._score_to_grade(overall_score),
                "total_issues_found": total_issues,
                "critical_issues": len(suggestions.get("immediate_actions", [])),
                "analysis_confidence": assessment.get("confidence_level", 0.8),
                "primary_concerns": self._extract_primary_concerns(static_results, assessment)
            }
            
            # è¯¦ç»†å‘ç°
            report["detailed_findings"] = {
                "ai_insights": {
                    "semantic_understanding": ai_analysis.get("code_embeddings_summary", {}),
                    "ai_quality_assessment": ai_analysis.get("quality_classification", {}),
                    "ai_generated_analysis": ai_analysis.get("detailed_analysis", {})
                },
                "static_analysis_results": {
                    "tool_coverage": static_results.get("tools_used", []),
                    "issue_breakdown": static_results.get("summary", {}),
                    "complexity_metrics": static_results.get("complexity_analysis", {})
                },
                "consistency_check": assessment.get("consistency_analysis", {})
            }
            
            # å»ºè®®æ±‡æ€»
            report["recommendations"] = {
                "immediate_fixes": suggestions.get("immediate_actions", []),
                "quality_enhancements": suggestions.get("ai_enhanced_recommendations", []),
                "strategic_improvements": suggestions.get("strategic_improvements", [])
            }
            
            # ä¸‹ä¸€æ­¥è¡ŒåŠ¨
            report["next_steps"] = self._generate_next_steps(assessment, suggestions)
            
        except Exception as e:
            report["report_generation_error"] = str(e)
            
        return report

    def _extract_primary_concerns(self, static_results: Dict[str, Any], 
                                 assessment: Dict[str, Any]) -> List[str]:
        """æå–ä¸»è¦å…³æ³¨ç‚¹"""
        concerns = []
        
        # å®‰å…¨é—®é¢˜
        security_issues = static_results.get("security_issues", [])
        if security_issues:
            concerns.append(f"å‘ç° {len(security_issues)} ä¸ªå®‰å…¨é—®é¢˜")
        
        # å¤æ‚åº¦é—®é¢˜
        complexity = static_results.get("complexity_analysis", {})
        maintainability = complexity.get("maintainability_index", 50)
        if maintainability < 30:
            concerns.append("ä»£ç å¯ç»´æŠ¤æ€§æŒ‡æ•°åä½")
        
        # é£é™©è¯„ä¼°
        risk_level = assessment.get("risk_assessment", {}).get("critical_risk_level", "low")
        if risk_level == "high":
            concerns.append("å­˜åœ¨é«˜é£é™©ä»£ç è´¨é‡é—®é¢˜")
        
        return concerns[:3]  # æœ€å¤š3ä¸ªä¸»è¦å…³æ³¨ç‚¹

    def _generate_next_steps(self, assessment: Dict[str, Any], 
                           suggestions: Dict[str, Any]) -> List[str]:
        """ç”Ÿæˆä¸‹ä¸€æ­¥è¡ŒåŠ¨å»ºè®®"""
        steps = []
        
        # åŸºäºå³æ—¶è¡ŒåŠ¨å»ºè®®
        immediate_actions = suggestions.get("immediate_actions", [])
        if immediate_actions:
            steps.append(f"ç«‹å³ä¿®å¤ {len(immediate_actions)} ä¸ªé«˜ä¼˜å…ˆçº§é—®é¢˜")
        
        # åŸºäºè´¨é‡åˆ†æ•°
        overall_score = assessment.get("overall_quality_score", 5.0)
        if overall_score < 7.0:
            steps.append("åˆ¶å®šä»£ç è´¨é‡æå‡è®¡åˆ’")
        
        # åŸºäºå·¥å…·è¦†ç›–ç‡
        confidence = assessment.get("confidence_level", 0.8)
        if confidence < 0.7:
            steps.append("è€ƒè™‘å¢åŠ æ›´å¤šé™æ€åˆ†æå·¥å…·ä»¥æé«˜æ£€æµ‹è¦†ç›–ç‡")
        
        return steps

    def _score_to_grade(self, score: float) -> str:
        """å°†åˆ†æ•°è½¬æ¢ä¸ºç­‰çº§"""
        if score >= 9.0:
            return "A"
        elif score >= 8.0:
            return "B"
        elif score >= 7.0:
            return "C"
        elif score >= 6.0:
            return "D"
        else:
            return "F"

    async def _ai_driven_quality_analysis(self, code_content: str, code_directory: str) -> Dict[str, Any]:
        """AIé©±åŠ¨çš„ä»£ç è´¨é‡åˆ†æ"""
        
        try:
            print("ğŸ§  AIæ­£åœ¨ç†è§£ä»£ç ç»“æ„å’Œè¯­ä¹‰...")
            
            # 1. è¯»å–æ‰€æœ‰ä»£ç æ–‡ä»¶
            all_code_content = await self._read_code_files(code_directory) if code_directory else code_content
            
            # 2. AIè¯­ä¹‰ç†è§£ - ä½¿ç”¨CodeBERTç†è§£ä»£ç 
            code_embeddings = await self._get_code_embeddings(all_code_content)
            
            # 3. AIè´¨é‡åˆ†ç±» - ä½¿ç”¨åˆ†ç±»æ¨¡å‹
            quality_classification = await self._classify_code_quality(all_code_content)
            
            # 4. AIç”Ÿæˆåˆ†ææŠ¥å‘Š - ä½¿ç”¨promptå·¥ç¨‹
            analysis_report = await self._generate_quality_report(all_code_content)
            
            # 5. AIç”Ÿæˆæ”¹è¿›å»ºè®®
            improvement_suggestions = await self._generate_improvement_suggestions(all_code_content)
            
            # 6. AIé‡æ„å»ºè®®
            refactoring_suggestions = await self._generate_refactoring_suggestions(all_code_content)
            
            print("âœ… AIåˆ†æå®Œæˆï¼Œç”Ÿæˆç»¼åˆè´¨é‡æŠ¥å‘Š")
            
            return {
                "ai_analysis_type": "comprehensive_quality_analysis",
                "model_used": self.model_config["name"],
                "code_embeddings_summary": code_embeddings,
                "quality_classification": quality_classification,
                "detailed_analysis": analysis_report,
                "improvement_suggestions": improvement_suggestions,
                "refactoring_suggestions": refactoring_suggestions,
                "ai_confidence": 0.85,  # AIæ¨¡å‹ç½®ä¿¡åº¦
                "analysis_timestamp": self._get_current_time(),
                "analysis_status": "completed"
            }
            
        except Exception as e:
            print(f"âŒ AIåˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return {
                "ai_analysis_type": "error",
                "error_message": str(e),
                "analysis_status": "failed"
            }

    async def _get_code_embeddings(self, code_content: str) -> Dict[str, Any]:
        """ä½¿ç”¨AIæ¨¡å‹è·å–ä»£ç åµŒå…¥è¡¨ç¤º - CPUä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            if not self.classification_model:
                return {"error": "AIæ¨¡å‹æœªåŠ è½½", "fallback": True}
            
            # åˆ†å—å¤„ç†å¤§ä»£ç æ–‡ä»¶ï¼Œå‡å°‘å†…å­˜ä½¿ç”¨
            chunks = self._split_code_into_chunks(code_content, max_length=256)  # å‡å°å—å¤§å°
            embeddings_summary = []
            
            print(f"ğŸ“Š å¤„ç† {len(chunks)} ä¸ªä»£ç å—...")
            
            for i, chunk in enumerate(chunks[:3]):  # è¿›ä¸€æ­¥é™åˆ¶å¤„ç†å—æ•°
                try:
                    # ä½¿ç”¨pipelineè€Œä¸æ˜¯ç›´æ¥è°ƒç”¨æ¨¡å‹ï¼Œå‡å°‘å†…å­˜å ç”¨
                    result = self.classification_model(chunk[:200])  # é™åˆ¶è¾“å…¥é•¿åº¦
                    
                    if result and len(result) > 0:
                        score = result[0].get('score', 0.5)
                        embeddings_summary.append({
                            "chunk_index": i,
                            "semantic_score": float(score),
                            "chunk_length": len(chunk),
                            "model_confidence": float(score)
                        })
                    
                    # æ·»åŠ å»¶è¿Ÿï¼Œé¿å…CPUè¿‡è½½
                    await asyncio.sleep(0.1)
                    
                except Exception as chunk_error:
                    print(f"âš ï¸ å¤„ç†å— {i} æ—¶å‡ºé”™: {chunk_error}")
                    continue
            
            if embeddings_summary:
                avg_score = sum(item["semantic_score"] for item in embeddings_summary) / len(embeddings_summary)
                return {
                    "total_chunks": len(chunks),
                    "processed_chunks": len(embeddings_summary),
                    "embedding_summary": embeddings_summary,
                    "semantic_complexity": avg_score,
                    "processing_mode": "cpu_optimized"
                }
            else:
                return {
                    "error": "æ— æ³•å¤„ç†ä»»ä½•ä»£ç å—",
                    "fallback": True,
                    "processing_mode": "cpu_optimized"
                }
            
        except Exception as e:
            print(f"âš ï¸ åµŒå…¥ç”Ÿæˆå¤±è´¥ï¼Œä½¿ç”¨ç®€åŒ–åˆ†æ: {e}")
            return {
                "error": f"åµŒå…¥ç”Ÿæˆå¤±è´¥: {e}",
                "fallback_analysis": {
                    "code_length": len(code_content),
                    "estimated_complexity": "medium" if len(code_content) > 1000 else "low"
                },
                "processing_mode": "fallback"
            }

    async def _classify_code_quality(self, code_content: str) -> Dict[str, Any]:
        """ä½¿ç”¨AIåˆ†ç±»æ¨¡å‹è¯„ä¼°ä»£ç è´¨é‡"""
        try:
            # å‡†å¤‡åˆ†ç±»ç”¨çš„prompt
            classification_prompt = f"""
            Analyze the following code for quality assessment:
            
            Code:
            {code_content[:1000]}  # é™åˆ¶é•¿åº¦
            
            Quality aspects to evaluate:
            - Readability
            - Maintainability 
            - Performance
            - Security
            """
            
            # ä½¿ç”¨åˆ†ç±»æ¨¡å‹
            if self.classification_model:
                result = self.classification_model(classification_prompt)
                
                return {
                    "predicted_quality": result[0]["label"] if result else "UNKNOWN",
                    "confidence": result[0]["score"] if result else 0.0,
                    "model_prediction": result
                }
            else:
                return {"error": "åˆ†ç±»æ¨¡å‹æœªåˆå§‹åŒ–"}
                
        except Exception as e:
            return {"error": f"è´¨é‡åˆ†ç±»å¤±è´¥: {e}"}

    async def _generate_quality_report(self, code_content: str) -> Dict[str, Any]:
        """ä½¿ç”¨AIç”Ÿæˆè¯¦ç»†çš„è´¨é‡åˆ†ææŠ¥å‘Š"""
        try:
            # æ„é€ ä¸“ä¸šçš„åˆ†æprompt
            prompt = self.quality_analysis_prompt.format(code_content=code_content[:2000])
            
            if self.text_generation_model:
                # ç”Ÿæˆåˆ†ææŠ¥å‘Š
                response = self.text_generation_model(
                    prompt,
                    max_length=500,
                    num_return_sequences=1,
                    temperature=0.7,
                    do_sample=True
                )
                
                generated_text = response[0]["generated_text"] if response else "æ— æ³•ç”ŸæˆæŠ¥å‘Š"
                
                # è§£æAIç”Ÿæˆçš„æŠ¥å‘Š
                analysis_result = self._parse_ai_analysis(generated_text)
                
                return {
                    "ai_generated_report": generated_text,
                    "structured_analysis": analysis_result,
                    "generation_successful": True
                }
            else:
                # é™çº§åˆ°åŸºç¡€åˆ†æ
                return self._fallback_quality_analysis(code_content)
                
        except Exception as e:
            return {"error": f"æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}"}

    async def _generate_improvement_suggestions(self, code_content: str) -> List[Dict[str, Any]]:
        """AIç”Ÿæˆæ”¹è¿›å»ºè®®"""
        try:
            improvement_prompt = f"""
            ä½œä¸ºä»£ç å®¡æŸ¥ä¸“å®¶ï¼Œä¸ºä»¥ä¸‹ä»£ç æä¾›å…·ä½“çš„æ”¹è¿›å»ºè®®:
            
            {code_content[:1500]}
            
            è¯·æä¾›:
            1. ä¼˜å…ˆçº§é«˜çš„æ”¹è¿›ç‚¹
            2. å…·ä½“çš„ä¿®æ”¹å»ºè®®
            3. æ”¹è¿›åçš„é¢„æœŸæ•ˆæœ
            """
            
            if self.text_generation_model:
                response = self.text_generation_model(
                    improvement_prompt,
                    max_length=300,
                    temperature=0.6
                )
                
                suggestions_text = response[0]["generated_text"] if response else ""
                
                # è§£æå»ºè®®ä¸ºç»“æ„åŒ–æ•°æ®
                suggestions = self._parse_suggestions(suggestions_text)
                
                return suggestions
            else:
                return self._fallback_improvement_suggestions(code_content)
                
        except Exception as e:
            return [{"error": f"å»ºè®®ç”Ÿæˆå¤±è´¥: {e}"}]

    async def _generate_refactoring_suggestions(self, code_content: str) -> Dict[str, Any]:
        """AIç”Ÿæˆé‡æ„å»ºè®®"""
        try:
            refactoring_prompt = self.refactoring_prompt.format(code_content=code_content[:1500])
            
            if self.text_generation_model:
                response = self.text_generation_model(
                    refactoring_prompt,
                    max_length=400,
                    temperature=0.5
                )
                
                refactoring_text = response[0]["generated_text"] if response else ""
                
                return {
                    "ai_refactoring_plan": refactoring_text,
                    "refactoring_priority": "medium",
                    "estimated_effort": "2-4 hours",
                    "expected_improvements": ["å¯è¯»æ€§æå‡", "ç»´æŠ¤æ€§å¢å¼º", "æ€§èƒ½ä¼˜åŒ–"]
                }
            else:
                return self._fallback_refactoring_suggestions(code_content)
                
        except Exception as e:
            return {"error": f"é‡æ„å»ºè®®ç”Ÿæˆå¤±è´¥: {e}"}

    def _split_code_into_chunks(self, code_content: str, max_length: int = 256) -> List[str]:
        """å°†ä»£ç åˆ†å‰²æˆè¾ƒå°çš„å—ä»¥é€‚åº”CPUå†…å­˜é™åˆ¶"""
        if len(code_content) <= max_length:
            return [code_content]
        
        chunks = []
        lines = code_content.split('\n')
        current_chunk = ""
        
        for line in lines:
            # å¦‚æœå•è¡Œå°±è¶…è¿‡æœ€å¤§é•¿åº¦ï¼Œç›´æ¥æˆªæ–­
            if len(line) > max_length:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                    current_chunk = ""
                chunks.append(line[:max_length])
                continue
            
            # æ£€æŸ¥æ·»åŠ è¿™ä¸€è¡Œæ˜¯å¦ä¼šè¶…è¿‡é™åˆ¶
            if len(current_chunk) + len(line) + 1 <= max_length:
                current_chunk += line + "\n"
            else:
                if current_chunk:
                    chunks.append(current_chunk.strip())
                current_chunk = line + "\n"
        
        # æ·»åŠ æœ€åä¸€ä¸ªå—
        if current_chunk:
            chunks.append(current_chunk.strip())
        
        # é™åˆ¶æœ€å¤§å—æ•°é‡ä»¥èŠ‚çœå†…å­˜
        return chunks[:10]

    def _parse_ai_analysis(self, generated_text: str) -> Dict[str, Any]:
        """è§£æAIç”Ÿæˆçš„åˆ†ææ–‡æœ¬ä¸ºç»“æ„åŒ–æ•°æ®"""
        # ç®€å•çš„è§£æé€»è¾‘ï¼Œå®é™…åº”ç”¨ä¸­å¯ä»¥æ›´å¤æ‚
        lines = generated_text.split('\n')
        
        analysis = {
            "issues_found": [],
            "quality_score": 7.0,  # é»˜è®¤åˆ†æ•°
            "recommendations": []
        }
        
        for line in lines:
            if "é—®é¢˜" in line or "issue" in line.lower():
                analysis["issues_found"].append(line.strip())
            elif "å»ºè®®" in line or "recommend" in line.lower():
                analysis["recommendations"].append(line.strip())
            elif "åˆ†æ•°" in line or "score" in line.lower():
                # å°è¯•æå–åˆ†æ•°
                import re
                score_match = re.search(r'(\d+(?:\.\d+)?)', line)
                if score_match:
                    analysis["quality_score"] = float(score_match.group(1))
        
        return analysis

    def _parse_suggestions(self, suggestions_text: str) -> List[Dict[str, Any]]:
        """è§£æå»ºè®®æ–‡æœ¬ä¸ºç»“æ„åŒ–æ•°æ®"""
        suggestions = []
        lines = suggestions_text.split('\n')
        
        for i, line in enumerate(lines):
            if line.strip() and not line.startswith('#'):
                suggestions.append({
                    "suggestion_id": i + 1,
                    "description": line.strip(),
                    "priority": "medium",
                    "category": "general"
                })
        
        return suggestions[:5]  # é™åˆ¶æ•°é‡

    async def _read_code_files(self, code_directory: str) -> str:
        """è¯»å–ç›®å½•ä¸­çš„ä»£ç æ–‡ä»¶"""
        import os
        
        code_content = ""
        supported_extensions = ['.py', '.js', '.java', '.cpp', '.c', '.cs', '.go', '.rs']
        
        try:
            for root, dirs, files in os.walk(code_directory):
                for file in files[:10]:  # é™åˆ¶æ–‡ä»¶æ•°é‡
                    if any(file.endswith(ext) for ext in supported_extensions):
                        file_path = os.path.join(root, file)
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                                code_content += f"\n\n# File: {file}\n{content}\n"
                        except Exception as e:
                            continue
                            
                if len(code_content) > 10000:  # é™åˆ¶æ€»é•¿åº¦
                    break
                    
        except Exception as e:
            print(f"è¯»å–ä»£ç æ–‡ä»¶æ—¶å‡ºé”™: {e}")
            
        return code_content

    def _fallback_quality_analysis(self, code_content: str) -> Dict[str, Any]:
        """AIæ¨¡å‹ä¸å¯ç”¨æ—¶çš„é™çº§åˆ†æ"""
        return {
            "fallback_analysis": True,
            "basic_metrics": {
                "lines_of_code": len(code_content.split('\n')),
                "estimated_complexity": "medium",
                "has_comments": "TODO" in code_content or "FIXME" in code_content
            },
            "basic_recommendations": [
                "å»ºè®®ä½¿ç”¨AIæ¨¡å‹è¿›è¡Œæ›´è¯¦ç»†çš„åˆ†æ",
                "æ£€æŸ¥ä»£ç æ³¨é‡Šå’Œæ–‡æ¡£",
                "è€ƒè™‘æ·»åŠ å•å…ƒæµ‹è¯•"
            ]
        }

    def _fallback_improvement_suggestions(self, code_content: str) -> List[Dict[str, Any]]:
        """é™çº§çš„æ”¹è¿›å»ºè®®"""
        return [
            {
                "suggestion_id": 1,
                "description": "å»ºè®®ä½¿ç”¨AIæ¨¡å‹è·å¾—æ›´ç²¾ç¡®çš„åˆ†æ",
                "priority": "high",
                "category": "system"
            }
        ]

    def _fallback_refactoring_suggestions(self, code_content: str) -> Dict[str, Any]:
        """é™çº§çš„é‡æ„å»ºè®®"""
        return {
            "fallback_refactoring": True,
            "basic_suggestions": [
                "æ£€æŸ¥å‡½æ•°é•¿åº¦å’Œå¤æ‚åº¦",
                "æå–é‡å¤ä»£ç ",
                "æ”¹å–„å‘½åè§„èŒƒ"
            ]
        }

    async def _execute_task_impl(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡ŒAIé©±åŠ¨çš„è´¨é‡åˆ†æä»»åŠ¡"""
        return await self._ai_driven_quality_analysis(
            task_data.get("code_content", ""),
            task_data.get("code_directory", "")
        )

    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        import datetime
        return datetime.datetime.now().isoformat()
