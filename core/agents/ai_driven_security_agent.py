import os
import torch
import asyncio
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification
from typing import Dict, Any, List, Tuple
from .base_agent import BaseAgent, Message
from infrastructure.database.sqlite.service import DatabaseService
from infrastructure.config.settings import HUGGINGFACE_CONFIG
from infrastructure.config.ai_agents import get_ai_agent_config
from infrastructure.config.prompts import get_prompt
from infrastructure.reports import report_manager

class AIDrivenSecurityAgent(BaseAgent):
    """AIé©±åŠ¨çš„å®‰å…¨åˆ†ææ™ºèƒ½ä½“ - åŸºäºpromptå·¥ç¨‹å’Œæ¨¡å‹æ¨ç†"""
    
    def __init__(self):
        super().__init__("ai_security_agent", "AIé©±åŠ¨å®‰å…¨åˆ†ææ™ºèƒ½ä½“")
        self.db_service = DatabaseService()
        # ä»ç»Ÿä¸€é…ç½®è·å–
        self.agent_config = get_ai_agent_config().get_security_agent_config()
        self.model_config = HUGGINGFACE_CONFIG["models"]["security"]
        # ç§»é™¤æœ¬åœ°ç¡¬ç¼–ç promptï¼Œç»Ÿä¸€ä½¿ç”¨ prompts.get_prompt
        self.security_model = None
        self.vulnerability_classifier = None
        self.threat_analyzer = None
        
    async def _initialize_models(self):
        """åˆå§‹åŒ–AIæ¨¡å‹ - CPUä¼˜åŒ–ç‰ˆæœ¬"""
        try:
            print("ğŸ”§ åˆå§‹åŒ–å®‰å…¨åˆ†æAIæ¨¡å‹ (CPUæ¨¡å¼)...")
            os.environ['CUDA_VISIBLE_DEVICES'] = ''
            cpu_threads = self.agent_config.get("cpu_threads", 4)
            torch.set_num_threads(cpu_threads)
            try:
                model_name = self.agent_config.get("model_name", "microsoft/codebert-base")
                torch_dtype = getattr(torch, self.agent_config.get("torch_dtype", "float32"))
                self.security_model = pipeline(
                    "text-classification",
                    model=model_name,
                    device=-1,
                    model_kwargs={
                        "low_cpu_mem_usage": self.agent_config.get("low_cpu_mem_usage", True),
                        "torch_dtype": torch_dtype
                    }
                )
                print(f"âœ… {model_name} å®‰å…¨æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ (CPU)")
            except Exception as e:
                print(f"âš ï¸ ä¸»æ¨¡å‹åŠ è½½å¤±è´¥,å°è¯•å¤‡ç”¨æ¨¡å‹: {e}")
                fallback_model = self.agent_config.get("fallback_model", "distilbert-base-uncased")
                self.security_model = pipeline(
                    "text-classification",
                    model=fallback_model,
                    device=-1,
                    model_kwargs={"low_cpu_mem_usage": True}
                )
                print(f"âœ… {fallback_model} å¤‡ç”¨æ¨¡å‹åˆå§‹åŒ–æˆåŠŸ (CPU)")
            try:
                text_gen_model = self.agent_config.get("text_generator_model", "gpt2")
                self.text_generator = pipeline(
                    "text-generation",
                    model=text_gen_model,
                    device=-1,
                    model_kwargs={"low_cpu_mem_usage": True, "pad_token_id": 50256}
                )
                print(f"âœ… {text_gen_model} æ–‡æœ¬ç”Ÿæˆæ¨¡å‹åˆå§‹åŒ–æˆåŠŸ (CPU)")
                # é‡‡ç”¨æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ä½œä¸ºå¨èƒå»ºæ¨¡ç”Ÿæˆå™¨
                self.threat_analyzer = self.text_generator
            except Exception as e:
                print(f"âš ï¸ æ–‡æœ¬ç”Ÿæˆæ¨¡å‹åŠ è½½å¤±è´¥: {e}")
                self.text_generator = None
                self.threat_analyzer = None
            self.models_loaded = True
            print("âœ… å®‰å…¨åˆ†æAIæ¨¡å‹åˆå§‹åŒ–å®Œæˆ (CPUä¼˜åŒ–æ¨¡å¼)")
        except Exception as e:
            print(f"âŒ å®‰å…¨åˆ†æAIæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.models_loaded = False
            self.security_model = None
            self.text_generator = None
            self.threat_analyzer = None

    async def handle_message(self, message: Message):
        """å¤„ç†å®‰å…¨åˆ†æè¯·æ±‚"""
        if message.message_type == "security_analysis_request":
            requirement_id = message.content.get("requirement_id")
            code_content = message.content.get("code_content", "")
            code_directory = message.content.get("code_directory", "")
            file_path = message.content.get("file_path")
            run_id = message.content.get('run_id')
            
            print(f"ğŸ”’ AIå®‰å…¨åˆ†æå¼€å§‹ - éœ€æ±‚ID: {requirement_id}")
            
            if not self.security_model:
                await self._initialize_models()
            
            # æ‰§è¡ŒAIé©±åŠ¨çš„å®‰å…¨åˆ†æ
            result = await self._ai_driven_security_analysis(code_content, code_directory)
            if run_id:
                try:
                    agent_payload = {
                        "requirement_id": requirement_id,
                        "file_path": file_path,
                        "run_id": run_id,
                        "security_result": result,
                        "generated_at": self._get_current_time()
                    }
                    report_manager.generate_run_scoped_report(run_id, agent_payload, f"security_req_{requirement_id}.json", subdir="agents/security")
                except Exception as e:
                    print(f"âš ï¸ å®‰å…¨Agentå•ç‹¬æŠ¥å‘Šç”Ÿæˆå¤±è´¥ requirement={requirement_id} run_id={run_id}: {e}")
            # å‘é€ç»“æœ
            await self.send_message(
                receiver="user_comm_agent",
                content={
                    "requirement_id": requirement_id,
                    "agent_type": "ai_security",
                    "results": result,
                    "analysis_complete": True,
                    "file_path": file_path,
                    "run_id": run_id
                },
                message_type="analysis_result"
            )
            await self.send_message(
                receiver="summary_agent",
                content={
                    "requirement_id": requirement_id,
                    "analysis_type": "security_analysis",
                    "result": result,
                    "file_path": file_path,
                    "run_id": run_id
                },
                message_type="analysis_result"
            )
            
            print(f"âœ… AIå®‰å…¨åˆ†æå®Œæˆ - éœ€æ±‚ID: {requirement_id}")

    async def _ai_driven_security_analysis(self, code_content: str, code_directory: str) -> Dict[str, Any]:
        """AIé©±åŠ¨çš„å…¨é¢å®‰å…¨åˆ†æ"""
        
        try:
            print("ğŸ” AIæ­£åœ¨è¿›è¡Œæ·±åº¦å®‰å…¨åˆ†æ...")
            code_context = await self._analyze_code_context(code_directory)
            vulnerabilities = await self._ai_vulnerability_detection(code_content, code_context)
            threat_model = await self._ai_threat_modeling(code_content, code_context)
            security_rating = await self._ai_security_rating(vulnerabilities, threat_model)
            remediation_plan = await self._ai_remediation_planning(vulnerabilities)
            hardening_recommendations = await self._ai_security_hardening(code_content, code_context)
            
            print("ğŸ›¡ï¸  AIå®‰å…¨åˆ†æå®Œæˆ,ç”Ÿæˆå®‰å…¨æŠ¥å‘Š")
            
            return {
                "ai_security_analysis": {
                    "overall_security_rating": security_rating,
                    "vulnerabilities_detected": vulnerabilities,
                    "threat_model": threat_model,
                    "remediation_plan": remediation_plan,
                    "hardening_recommendations": hardening_recommendations,
                    "code_context": code_context,
                    "ai_confidence": 0.92,
                    "model_used": self.model_config["name"],
                    "analysis_timestamp": self._get_current_time()
                },
                "analysis_status": "completed"
            }
            
        except Exception as e:
            print(f"âŒ AIå®‰å…¨åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return {
                "ai_security_analysis": {"error": str(e)},
                "analysis_status": "failed"
            }

    async def _analyze_code_context(self, code_directory: str) -> Dict[str, Any]:
        """åˆ†æä»£ç ç¯å¢ƒå’Œä¸Šä¸‹æ–‡"""
        context = {
            "application_type": "unknown",
            "framework_detected": [],
            "database_usage": False,
            "network_operations": False,
            "authentication_present": False,
            "encryption_usage": False,
            "data_sensitivity": "medium"
        }
        
        if code_directory:
            # è¯»å–ä»£ç æ–‡ä»¶å¹¶åˆ†æ
            code_files = await self._read_security_relevant_files(code_directory)
            
            # AIåˆ†æåº”ç”¨ç±»å‹
            for file_content in code_files[:5]:
                if "flask" in file_content.lower() or "django" in file_content.lower():
                    context["application_type"] = "web_application"
                    context["framework_detected"].append("Python Web Framework")
                
                if "password" in file_content.lower() or "auth" in file_content.lower():
                    context["authentication_present"] = True
                    
                if "sql" in file_content.lower() or "database" in file_content.lower():
                    context["database_usage"] = True
                    
                if "requests" in file_content.lower() or "http" in file_content.lower():
                    context["network_operations"] = True
                    
                if "encrypt" in file_content.lower() or "hash" in file_content.lower():
                    context["encryption_usage"] = True
        
        return context

    async def _ai_vulnerability_detection(self, code_content: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """AIé©±åŠ¨çš„æ¼æ´æ£€æµ‹"""
        vulnerabilities = []
        
        try:
            # å°†ä»£ç åˆ†å—è¿›è¡Œåˆ†æ
            code_chunks = self._split_code_for_analysis(code_content)
            
            for i, chunk in enumerate(code_chunks[:3]):  # é™åˆ¶åˆ†æå—æ•°
                security_prompt = get_prompt(
                    task_type="security",
                    variant="vulnerability_detection",
                    code_snippet=chunk
                )
                
                # ä½¿ç”¨AIæ¨¡å‹è¿›è¡Œæ¼æ´åˆ†ç±»
                if self.vulnerability_classifier:
                    classification_result = self.vulnerability_classifier(
                        f"Security analysis: {chunk[:500]}"
                    )
                    
                    # è§£æAIåˆ†æç»“æœ
                    vuln_data = await self._parse_vulnerability_result(
                        classification_result, chunk, i
                    )
                    
                    if vuln_data:
                        vulnerabilities.append(vuln_data)
                
                # ä½¿ç”¨å¨èƒåˆ†æå™¨ç”Ÿæˆè¯¦ç»†åˆ†æ
                if self.threat_analyzer and len(vulnerabilities) < 3:
                    threat_analysis = self.threat_analyzer(
                        security_prompt,
                        max_length=200,
                        temperature=0.3
                    )
                    
                    detailed_vuln = await self._extract_vulnerability_details(
                        threat_analysis, chunk, i
                    )
                    
                    if detailed_vuln:
                        vulnerabilities.append(detailed_vuln)
            
            # AIé£é™©è¯„ä¼°å’Œä¼˜å…ˆçº§æ’åº
            vulnerabilities = await self._ai_risk_assessment(vulnerabilities)
            
        except Exception as e:
            vulnerabilities.append({
                "vulnerability_id": "AI_ERROR_001",
                "type": "analysis_error",
                "description": f"AIåˆ†æè¿‡ç¨‹å‡ºé”™: {e}",
                "severity": "info"
            })
        
        return vulnerabilities

    async def _ai_threat_modeling(self, code_content: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """AIé©±åŠ¨çš„å¨èƒå»ºæ¨¡"""
        try:
            threat_prompt = get_prompt(
                task_type="security",
                variant="threat_modeling",
                system_components=str(context.get("framework_detected", [])),
                data_flow="User Input -> Application -> Database -> Response",
                code_content=code_content[:1000]
            )
            if self.threat_analyzer:
                try:
                    threat_analysis = self.threat_analyzer(
                        threat_prompt,
                        max_length=300,
                        temperature=0.4
                    )
                    threat_model = await self._parse_threat_model(threat_analysis)
                    return threat_model
                except Exception as gen_err:
                    print(f"âš ï¸ å¨èƒå»ºæ¨¡ç”Ÿæˆå¤±è´¥,é™çº§ä½¿ç”¨fallback: {gen_err}")
                    return self._fallback_threat_model(context)
            else:
                print("âš ï¸ å¨èƒå»ºæ¨¡ç”Ÿæˆå™¨æœªåˆå§‹åŒ–,ä½¿ç”¨fallbackç®€åŒ–æ¨¡å‹")
                return self._fallback_threat_model(context)
        except Exception as e:
            print(f"âš ï¸ å¨èƒå»ºæ¨¡promptæ„é€ æˆ–å¤„ç†å¼‚å¸¸: {e}")
            return {"error": f"å¨èƒå»ºæ¨¡å¤±è´¥: {e}"}

    async def _ai_security_rating(self, vulnerabilities: List[Dict[str, Any]], 
                                 threat_model: Dict[str, Any]) -> Dict[str, Any]:
        """AIé©±åŠ¨çš„å®‰å…¨è¯„çº§"""
        try:
            # è®¡ç®—åŸºç¡€å®‰å…¨åˆ†æ•°
            base_score = 10.0
            
            # æ ¹æ®æ¼æ´ä¸¥é‡ç¨‹åº¦è°ƒæ•´åˆ†æ•°
            for vuln in vulnerabilities:
                severity = vuln.get("severity", "low")
                if severity == "critical":
                    base_score -= 3.0
                elif severity == "high":
                    base_score -= 2.0
                elif severity == "medium":
                    base_score -= 1.0
                elif severity == "low":
                    base_score -= 0.5
            
            # æ ¹æ®å¨èƒæ¨¡å‹è°ƒæ•´åˆ†æ•°
            threat_score = threat_model.get("overall_risk_score", 5.0)
            adjusted_score = (base_score + threat_score) / 2
            
            # ç¡®ä¿åˆ†æ•°åœ¨åˆç†èŒƒå›´å†…
            final_score = max(0.0, min(10.0, adjusted_score))
            
            # AIç”Ÿæˆè¯„çº§è¯´æ˜
            rating_explanation = await self._generate_rating_explanation(
                final_score, vulnerabilities, threat_model
            )
            
            return {
                "security_score": final_score,
                "rating_level": self._score_to_rating(final_score),
                "explanation": rating_explanation,
                "factors_considered": [
                    "æ¼æ´æ•°é‡å’Œä¸¥é‡ç¨‹åº¦",
                    "å¨èƒæ¨¡å‹åˆ†æ",
                    "å®‰å…¨æ§åˆ¶æªæ–½",
                    "ä»£ç è´¨é‡æŒ‡æ ‡"
                ]
            }
            
        except Exception as e:
            return {"error": f"å®‰å…¨è¯„çº§å¤±è´¥: {e}"}

    async def _ai_remediation_planning(self, vulnerabilities: List[Dict[str, Any]]) -> Dict[str, Any]:
        """AIç”Ÿæˆä¿®å¤è®¡åˆ’"""
        try:
            remediation_plan = {
                "immediate_actions": [],
                "short_term_fixes": [],
                "long_term_improvements": [],
                "estimated_effort": "unknown"
            }
            
            for vuln in vulnerabilities:
                severity = vuln.get("severity", "low")
                fix_suggestion = await self._generate_fix_suggestion(vuln)
                
                if severity in ["critical", "high"]:
                    remediation_plan["immediate_actions"].append(fix_suggestion)
                elif severity == "medium":
                    remediation_plan["short_term_fixes"].append(fix_suggestion)
                else:
                    remediation_plan["long_term_improvements"].append(fix_suggestion)
            
            # AIä¼°ç®—ä¿®å¤å·¥ä½œé‡
            total_vulns = len(vulnerabilities)
            if total_vulns <= 2:
                remediation_plan["estimated_effort"] = "1-2 days"
            elif total_vulns <= 5:
                remediation_plan["estimated_effort"] = "3-5 days"
            else:
                remediation_plan["estimated_effort"] = "1-2 weeks"
            
            return remediation_plan
            
        except Exception as e:
            return {"error": f"ä¿®å¤è®¡åˆ’ç”Ÿæˆå¤±è´¥: {e}"}

    async def _ai_security_hardening(self, code_content: str, context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """AIç”Ÿæˆå®‰å…¨åŠ å›ºå»ºè®®"""
        hardening_recommendations = []
        
        try:
            # åŸºäºä¸Šä¸‹æ–‡ç”Ÿæˆé’ˆå¯¹æ€§å»ºè®®
            if context.get("application_type") == "web_application":
                hardening_recommendations.extend([
                    {
                        "category": "è¾“å…¥éªŒè¯",
                        "recommendation": "å®æ–½ä¸¥æ ¼çš„è¾“å…¥éªŒè¯å’Œè¾“å‡ºç¼–ç ",
                        "priority": "high",
                        "implementation": "ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢å’Œè¾“å…¥éªŒè¯åº“"
                    },
                    {
                        "category": "èº«ä»½è®¤è¯",
                        "recommendation": "å®æ–½å¤šå› ç´ è®¤è¯",
                        "priority": "medium",
                        "implementation": "é›†æˆTOTPæˆ–SMSéªŒè¯"
                    }
                ])
            
            if context.get("database_usage"):
                hardening_recommendations.append({
                    "category": "æ•°æ®åº“å®‰å…¨",
                    "recommendation": "ä½¿ç”¨æ•°æ®åº“è¿æ¥æ± å’Œæœ€å°æƒé™åŸåˆ™",
                    "priority": "high",
                    "implementation": "é…ç½®ä¸“ç”¨æ•°æ®åº“ç”¨æˆ·,é™åˆ¶æƒé™"
                })
            
            # AIç”Ÿæˆä¸ªæ€§åŒ–å»ºè®®
            if len(code_content) > 500:
                custom_recommendations = await self._generate_custom_hardening(code_content)
                hardening_recommendations.extend(custom_recommendations)
            
        except Exception as e:
            hardening_recommendations.append({
                "category": "é”™è¯¯",
                "recommendation": f"å®‰å…¨åŠ å›ºå»ºè®®ç”Ÿæˆå¤±è´¥: {e}",
                "priority": "info"
            })
        
        return hardening_recommendations

    # è¾…åŠ©æ–¹æ³•
    async def _parse_vulnerability_result(self, classification_result: List[Dict], 
                                        code_chunk: str, chunk_index: int) -> Dict[str, Any]:
        """è§£ææ¼æ´åˆ†æç»“æœ"""
        if not classification_result:
            return None
            
        result = classification_result[0]
        confidence = result.get("score", 0.0)
        
        # åªæœ‰å½“ç½®ä¿¡åº¦è¾ƒé«˜æ—¶æ‰æŠ¥å‘Šæ¼æ´
        if confidence > 0.7:
            return {
                "vulnerability_id": f"AI_VULN_{chunk_index:03d}",
                "type": "ai_detected_issue",
                "description": f"AIæ£€æµ‹åˆ°æ½œåœ¨å®‰å…¨é—®é¢˜ (ç½®ä¿¡åº¦: {confidence:.2f})",
                "severity": "medium" if confidence > 0.85 else "low",
                "location": f"ä»£ç å— {chunk_index + 1}",
                "code_snippet": code_chunk[:200],
                "ai_confidence": confidence
            }
        
        return None

    def _split_code_for_analysis(self, code_content: str, chunk_size: int = 800) -> List[str]:
        """å°†ä»£ç åˆ†å‰²æˆé€‚åˆå®‰å…¨åˆ†æçš„å—"""
        # æŒ‰å‡½æ•°æˆ–ç±»åˆ†å‰²ä¼šæ›´å¥½,è¿™é‡Œç®€åŒ–å¤„ç†
        lines = code_content.split('\n')
        chunks = []
        current_chunk = []
        current_size = 0
        
        for line in lines:
            if current_size + len(line) > chunk_size and current_chunk:
                chunks.append('\n'.join(current_chunk))
                current_chunk = [line]
                current_size = len(line)
            else:
                current_chunk.append(line)
                current_size += len(line)
        
        if current_chunk:
            chunks.append('\n'.join(current_chunk))
        
        return chunks

    def _score_to_rating(self, score: float) -> str:
        """å°†æ•°å­—åˆ†æ•°è½¬æ¢ä¸ºç­‰çº§è¯„ä»·"""
        if score >= 8.5:
            return "Excellent"
        elif score >= 7.0:
            return "Good"
        elif score >= 5.0:
            return "Fair"
        elif score >= 3.0:
            return "Poor"
        else:
            return "Critical"

    async def _read_security_relevant_files(self, code_directory: str) -> List[str]:
        """è¯»å–å®‰å…¨ç›¸å…³çš„ä»£ç æ–‡ä»¶"""
        import os
        
        security_files = []
        security_keywords = ["auth", "login", "password", "security", "crypto", "hash"]
        
        try:
            for root, dirs, files in os.walk(code_directory):
                for file in files[:10]:
                    if (file.endswith(('.py', '.js', '.java', '.php')) or 
                        any(keyword in file.lower() for keyword in security_keywords)):
                        
                        file_path = os.path.join(root, file)
                        try:
                            with open(file_path, 'r', encoding='utf-8') as f:
                                content = f.read()
                                security_files.append(content)
                        except Exception:
                            continue
                            
                if len(security_files) >= 5:
                    break
                    
        except Exception as e:
            print(f"è¯»å–å®‰å…¨ç›¸å…³æ–‡ä»¶æ—¶å‡ºé”™: {e}")
        
        return security_files

    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³ (è¡¥å……ç¼ºå¤±çš„æ–¹æ³•ä»¥é¿å…è¿è¡Œæ—¶æŠ¥é”™)"""
        import datetime
        return datetime.datetime.now().isoformat()

    # --- Newly added helper / AI synthesis methods to avoid missing attribute errors ---
    async def _ai_risk_assessment(self, vulnerabilities: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ä¸ºæ£€æµ‹åˆ°çš„æ¼æ´æ‰§è¡Œç®€æ˜“é£é™©è¯„ä¼°ä¸æ’åºã€‚
        - è®¡ç®— risk_score (0-10)
        - è§„èŒƒ severity å­—æ®µ (critical/high/medium/low/info)
        - æ ¹æ®è¯„åˆ†è¿›è¡Œæ’åº
        """
        assessed: List[Dict[str, Any]] = []
        for v in vulnerabilities:
            sev = v.get("severity") or "info"
            # åŸºç¡€ä¸¥é‡åº¦æƒé‡
            base = {
                "critical": 9.0,
                "high": 7.5,
                "medium": 5.5,
                "low": 3.0,
                "info": 1.0
            }.get(sev, 2.0)
            # åˆ©ç”¨ç½®ä¿¡åº¦æå‡
            confidence = float(v.get("ai_confidence", 0.5))
            risk_score = min(10.0, base + confidence * 1.5)
            v["risk_score"] = round(risk_score, 2)
            # è‹¥ severity ç¼ºå¤±, æŒ‰é£é™©è¯„åˆ†æ¨å¯¼
            if sev not in ["critical", "high", "medium", "low", "info"]:
                if risk_score >= 8.5:
                    v["severity"] = "high"
                elif risk_score >= 6.5:
                    v["severity"] = "medium"
                elif risk_score >= 4.0:
                    v["severity"] = "low"
                else:
                    v["severity"] = "info"
            assessed.append(v)
        # æŒ‰é£é™©æ’åº
        assessed.sort(key=lambda x: x.get("risk_score", 0.0), reverse=True)
        return assessed

    async def _generate_rating_explanation(self, final_score: float, vulnerabilities: List[Dict[str, Any]], threat_model: Dict[str, Any]) -> str:
        """ç”Ÿæˆå®‰å…¨è¯„åˆ†è§£é‡Šæ–‡æœ¬ã€‚ä½¿ç”¨è½»é‡é€»è¾‘ + å¯é€‰æ–‡æœ¬ç”Ÿæˆæ¨¡å‹ã€‚"""
        high_count = sum(1 for v in vulnerabilities if v.get("severity") in {"critical", "high"})
        medium_count = sum(1 for v in vulnerabilities if v.get("severity") == "medium")
        low_count = sum(1 for v in vulnerabilities if v.get("severity") == "low")
        rating_level = self._score_to_rating(final_score)
        stride_summary = threat_model.get("stride_summary") or threat_model.get("summary") or "(æ— è¯¦ç»†å¨èƒæ¨¡å‹)"
        base_text = (
            f"æ€»ä½“å®‰å…¨è¯„åˆ† {final_score:.2f} ({rating_level}). "
            f"é«˜/ä¸¥é‡æ¼æ´: {high_count}, ä¸­ç­‰: {medium_count}, ä½: {low_count}. "
            f"å¨èƒå»ºæ¨¡æ‘˜è¦: {stride_summary}. "
            "è¯„åˆ†åŸºäºå‘ç°æ¼æ´çš„æ•°é‡ä¸ä¸¥é‡åº¦ã€å¨èƒç±»åˆ«è¦†ç›–åŠä»£ç ä¸Šä¸‹æ–‡ä¸­çš„å®‰å…¨æ§åˆ¶è¿¹è±¡ã€‚"
        )
        # è‹¥æœ‰æ–‡æœ¬ç”Ÿæˆæ¨¡å‹, æ·»åŠ æ›´è‡ªç„¶è¯­è¨€è¡¥å……
        if getattr(self, "text_generator", None):
            try:
                gen = self.text_generator(
                    base_text + " è¯·ç”¨ä¸€å¥è¯æ€»ç»“é£é™©ä¼˜å…ˆçº§ã€‚",
                    max_length=base_text.count(" ") + 40,
                    num_return_sequences=1,
                    temperature=0.7,
                    pad_token_id=50256
                )
                if gen and isinstance(gen, list):
                    completion = gen[0].get("generated_text", "")
                    # å»é‡åˆå¹¶
                    if completion and completion not in base_text:
                        base_text += " " + completion.strip()[:200]
            except Exception:
                pass
        return base_text

    async def _generate_fix_suggestion(self, vuln: Dict[str, Any]) -> str:
        """æ ¹æ®æ¼æ´æ¡ç›®ç”Ÿæˆä¿®å¤å»ºè®® (å¯å‘å¼)ã€‚"""
        vtype = (vuln.get("type") or "issue").lower()
        desc = (vuln.get("description") or "").lower()
        if "injection" in vtype or "sql" in desc:
            return "ä½¿ç”¨å‚æ•°åŒ–æŸ¥è¯¢å¹¶ä¸¥æ ¼æ ¡éªŒ/è½¬ä¹‰æ‰€æœ‰å¤–éƒ¨è¾“å…¥ã€‚"
        if "xss" in vtype or "script" in desc:
            return "å¯¹è¾“å‡ºè¿›è¡ŒHTMLè½¬ä¹‰å¹¶ä½¿ç”¨å†…å®¹å®‰å…¨ç­–ç•¥(CSP)ã€‚"
        if "auth" in desc or "login" in desc:
            return "å®æ–½å¼ºå¯†ç ç­–ç•¥å¹¶å¢åŠ å¤šå› ç´ è®¤è¯ï¼Œé™åˆ¶å¤±è´¥å°è¯•ã€‚"
        if "crypto" in desc or "encrypt" in desc or "hash" in desc:
            return "ä½¿ç”¨ç»éªŒè¯çš„åº“(å¦‚ hashlib/cryptography)å¹¶åº”ç”¨ç›å€¼+è¿­ä»£ã€‚"
        if "config" in desc:
            return "æ£€æŸ¥é»˜è®¤é…ç½®å¹¶æœ€å°åŒ–æƒé™ï¼Œç§»é™¤æœªä½¿ç”¨ç«¯ç‚¹æˆ–è°ƒè¯•æ ‡å¿—ã€‚"
        # ç½®ä¿¡åº¦é«˜ä¸”æ— åŒ¹é…è§„åˆ™ -> é€šç”¨å»ºè®®
        if vuln.get("ai_confidence", 0) > 0.8:
            return "å®¡æŸ¥æ­¤é«˜ç½®ä¿¡åº¦æ¡ç›®ï¼Œæ·»åŠ è¾“å…¥éªŒè¯ä¸è®¿é—®æ§åˆ¶å®¡æŸ¥ã€‚"
        return "è¿›è¡Œä»£ç å®¡æŸ¥å¹¶æ·»åŠ è¾“å…¥éªŒè¯ã€é”™è¯¯å¤„ç†ä¸æœ€å°æƒé™ç­–ç•¥ã€‚"

    async def _generate_custom_hardening(self, code_content: str) -> List[Dict[str, Any]]:
        """åŸºäºä»£ç æ¨¡å¼ç”Ÿæˆå®šåˆ¶åŠ å›ºå»ºè®®ã€‚"""
        recs: List[Dict[str, Any]] = []
        lowered = code_content.lower()
        def add(category, recommendation, priority, implementation):
            recs.append({
                "category": category,
                "recommendation": recommendation,
                "priority": priority,
                "implementation": implementation
            })
        if "exec(" in lowered or "eval(" in lowered:
            add("å±é™©è°ƒç”¨", "é¿å…ä½¿ç”¨ eval/exec, æ”¹ä¸ºæ˜¾å¼é€»è¾‘æˆ–å®‰å…¨è§£æåº“", "high", "ç§»é™¤æˆ–æ›¿æ¢ eval/exec")
        if "subprocess" in lowered:
            add("å‘½ä»¤æ‰§è¡Œ", "ä½¿ç”¨å®‰å…¨å‚æ•°åˆ—è¡¨å¹¶é¿å… shell=True", "medium", "subprocess.run([...], shell=False)")
        if "password" in lowered and "hash" not in lowered:
            add("å‡­æ®å¤„ç†", "ç¡®ä¿å¯¹å¯†ç è¿›è¡Œå“ˆå¸Œå­˜å‚¨ (bcrypt/argon2)", "high", "é›†æˆ passlib æˆ– argon2 åº“")
        if "http://" in lowered:
            add("ä¼ è¾“å®‰å…¨", "å‡çº§åˆ° HTTPS ä»¥é˜²æ­¢ä¸­é—´äººæ”»å‡»", "medium", "æ›¿æ¢æ‰€æœ‰ http:// é“¾æ¥ä¸º https://")
        if "debug" in lowered:
            add("è°ƒè¯•é…ç½®", "ç”Ÿäº§ç¯å¢ƒå…³é—­è°ƒè¯•æ¨¡å¼ä¸è¯¦ç»†é”™è¯¯è¾“å‡º", "low", "è®¾ç½® DEBUG=False å¹¶ä½¿ç”¨ç»Ÿä¸€é”™è¯¯å¤„ç†")
        # å»é‡ä¸æœ‰é™é•¿åº¦
        return recs[:8]

    # --- Threat modeling helper methods (fix for missing _fallback_threat_model) ---
    def _fallback_threat_model(self, context: Dict[str, Any]) -> Dict[str, Any]:
        """åœ¨æœªåŠ è½½ç”Ÿæˆæ¨¡å‹æ—¶æä¾›ç®€æ˜“ STRIDE å¨èƒæ¨¡å‹ã€‚"""
        stride_categories = [
            ("Spoofing", "å¯èƒ½ç¼ºå°‘å¼ºèº«ä»½è®¤è¯" if not context.get("authentication_present") else "èº«ä»½è®¤è¯è¿¹è±¡å­˜åœ¨"),
            ("Tampering", "æœªå‘ç°å®Œæ•´æ€§æ ¡éªŒé€»è¾‘"),
            ("Repudiation", "ç¼ºå°‘å®¡è®¡/æ—¥å¿—æœºåˆ¶è¿¹è±¡"),
            ("Information Disclosure", "æ½œåœ¨ä¸­ç­‰é£é™©; æœªå‘ç°åŠ å¯†è°ƒç”¨" if not context.get("encryption_usage") else "å­˜åœ¨åŠ å¯†è¿¹è±¡"),
            ("Denial of Service", "èµ„æºæ§åˆ¶é€»è¾‘æœªæ˜¾å¼æ£€æµ‹"),
            ("Elevation of Privilege", "æƒé™è¾¹ç•Œæœªæ˜ç¡®")
        ]
        analyzed = []
        total_risk = 0.0
        for name, desc in stride_categories:
            # ç®€å•é£é™©æ‰“åˆ†: æ ¹æ®ä¸Šä¸‹æ–‡ç¼ºå¤±æƒ…å†µ
            base = 5.0
            if "è¿¹è±¡å­˜åœ¨" in desc:
                base -= 2.0
            analyzed.append({
                "category": name,
                "summary": desc,
                "risk_score": base
            })
            total_risk += base
        overall = round(total_risk / len(analyzed), 2)
        return {
            "stride_analysis": analyzed,
            "overall_risk_score": overall,
            "stride_summary": f"å…­ç±»å¹³å‡é£é™©è¯„åˆ† {overall}",
            "model": "fallback"
        }

    async def _parse_threat_model(self, threat_analysis: Any) -> Dict[str, Any]:
        """è§£ææ¨¡å‹ç”Ÿæˆçš„å¨èƒå»ºæ¨¡æ–‡æœ¬ä¸ºç»“æ„åŒ–æ•°æ®ã€‚"""
        # ç»Ÿä¸€ä¸ºæ–‡æœ¬
        if isinstance(threat_analysis, list):
            # transformers text-generation å¸¸ä¸º list[{'generated_text': str}]
            text = threat_analysis[0].get("generated_text", "") if threat_analysis else ""
        elif isinstance(threat_analysis, dict):
            text = threat_analysis.get("generated_text", str(threat_analysis))
        else:
            text = str(threat_analysis)
        lowered = text.lower()
        def extract_section(keyword: str) -> str:
            # ç²—ç³™æˆªå–: ä»å…³é”®å­—åˆ°ä¸‹ä¸€ä¸ªæ¢è¡Œæˆ– 160 å­—ç¬¦
            idx = lowered.find(keyword.lower())
            if idx == -1:
                return "æœªæåŠ"
            snippet = text[idx: idx + 180]
            return snippet.split('\n')[0][:160]
        categories = ["Spoofing", "Tampering", "Repudiation", "Information Disclosure", "Denial of Service", "Elevation of Privilege"]
        stride_analysis = []
        total = 0.0
        for cat in categories:
            detail = extract_section(cat)
            # ç®€å•è¯„åˆ†: å‡ºç°åˆ™ 4-6 ä¹‹é—´éšæœº, æœªå‡ºç°åˆ™ 5 é»˜è®¤ã€‚è¿™é‡Œç”¨è§„åˆ™: é•¿åº¦>20 => 5.5 else 4.5
            score = 5.5 if len(detail) > 20 and detail != "æœªæåŠ" else 4.5
            total += score
            stride_analysis.append({
                "category": cat,
                "summary": detail,
                "risk_score": score
            })
        overall = round(total / len(stride_analysis), 2)
        return {
            "stride_analysis": stride_analysis,
            "overall_risk_score": overall,
            "stride_summary": f"ç”Ÿæˆæ–‡æœ¬è§£æå¹³å‡é£é™© {overall}",
            "raw_text_length": len(text),
            "model": "generated"
        }

    async def _extract_vulnerability_details(self, threat_analysis: Any, code_chunk: str, chunk_index: int) -> Dict[str, Any]:
        """ä»ç”Ÿæˆçš„å¨èƒåˆ†æä¸­æå–æ½œåœ¨æ¼æ´è¯¦æƒ… (ç®€åŒ–å ä½å®ç°)ã€‚"""
        if not threat_analysis:
            return None
        # ä½¿ç”¨è§£æåçš„æ–‡æœ¬é•¿åº¦ä¸å…³é”®è¯ä½œä¸ºç½®ä¿¡åº¦ä¼°è®¡
        if isinstance(threat_analysis, list):
            text = threat_analysis[0].get("generated_text", "")
        elif isinstance(threat_analysis, dict):
            text = threat_analysis.get("generated_text", str(threat_analysis))
        else:
            text = str(threat_analysis)
        keywords = ["inject", "xss", "csrf", "overflow", "leak"]
        found = [k for k in keywords if k in text.lower()]
        if not found:
            return None
        return {
            "vulnerability_id": f"AI_GEN_{chunk_index:03d}",
            "type": "generated_threat_indicator",
            "description": f"ç”Ÿæˆå¨èƒæ–‡æœ¬ä¸­æåŠå…³é”®è¯: {', '.join(found)}",
            "severity": "medium" if len(found) > 1 else "low",
            "location": f"ä»£ç å— {chunk_index + 1}",
            "code_snippet": code_chunk[:160],
            "ai_confidence": min(0.95, 0.6 + 0.1 * len(found))
        }
    # ...existing code...
    # --- Backward compatibility layer for legacy synchronous tests ---
    def __getattr__(self, item):
        removed = {
            'scan_vulnerabilities', 'detect_sql_injection', 'detect_xss',
            'detect_insecure_deserialization', 'detect_hardcoded_secrets',
            'calculate_security_score', 'generate_security_report', 'analyze_file'
        }
        if item in removed:
            raise AttributeError(
                f"'{item}' removed. Use async workflow: send 'security_analysis_request' and await 'analysis_result'."
            )
        raise AttributeError(item)
    
    async def _execute_task_impl(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """Execute a security analysis task (required by BaseAgent)."""
        return await self._ai_driven_security_analysis(
            task_data.get("code_content", ""),
            task_data.get("code_directory", "")
        )
