"""
AIé©±åŠ¨çš„ç”¨æˆ·æ²Ÿé€šä»£ç† - å®Œå…¨åŸºäºAIæ¨¡å‹é©±åŠ¨
ä½¿ç”¨çœŸå®çš„AIæ¨¡å‹è¿›è¡Œè‡ªç„¶è¯­è¨€ç†è§£å’Œå¯¹è¯ç”Ÿæˆ
"""
import os
import re
import json
import datetime
import asyncio
from typing import Dict, Any, Optional, List, Tuple
from pathlib import Path
from .base_agent import BaseAgent, Message
from infrastructure.config.ai_agents import get_ai_agent_config
from utils import log, LogLevel

# å¯¼å…¥æŠ¥å‘Šç®¡ç†å™¨
try:
    from infrastructure.reports import report_manager
except ImportError:
    report_manager = None

try:
    from infrastructure.config.prompts import get_prompt
except ImportError:
    try:
        import sys
        import os
        sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
        from infrastructure.config.prompts import get_prompt
    except ImportError:
        # æœ€åçš„é™çº§æ–¹æ¡ˆ,å®šä¹‰ä¸€ä¸ªç®€å•çš„get_promptå‡½æ•°å’Œç³»ç»Ÿæç¤ºè¯
        def get_prompt(task_type, model_name=None, **kwargs):
            if task_type == "conversation":
                user_message = kwargs.get("user_message", "")
                return f"ç”¨æˆ·: {user_message}\nAIåŠ©æ‰‹:"
            return "AIåŠ©æ‰‹:"

class AIDrivenUserCommunicationAgent(BaseAgent):
    """AIé©±åŠ¨ç”¨æˆ·æ²Ÿé€šæ™ºèƒ½ä½“ - å®Œå…¨åŸºäºçœŸå®AIæ¨¡å‹
    
    æ ¸å¿ƒåŠŸèƒ½:
    1. çœŸæ­£çš„AIå¯¹è¯æ¨¡å‹ - ä½¿ç”¨transformersæ¨¡å‹è¿›è¡Œè‡ªç„¶è¯­è¨€ç†è§£
    2. AIé©±åŠ¨çš„æ„å›¾è¯†åˆ« - é€šè¿‡prompt engineeringå®ç°æ™ºèƒ½ç†è§£
    3. ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¯¹è¯ - ç»´æŠ¤ä¼šè¯çŠ¶æ€å’Œè®°å¿†
    4. æ™ºèƒ½ä»»åŠ¡åˆ†æ´¾ - AIå†³ç­–ä½•æ—¶å¯åŠ¨ä»£ç åˆ†æ
    """
    
    def __init__(self):
        super().__init__("user_comm_agent", "AI User Communication Agent")
        
        # AIæ¨¡å‹ç»„ä»¶
        self.conversation_model = None
        self.tokenizer = None
        self.used_device = "gpu"
        self.used_device_map = None  # æ·»åŠ è®¾å¤‡æ˜ å°„å‚æ•°
        
        # ä¼šè¯ç®¡ç†
        self.session_memory = {}
        self.agent_integration = None
        
        # ä»ç»Ÿä¸€é…ç½®è·å–
        self.agent_config = get_ai_agent_config().get_user_communication_agent_config()
        
        # æ¨¡å‹é…ç½® - ä»…ä½¿ç”¨éªŒè¯é€šè¿‡çš„æ¨¡å‹
        self.model_name = self.agent_config.get("model_name", "Qwen/Qwen1.5-7B-Chat")  # å…¼å®¹transformers 4.56.0
        
        # ç¡¬ä»¶è¦æ±‚ï¼šä»é…ç½®è¯»å–
        self.max_memory_mb = self.agent_config.get("max_memory_mb", 14336)
        
        # æ•°æ®åº“é…ç½®
        self._mock_requirement_id = 1000
        # è°ƒè¯•å¼€å…³ï¼šå…è®¸è·³è¿‡åç»­ä»£ç åˆ†ææ­¥éª¤ï¼Œä¾¿äºå¿«é€ŸéªŒè¯è·¯ç”±é€»è¾‘
        self.mock_code_analysis = os.getenv("MAS_MOCK_CODE_ANALYSIS", "0") == "1"
        
        # AIæ¨¡å‹çŠ¶æ€
        self.ai_enabled = False
        
        # åˆ†æç»“æœå­˜å‚¨
        self.analysis_results = {}
    
    def set_model(self, model_name: str):
        """åŠ¨æ€è®¾ç½®AIæ¨¡å‹"""
        if model_name != "Qwen/Qwen1.5-7B-Chat":
            log("user_comm_agent", LogLevel.WARNING, f"âš ï¸ ä»…æ”¯æŒ Qwen/Qwen1.5-7B-Chat æ¨¡å‹")
            return
        
        self.model_name = model_name
        log("user_comm_agent", LogLevel.INFO, f"ğŸ”„ å·²åˆ‡æ¢åˆ°æ¨¡å‹: {model_name}")
        
        # å¦‚æœAIå·²ç»åˆå§‹åŒ–ï¼Œéœ€è¦é‡æ–°åˆå§‹åŒ–
        if self.ai_enabled:
            log("user_comm_agent", LogLevel.INFO, "â™»ï¸ æ£€æµ‹åˆ°æ¨¡å‹å·²åˆå§‹åŒ–ï¼Œå°†é‡æ–°åŠ è½½...")
            self.ai_enabled = False
            self.conversation_model = None
            self.tokenizer = None
    
    def get_supported_models(self) -> list:
        """è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
        return ["Qwen/Qwen1.5-7B-Chat"]
    
    async def initialize(self, agent_integration=None):
        """åˆå§‹åŒ–AIæ¨¡å‹å’Œä»£ç†é›†æˆ"""
        self.agent_integration = agent_integration
        await self._initialize_ai_models()
        log("user_comm_agent", LogLevel.INFO, "âœ… AIç”¨æˆ·æ²Ÿé€šä»£ç†åˆå§‹åŒ–å®Œæˆ")
    
    async def initialize_ai_communication(self):
        """åˆå§‹åŒ–AIç”¨æˆ·äº¤æµèƒ½åŠ› - å‘åå…¼å®¹æ–¹æ³•"""
        try:
            log("user_comm_agent", LogLevel.INFO, "åˆå§‹åŒ–æ™ºèƒ½å¯¹è¯AI...")
            await self._initialize_ai_models()
            log("user_comm_agent", LogLevel.INFO, "æ™ºèƒ½å¯¹è¯AIåˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            log("user_comm_agent", LogLevel.ERROR, f"AIäº¤æµåˆå§‹åŒ–é”™è¯¯: {e}")
            return False

    async def _initialize_ai_models(self):
        """åˆå§‹åŒ–Qwen1.5-7Bæ¨¡å‹"""
        try:
            from transformers import pipeline, AutoTokenizer

            log("user_comm_agent", LogLevel.INFO, "ğŸ”§ å¼€å§‹åˆå§‹åŒ–AIå¯¹è¯æ¨¡å‹...")
            log("user_comm_agent", LogLevel.INFO, f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_name}")

            cache_dir = get_ai_agent_config().get_model_cache_dir()
            # ç¡®ä¿ç¼“å­˜ç›®å½•æ˜¯ç»å¯¹è·¯å¾„
            if not os.path.isabs(cache_dir):
                cache_dir = os.path.abspath(cache_dir)
            log("user_comm_agent", LogLevel.INFO, f"ğŸ’¾ ç¼“å­˜ç›®å½•: {cache_dir}")

            # ç¡®ä¿ç¼“å­˜ç›®å½•å­˜åœ¨
            os.makedirs(cache_dir, exist_ok=True)

            local_files_only = False
            # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
            model_path = os.path.join(cache_dir, f"models--{self.model_name.replace('/', '--')}")
            # æ£€æŸ¥å¿«ç…§ç›®å½•æ˜¯å¦å­˜åœ¨ä¸”ä¸ä¸ºç©º
            snapshots_path = os.path.join(model_path, "snapshots")
            model_files_exist = (
                os.path.exists(model_path) and 
                os.path.exists(snapshots_path) and 
                os.listdir(snapshots_path)
            )

            if model_files_exist:
                local_files_only = True
                log("user_comm_agent", LogLevel.INFO, "ğŸ” æ£€æµ‹åˆ°æœ¬åœ°ç¼“å­˜æ¨¡å‹æ–‡ä»¶ï¼Œå°†ä½¿ç”¨æœ¬åœ°æ–‡ä»¶åŠ è½½")
            else:
                log("user_comm_agent", LogLevel.INFO, "ğŸŒ æœªæ£€æµ‹åˆ°æœ¬åœ°ç¼“å­˜æ¨¡å‹ï¼Œå°†ä»ç½‘ç»œä¸‹è½½")

            # åˆå§‹åŒ–tokenizer
            log("user_comm_agent", LogLevel.INFO, "ğŸ”§ ä½¿ç”¨Qwené…ç½®åŠ è½½tokenizer...")
            if local_files_only and model_files_exist:
                # ä½¿ç”¨æœ¬åœ°è·¯å¾„åŠ è½½tokenizerï¼Œé¿å…ç½‘ç»œè¯·æ±‚
                snapshot_dirs = os.listdir(snapshots_path)
                if snapshot_dirs:
                    model_local_path = os.path.join(snapshots_path, snapshot_dirs[0])
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        model_local_path,
                        cache_dir=cache_dir,
                        trust_remote_code=True,
                        local_files_only=True
                    )
                else:
                    raise Exception("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹å¿«ç…§ç›®å½•")
            else:
                # åœ¨çº¿æ¨¡å¼æˆ–æœ¬åœ°æ–‡ä»¶ä¸å®Œæ•´æ—¶ä½¿ç”¨æ¨¡å‹åç§°
                self.tokenizer = AutoTokenizer.from_pretrained(
                    self.model_name, 
                    cache_dir=cache_dir,
                    trust_remote_code=True,
                    local_files_only=local_files_only
                )
            log("user_comm_agent", LogLevel.INFO, "âœ… TokenizeråŠ è½½æˆåŠŸ")

            # é…ç½®tokenizer
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                log("user_comm_agent", LogLevel.INFO, "âœ… Tokenizeré…ç½®æˆåŠŸ")

            # è®¾ç½®padding_side
            self.tokenizer.padding_side = "left"
            log("user_comm_agent", LogLevel.INFO, "ğŸ”§ å·²è®¾ç½®padding_side")

            # åˆå§‹åŒ–å¯¹è¯ç”Ÿæˆpipeline
            log("user_comm_agent", LogLevel.INFO, f"ğŸ’» ä½¿ç”¨è®¾å¤‡: {self.used_device}")

            log("user_comm_agent", LogLevel.INFO, "ğŸ”§ æ­£åœ¨åˆ›å»ºå¯¹è¯ç”Ÿæˆpipeline...")
            # ä½¿ç”¨æ›´æ˜ç¡®çš„æ–¹å¼æŒ‡å®šæ¨¡å‹è·¯å¾„ä»¥é¿å…ç½‘ç»œè¯·æ±‚
            if local_files_only and model_files_exist:
                # ç›´æ¥ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„è€Œä¸æ˜¯æ¨¡å‹æ ‡è¯†ç¬¦
                snapshot_dirs = os.listdir(snapshots_path)
                if snapshot_dirs:
                    model_local_path = os.path.join(snapshots_path, snapshot_dirs[0])
                    self.conversation_model = pipeline(
                        "text-generation",
                        model=model_local_path,  # ä½¿ç”¨æœ¬åœ°è·¯å¾„è€Œéæ¨¡å‹å
                        tokenizer=self.tokenizer,
                        device=self.used_device,
                        trust_remote_code=True,
                        model_kwargs={
                            "cache_dir": cache_dir,
                            "local_files_only": True,
                            "device_map": "auto" if self.used_device_map == "gpu" else None,
                        }
                    )
                else:
                    raise Exception("æœªæ‰¾åˆ°æœ‰æ•ˆçš„æ¨¡å‹å¿«ç…§ç›®å½•")
            else:
                # åœ¨çº¿æ¨¡å¼æˆ–æœ¬åœ°æ–‡ä»¶ä¸å®Œæ•´æ—¶ä½¿ç”¨æ¨¡å‹åç§°
                self.conversation_model = pipeline(
                    "text-generation",
                    model=self.model_name,
                    tokenizer=self.tokenizer,
                    device=self.used_device,
                    trust_remote_code=True,
                    model_kwargs={
                        "cache_dir": cache_dir,
                        "local_files_only": local_files_only,
                        "device_map": "auto" if self.used_device_map == "gpu" else None,
                    }
                )
            log("user_comm_agent", LogLevel.INFO, "âœ… Pipelineåˆ›å»ºæˆåŠŸ")

            # é¢„çƒ­æ¨¡å‹
            log("user_comm_agent", LogLevel.INFO, "ğŸ”¥ é¢„çƒ­AIæ¨¡å‹...")
            test_result = self.conversation_model("ä½ å¥½", max_new_tokens=10, do_sample=False)
            if test_result and len(test_result) > 0:
                log("user_comm_agent", LogLevel.INFO, "âœ… æ¨¡å‹é¢„çƒ­æˆåŠŸ")

            self.ai_enabled = True
            log("user_comm_agent", LogLevel.INFO, "ğŸ‰ AIå¯¹è¯æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        except ImportError:
            error_msg = "transformersåº“æœªå®‰è£…,AIåŠŸèƒ½æ— æ³•ä½¿ç”¨"
            log("user_comm_agent", LogLevel.ERROR, f"âŒ {error_msg}")
            raise ImportError(error_msg)
        except Exception as e:
            error_msg = f"AIæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}"
            log("user_comm_agent", LogLevel.ERROR, f"âŒ {error_msg}")
            raise Exception(error_msg)

    async def handle_message(self, message: Message):
        """å¤„ç†ç”¨æˆ·è¾“å…¥æ¶ˆæ¯"""
        try:
            if message.message_type == "user_input":
                await self._process_user_input(message.content)
            elif message.message_type == "system_feedback":
                await self._process_system_feedback(message.content)
            elif message.message_type == "analysis_result":
                await self._process_analysis_result(message.content)
            else:
                log("user_comm_agent", LogLevel.ERROR, f"âŒ ç³»ç»Ÿé”™è¯¯: æ”¶åˆ°æœªçŸ¥æ¶ˆæ¯ç±»å‹: {message.message_type}")
        except Exception as e:
            log("user_comm_agent", LogLevel.ERROR, f"âŒ ç³»ç»Ÿé”™è¯¯: æ¶ˆæ¯å¤„ç†å¼‚å¸¸ ({str(e)})")
            raise

    async def _process_user_input(self, content: Dict[str, Any]):
        """å¤„ç†ç”¨æˆ·è¾“å…¥ - AIé©±åŠ¨çš„å¯¹è¯å¼•æ“"""
        user_message = content.get("message", "")
        session_id = content.get("session_id", "default")
        target_directory = content.get("target_directory")
        wait_for_db = bool(content.get("wait_for_db"))

        log("user_comm_agent", LogLevel.INFO, "ğŸ“¦ å¤„ç†ç”¨æˆ·è¾“å…¥...")
        log("user_comm_agent", LogLevel.INFO, f"ğŸ” ç”¨æˆ·è¾“å…¥: {user_message}")
        log("user_comm_agent", LogLevel.INFO, f"ğŸ” ä¼šè¯ID: {session_id}")
        log("user_comm_agent", LogLevel.INFO, f"ğŸ” ç›®æ ‡ç›®å½•: {target_directory}")
        log("user_comm_agent", LogLevel.INFO, f"ğŸ” ç­‰å¾…æ•°æ®åº“: {wait_for_db}")
        
        # ä½¿ç”¨AIé©±åŠ¨çš„å¯¹è¯å¤„ç†
        if self.ai_enabled and self.conversation_model:
            try:
                log("user_comm_agent", LogLevel.INFO, "ğŸš€ å¼€å§‹AIå¯¹è¯å¤„ç†...")
                response, actions = await self.process_ai_conversation(
                    user_message, session_id, target_directory
                )
                
                if response:
                    log("user_comm_agent", LogLevel.INFO, f"âœ… AIå›åº”ç”ŸæˆæˆåŠŸ: {len(response)} å­—ç¬¦")
                    
                    await self._execute_ai_actions(
                        actions,
                        session_id,
                        wait_for_db=wait_for_db,
                        raw_user_message=user_message,
                    )
                    return
                
            except Exception as e:
                log("user_comm_agent", LogLevel.ERROR, f"âŒ AIå¤„ç†å¼‚å¸¸: {str(e)}")
                return
        
        # AIæ¨¡å‹æœªå¯ç”¨
        log("user_comm_agent", LogLevel.ERROR, "âŒ ç³»ç»Ÿé”™è¯¯: AIæ¨¡å‹æœªå¯ç”¨æˆ–åˆå§‹åŒ–å¤±è´¥")

    async def process_ai_conversation(self, user_message: str, session_id: str, target_directory: str = None):
        """AIé©±åŠ¨çš„å¯¹è¯å¤„ç†"""
        try:
            log("user_comm_agent", LogLevel.INFO, "ğŸš€ å¼€å§‹AIå¯¹è¯å¤„ç†æµç¨‹...")
            
            # 1. æ›´æ–°ä¼šè¯ä¸Šä¸‹æ–‡
            self._update_session_context(user_message, session_id, target_directory)
            # è‹¥å­˜åœ¨å¾…ç¡®è®¤çš„å±é™©æ“ä½œï¼Œä¼˜å…ˆè¿›è¡Œç¡®è®¤åˆ¤å®š
            pending_confirm = self._get_pending_db_confirm(session_id)
            if pending_confirm:
                confirmed = await self._classify_delete_all_confirm(user_message, pending_confirm)
                if confirmed:
                    forced_tasks = self._apply_confirm_to_tasks(pending_confirm.get("tasks", []))
                    self._clear_pending_db_confirm(session_id)
                    actions = {
                        "intent": "db",
                        "next_action": "handle_db_tasks",
                        "extracted_info": {
                            "code_analysis_tasks": [],
                            "db_tasks": [],
                            "explanation": "",
                        },
                        "forced_db_tasks": forced_tasks,
                        "confidence": 1.0,
                    }
                    return "å·²æ”¶åˆ°ç¡®è®¤ï¼Œæ­£åœ¨æ‰§è¡Œåˆ é™¤æ“ä½œã€‚", actions
                else:
                    actions = {
                        "intent": "conversation",
                        "next_action": "continue_conversation",
                        "extracted_info": {
                            "code_analysis_tasks": [],
                            "db_tasks": [],
                            "explanation": "è¯¥æ“ä½œéœ€è¦ç¡®è®¤ã€‚å¦‚éœ€ç»§ç»­ï¼Œè¯·æ˜ç¡®ç¡®è®¤åˆ é™¤å…¨éƒ¨æ•°æ®ã€‚",
                        },
                        "confidence": 1.0,
                    }
                    return "è¯¥æ“ä½œéœ€è¦ç¡®è®¤ã€‚å¦‚éœ€ç»§ç»­ï¼Œè¯·æ˜ç¡®ç¡®è®¤åˆ é™¤å…¨éƒ¨æ•°æ®ã€‚", actions
            # 2. å‡†å¤‡AIå¯¹è¯ä¸Šä¸‹æ–‡
            conversation_history = self._format_conversation_history(session_id)
            
            # 3. æ ¹æ®æ¨¡å‹èƒ½åŠ›æ„å»º system / user å†…å®¹æˆ–é€€å›æ—§ prompt
            use_qwen_chat = (
                self.tokenizer is not None
                and hasattr(self.tokenizer, "apply_chat_template")
                and isinstance(self.model_name, str)
                and self.model_name.startswith("Qwen/")
            )

            if use_qwen_chat:
                # Qwen chat è·¯å¾„ï¼šä½¿ç”¨ç³»ç»Ÿçº§æç¤ºè¯ + ç”¨æˆ·å†…å®¹
                system_prompt = get_prompt(
                        task_type="conversation",
                        model_name=self.model_name,
                        user_message=user_message,
                        conversation_history=conversation_history,
                    )
                user_content = f"ç”¨æˆ·è¯´: {user_message}\nä¼šè¯å†å²: {conversation_history}"
                raw_ai_response = await self._generate_ai_response(
                    system_prompt=system_prompt,
                    user_content=user_content,
                )
            else:
                # å…¼å®¹æ—§æ¨¡å‹ï¼šç»§ç»­é€šè¿‡ get_prompt æ„é€ å•ä¸€å­—ç¬¦ä¸² prompt
                try:
                    ai_prompt = get_prompt(
                        task_type="conversation",
                        model_name=self.model_name,
                        user_message=user_message,
                        conversation_history=conversation_history,
                    )
                except (ValueError, KeyError) as e:
                    log("user_comm_agent", LogLevel.WARNING, f"è·å–Promptå¤±è´¥,ä½¿ç”¨ç®€åŒ–æ ¼å¼: {e}")
                    ai_prompt = f"ç”¨æˆ·: {user_message}\nåŠ©æ‰‹:"

                raw_ai_response = await self._generate_ai_response(prompt=ai_prompt)
            
            if not raw_ai_response:
                log("user_comm_agent", LogLevel.ERROR, "âŒ AIå›åº”ç”Ÿæˆå¤±è´¥")
                raise Exception("AIå›åº”ç”Ÿæˆå¤±è´¥")

            # æ‰“å°åŸå§‹AIå“åº”ç”¨äºè°ƒè¯•
            log("user_comm_agent", LogLevel.INFO, f" Raw AI Response: {raw_ai_response}")

            # 5. ä»å›åº”ä¸­è§£æä»»åŠ¡è§„åˆ’ JSONï¼ˆå¦‚å­˜åœ¨ï¼‰
            ai_response, task_plan = self._parse_task_plan_from_response(raw_ai_response)
            log("user_comm_agent", LogLevel.INFO, f"âœ… AIå›åº”ç”ŸæˆæˆåŠŸ: {len(ai_response)} å­—ç¬¦ (åŸå§‹é•¿åº¦: {len(raw_ai_response)})")

            if not task_plan:
                log("user_comm_agent", LogLevel.ERROR, "âš ï¸ AIå›å¤è§£æå¤±è´¥")
            # else:
            #     # æ‰“å°è§£æåçš„ä»»åŠ¡è®¡åˆ’ç”¨äºè°ƒè¯•
            #     log("user_comm_agent", LogLevel.INFO, f" Parsed Task Plan: {task_plan}")

            # 6. æ›´æ–°ä¼šè¯è®°å¿†ï¼ˆä»…è®°å½•å¯¹ç”¨æˆ·å¯è§çš„å›ç­”éƒ¨åˆ†ï¼‰
            self._update_session_memory_simple(session_id, ai_response, user_message)
            
            code_tasks = task_plan.get("code_analysis_tasks", []) if task_plan else []
            db_tasks = task_plan.get("db_tasks", []) if task_plan else []
            explanation = task_plan.get("explanation", "") if task_plan else ""

            # 7. å†³ç­–åç»­åŠ¨ä½œï¼šè‹¥æ²¡æœ‰ç»“æ„åŒ–ä»»åŠ¡ï¼Œåˆ™å›é€€åˆ°ç®€å•æ„å›¾æ£€æµ‹
            intent = (task_plan.get("intent") if task_plan else "") or ""
            if intent == "code" or code_tasks:
                next_action = "start_analysis"
            elif intent == "db" or db_tasks:
                next_action = "handle_db_tasks"
            else:
                next_action = "continue_conversation"

            actions: Dict[str, Any] = {
                "intent": "conversation",
                "next_action": next_action,
                "extracted_info": {
                    "code_analysis_tasks": code_tasks,
                    "db_tasks": db_tasks,
                    "explanation": explanation,
                },
                "confidence": 1.0,
            }
            
            return ai_response, actions
            
        except Exception as e:
            log("user_comm_agent", LogLevel.ERROR, f"âŒ AIå¯¹è¯å¤„ç†å¤±è´¥: {e}")
            raise
    
    # === ä¼šè¯ç®¡ç†æ–¹æ³• ===
    
    def _update_session_context(self, message: str, session_id: str, target_directory: str = None):
        """æ›´æ–°ä¼šè¯ä¸Šä¸‹æ–‡"""
        if session_id not in self.session_memory:
            self.session_memory[session_id] = {
                "messages": [],
                "collected_info": {},
                "last_active": self._get_current_time()
            }
        
        session = self.session_memory[session_id]
        session["messages"].append({
            "content": message,
            "timestamp": self._get_current_time(),
            "type": "user"
        })
        session["last_active"] = self._get_current_time()
        
        if target_directory:
            session["target_directory"] = target_directory

    def _get_pending_db_confirm(self, session_id: str) -> Optional[Dict[str, Any]]:
        session = self.session_memory.get(session_id, {})
        return session.get("pending_db_confirm")

    def _set_pending_db_confirm(self, session_id: str, pending: Dict[str, Any]):
        if session_id not in self.session_memory:
            self.session_memory[session_id] = {
                "messages": [],
                "collected_info": {},
                "last_active": self._get_current_time()
            }
        self.session_memory[session_id]["pending_db_confirm"] = pending

    def _clear_pending_db_confirm(self, session_id: str):
        session = self.session_memory.get(session_id, {})
        if "pending_db_confirm" in session:
            session.pop("pending_db_confirm", None)

    def _apply_confirm_to_tasks(self, tasks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        confirmed: List[Dict[str, Any]] = []
        for task in tasks:
            if not isinstance(task, dict):
                continue
            action = str(task.get("action") or "")
            target = task.get("target") or task.get("table")
            if not target:
                continue
            data = task.get("data") if isinstance(task.get("data"), dict) else {}
            if action.lower() in ("delete", "delete_all", "delete-all", "deleteall"):
                action = "delete_all"
            data["confirm"] = True
            confirmed.append({"target": target, "action": action, "data": data})

        if confirmed:
            return confirmed

        return [
            {"target": "review_session", "action": "delete_all", "data": {"confirm": True}},
            {"target": "curated_issue", "action": "delete_all", "data": {"confirm": True}},
            {"target": "issue_pattern", "action": "delete_all", "data": {"confirm": True}},
        ]

    async def _classify_delete_all_confirm(self, user_message: str, pending: Dict[str, Any]) -> bool:
        """ä½¿ç”¨LLMåˆ¤æ–­ç”¨æˆ·æ˜¯å¦æ˜ç¡®ç¡®è®¤åˆ é™¤å…¨éƒ¨æ•°æ®ã€‚"""
        try:
            prompt = get_prompt(
                task_type="db_delete_confirm",
                model_name=self.model_name,
                user_message=user_message,
                pending_action=pending.get("pending_action", "delete_all"),
            )
            payload = {
                "pending_action": pending.get("pending_action", "delete_all"),
                "user_message": user_message,
            }
            try:
                user_content = json.dumps(payload, ensure_ascii=False)
            except Exception:
                user_content = str(payload)
            raw = await self._generate_ai_response(
                system_prompt=prompt,
                user_content=user_content,
            )
            log("user_comm_agent", LogLevel.INFO, f"âš ï¸ ç¡®è®¤æ„å›¾è¯†åˆ«åŸå§‹å“åº”: {raw}")
            if raw and (
                raw.strip().startswith("ä½ æ˜¯æ•°æ®åº“å®‰å…¨ç¡®è®¤åŠ©æ‰‹")
                or "è¾“å…¥æ˜¯ JSON" in raw
                or "è¾“å‡º JSON" in raw
            ):
                log("user_comm_agent", LogLevel.WARNING, "âš ï¸ ç¡®è®¤åˆ¤å®šç–‘ä¼¼å›æ˜¾æç¤ºè¯ï¼Œå¯ç”¨è§„åˆ™å›é€€")
                message = user_message.strip()
                if any(token in message for token in ["ä¸", "ä¸è¦", "å–æ¶ˆ", "å¦", "æ‹’ç»"]):
                    return False
                if any(
                    token in message
                    for token in ["ç¡®è®¤", "æ˜¯çš„", "åŒæ„", "å¯ä»¥", "ç»§ç»­", "æ‰§è¡Œ", "åˆ é™¤å…¨éƒ¨", "å…¨éƒ¨åˆ é™¤", "æ¸…ç©º", "æ²¡é—®é¢˜"]
                ):
                    return True
                return False
            confirm, _ = self._parse_confirm_response(raw)
            return confirm
        except Exception as e:
            log("user_comm_agent", LogLevel.WARNING, f"âš ï¸ ç¡®è®¤æ„å›¾è¯†åˆ«å¤±è´¥: {e}")
            return False

    def _parse_confirm_response(self, ai_response: str) -> Tuple[bool, str]:
        """è§£æç¡®è®¤åˆ¤å®šå“åº”ï¼Œè¿”å›(confirm, explanation)ã€‚"""
        try:
            cleaned = self._sanitize_json_like_text(ai_response)
            data = json.loads(cleaned)
            if isinstance(data, dict):
                confirm = bool(data.get("confirm"))
                explanation = str(data.get("explanation", ""))
                return confirm, explanation
        except Exception:
            pass
        return False, ""
    
    def _format_conversation_history(self, session_id: str) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²"""
        session = self.session_memory.get(session_id, {})
        messages = session.get("messages", [])
        
        if not messages:
            return "é¦–æ¬¡å¯¹è¯"
        
        # è·å–æœ€è¿‘çš„3-5æ¡æ¶ˆæ¯
        recent_messages = messages[-5:]
        formatted = []
        
        for msg in recent_messages:
            role = "ç”¨æˆ·" if msg.get("type") == "user" else "AI"
            content = msg.get("content", "")[:100]  # é™åˆ¶é•¿åº¦
            formatted.append(f"{role}: {content}")
        
        return "\n".join(formatted)
    
    def _update_session_memory_simple(self, session_id: str, ai_response: str, user_message: str):
        """ç®€åŒ–çš„ä¼šè¯è®°å¿†æ›´æ–°"""
        if session_id not in self.session_memory:
            self.session_memory[session_id] = {
                "messages": [],
                "last_active": self._get_current_time()
            }
        
        session = self.session_memory[session_id]
        
        # æ·»åŠ AIå›åº”åˆ°å¯¹è¯å†å²
        session["messages"].append({
            "content": ai_response,
            "timestamp": self._get_current_time(),
            "type": "ai"
        })
        
        session["last_active"] = self._get_current_time()
        
        # ä¿æŒå¯¹è¯å†å²åœ¨åˆç†èŒƒå›´å†…
        if len(session["messages"]) > 20:
            session["messages"] = session["messages"][-15:]

    def _parse_task_plan_from_response(self, ai_response: str) -> Tuple[str, Dict[str, Any]]:
        """
        ä» AI å›åº”ä¸­ç›´æ¥è§£æ code_analysis_tasks å’Œ db_tasks å…³é”®å­—çš„ JSON ä»»åŠ¡è§„åˆ’ã€‚

        è¿”å›:
            user_visible_response: ç»™ç”¨æˆ·å±•ç¤ºçš„æ–‡æœ¬
            task_plan: è§£æå‡ºçš„ dictï¼Œè§£æå¤±è´¥æ—¶ä¸ºç©º dict
        """
        # å°è¯•åœ¨æ•´ä¸ªå“åº”ä¸­æŸ¥æ‰¾ JSON å¯¹è±¡
        import re
        import json

        # æå–å¯èƒ½çš„ JSON ç‰‡æ®µï¼Œä¼˜å…ˆè§£æ fenced ```json```ï¼Œå…¶æ¬¡åšå¹³è¡¡æ‹¬å·æ‰«æ
        candidates: List[str] = []

        # å¹³è¡¡æ‹¬å·æ‰«æï¼Œæå–å¯¹è±¡æˆ–æ•°ç»„
        stack = []
        start_idx = None
        for idx, ch in enumerate(ai_response):
            if ch in "{[":
                if not stack:
                    start_idx = idx
                stack.append(ch)
            elif ch in "}]":
                if stack:
                    stack.pop()
                    if not stack and start_idx is not None:
                        candidates.append(ai_response[start_idx : idx + 1])
                        start_idx = None

        best_match = None
        best_plan: Dict[str, Any] = {}

        for snippet in candidates:
            try:
                cleaned = self._sanitize_json_like_text(snippet)
                plan = json.loads(cleaned)
                if isinstance(plan, dict) and (
                    "code_analysis_tasks" in plan or "db_tasks" in plan or "intent" in plan
                ):
                    best_match = snippet
                    best_plan = self._normalize_task_plan(plan)
                    break
            except Exception:
                continue

        # å…œåº•ï¼šå°è¯•ä»é¦–ä¸ª { åˆ° æœ€åä¸€ä¸ª } çš„å­ä¸²è§£æ
        if not best_plan:
            try:
                start = ai_response.find("{")
                end = ai_response.rfind("}")
                if start != -1 and end != -1 and end > start:
                    fallback = ai_response[start : end + 1]
                    cleaned = self._sanitize_json_like_text(fallback)
                    plan = json.loads(cleaned)
                    if isinstance(plan, dict) and (
                        "code_analysis_tasks" in plan or "db_tasks" in plan or "intent" in plan
                    ):
                        best_match = fallback
                        best_plan = self._normalize_task_plan(plan)
            except Exception:
                pass

        if not best_match:
            # æœªæ‰¾åˆ°ç»“æ„åŒ–ä»»åŠ¡è§„åˆ’ï¼Œç›´æ¥è¿”å›åŸå§‹æ–‡æœ¬
            # log("user_comm_agent", LogLevel.WARNING, f"âš ï¸ æœªè§£æå‡ºç»“æ„åŒ–ä»»åŠ¡è§„åˆ’JSON, å“åº”å†…å®¹: {ai_response[:200]}...")
            log("user_comm_agent", LogLevel.WARNING, "âš ï¸ æœªè§£æå‡ºç»“æ„åŒ–ä»»åŠ¡è§„åˆ’JSON")
            return ai_response.strip(), {}

        explanation = best_plan.get("explanation", "")
        if explanation and explanation.strip():
            user_visible = explanation.strip()
        else:
            user_visible = ai_response.replace(best_match, "").strip()

        # best_plan å·²åœ¨ _normalize_task_plan ä¸­åšè¿‡ç™½åå•/åˆ—è¡¨åŒ–
        best_plan["explanation"] = explanation

        if not user_visible:
            user_visible = ai_response.strip()

        return user_visible, best_plan

    def _sanitize_json_like_text(self, text: str) -> str:
        """
        å¯¹â€œçœ‹èµ·æ¥åƒ JSON çš„æ–‡æœ¬â€åšæœ€å°ä¿®å¤ï¼š
        - å»é™¤ ```json / ``` å›´æ 
        - ç§»é™¤ // è¡Œæ³¨é‡Š ä¸ /* */ å—æ³¨é‡Š
        - ç§»é™¤å°¾é€—å·ï¼ˆ,} / ,]ï¼‰
        """
        s = text.strip()
        # å»å›´æ 
        s = re.sub(r"^```(?:json)?\\s*", "", s, flags=re.IGNORECASE)
        s = re.sub(r"\\s*```$", "", s)
        # å»æ³¨é‡Šï¼ˆåªé’ˆå¯¹ JSON ç‰‡æ®µåšï¼Œé¿å…å½±å“è‡ªç„¶è¯­è¨€å¤§æ®µï¼‰
        s = re.sub(r"//.*?$", "", s, flags=re.MULTILINE)
        s = re.sub(r"/\\*.*?\\*/", "", s, flags=re.DOTALL)
        # å»å°¾é€—å·
        s = re.sub(r",\\s*([}\\]])", r"\\1", s)
        return s.strip()

    def _normalize_task_plan(self, plan: Dict[str, Any]) -> Dict[str, Any]:
        """
        å°†æ¨¡å‹è¾“å‡ºå½’ä¸€åŒ–ä¸ºè§£æ/è·¯ç”±ç¨³å®šçš„ç™½åå•ç»“æ„ï¼š
        - é¡¶å±‚ä»…ä¿ç•™ intent/code_analysis_tasks/explanation
        - code_analysis_tasks å…ƒç´ ä»…ä¿ç•™ target_path
        """
        normalized: Dict[str, Any] = {
            "intent": plan.get("intent", "") if isinstance(plan.get("intent"), str) else "",
            "code_analysis_tasks": [],
            "explanation": plan.get("explanation", "") if isinstance(plan.get("explanation", ""), str) else "",
        }

        code_tasks = plan.get("code_analysis_tasks") or []
        if not isinstance(code_tasks, list):
            code_tasks = [code_tasks]
        for item in code_tasks:
            if isinstance(item, dict) and item.get("target_path"):
                normalized["code_analysis_tasks"].append({"target_path": item.get("target_path")})

        # æ„å›¾ä¼˜å…ˆï¼šå·²è¯†åˆ«ä¸º db æ—¶ä¸å…è®¸ code_analysis_tasksï¼›è¯†åˆ«ä¸º code æ—¶è¦æ±‚ä»£ç è·¯å¾„
        intent = normalized.get("intent")
        if intent == "db":
            normalized["code_analysis_tasks"] = []
        elif intent == "code":
            if not normalized["code_analysis_tasks"]:
                normalized["code_analysis_tasks"] = []

        return normalized

    async def _execute_ai_actions(
        self,
        actions: Dict[str, Any],
        session_id: str,
        wait_for_db: bool = False,
        raw_user_message: str = "",
    ):
        """æ‰§è¡ŒAIå»ºè®®çš„æ“ä½œ"""
        next_action = actions.get("next_action")
        extracted_info = actions.get("extracted_info", {})
        code_tasks = extracted_info.get("code_analysis_tasks") or []
        db_tasks = extracted_info.get("db_tasks") or []
        explanation = extracted_info.get("explanation", "")
        forced_db_tasks = actions.get("forced_db_tasks")
        # log("user_comm_agent", LogLevel.INFO,
        #     f"æ‰§è¡ŒAIåŠ¨ä½œ next_action={next_action} code_tasks={len(code_tasks)} db_tasks={len(db_tasks)} mock_code_analysis={self.mock_code_analysis} session_id={session_id}")

        log("user_comm_agent", LogLevel.INFO,  f"ğŸ“¦ next_action={next_action}")

        if next_action == "start_analysis":
            extracted_info = actions.get("extracted_info", {})
            if self.mock_code_analysis:
                log("user_comm_agent", LogLevel.INFO, "ğŸ§ª [MockAnalysis] æ‹¦æˆªä»£ç åˆ†æä»»åŠ¡ï¼Œä»¥ä¸‹ä¿¡æ¯ä»…æ—¥å¿—å±•ç¤ºï¼š")
                try:
                    pretty = json.dumps(code_tasks, ensure_ascii=False, indent=2)
                except TypeError:
                    pretty = str(code_tasks)
                log("user_comm_agent", LogLevel.INFO, pretty if pretty else "(æ—  code_tasks)")
            else:
                if explanation:
                    log("user_comm_agent", LogLevel.INFO, f"âš ï¸ {explanation}")
                await self._start_code_analysis(extracted_info, session_id)
        elif next_action == "handle_db_tasks":
            if explanation:
                log("user_comm_agent", LogLevel.INFO, f"âš ï¸ {explanation}")
            await self._dispatch_db_tasks(
                db_tasks,
                session_id,
                wait_for_db=wait_for_db,
                raw_user_message=raw_user_message,
                forced_tasks=forced_db_tasks,
            )
            pass
        elif next_action == "continue_conversation":
            # ç»§ç»­ä¿¡æ¯æ”¶é›† - æä¾›æ˜ç¡®çš„ç”¨æˆ·æŒ‡å¯¼
            if explanation:
                log("user_comm_agent", LogLevel.INFO, f"âš ï¸ {explanation}")
            else:
                log("user_comm_agent", LogLevel.INFO, "âš ï¸ ä¸ºäº†æ›´å¥½åœ°å¸®åŠ©æ‚¨ï¼Œæˆ‘éœ€è¦æ›´å¤šä¿¡æ¯ã€‚è¯·æä¾›æ›´å¤šå…³äºæ‚¨æƒ³è¦æ‰§è¡Œçš„ä»»åŠ¡çš„è¯¦ç»†ä¿¡æ¯ã€‚")
        else:
            # ç»§ç»­å¯¹è¯
            pass
    
    async def _dispatch_db_tasks(
        self,
        db_tasks: List[Dict[str, Any]],
        session_id: str,
        wait_for_db: bool = False,
        raw_user_message: str = "",
        forced_tasks: Optional[List[Dict[str, Any]]] = None,
    ):
        """
        å°†è§£æå‡ºçš„ db_tasks åˆ†å‘ç»™ DB Agentã€‚
        
        çº¦æŸï¼š
        - å¿…é¡»é€šè¿‡ AgentIntegration(å•ä¾‹) æŒæœ‰çš„ data_manage å®ä¾‹è½¬å‘ï¼Œç¡®ä¿ä½¿ç”¨ç³»ç»Ÿåˆå§‹åŒ–æ—¶åˆ›å»ºçš„åŒä¸€å¯¹è±¡ã€‚
        - ä¸åœ¨æœ¬ Agent å†…éƒ¨ new ä»»ä½• DB Agentã€‚
        """
        # é›¶ç¿»è¯‘è·¯ç”±ï¼šä¿æŒç”¨æˆ·åŸå§‹è¯­ä¹‰ï¼Œä¸å¯¹ description è¿›è¡Œæ”¹å†™
        # é›¶ç¿»è¯‘è·¯ç”±ï¼šä»…ä¼  raw_text
        normalized_db_tasks: List[Dict[str, Any]] = []
        
        try:
            if not self.agent_integration or not hasattr(self.agent_integration, "agents"):
                log("user_comm_agent", LogLevel.WARNING, "âš ï¸ AgentIntegration æœªå°±ç»ªï¼Œæ— æ³•è½¬å‘ db_tasks")
                return

            data_manage_agent = self.agent_integration.agents.get("data_manage")
            if not data_manage_agent:
                log("user_comm_agent", LogLevel.WARNING, "âš ï¸ æœªæ‰¾åˆ° data_manage agentï¼Œæ— æ³•å¤„ç† db_tasks")
                return

            if wait_for_db and hasattr(data_manage_agent, "user_requirement_interpret"):
                log(
                    "user_comm_agent",
                    LogLevel.INFO,
                    f"â³ åŒæ­¥æ‰§è¡Œ db_tasks â†’ {data_manage_agent.agent_id} (count=0)",
                )
                result = await data_manage_agent.user_requirement_interpret(
                    user_requirement={
                        "db_tasks": [],
                        "raw_text": raw_user_message,
                        "forced_tasks": forced_tasks or [],
                    },
                    session_id=session_id,
                )
                log(
                    "user_comm_agent",
                    LogLevel.INFO,
                    f"âœ… DB ä»»åŠ¡æ‰§è¡Œå®Œæˆ status={result.get('status')}",
                )
                if result.get("status") == "need_confirm":
                    pending_tasks = result.get("pending_tasks") if isinstance(result.get("pending_tasks"), list) else []
                    if pending_tasks:
                        self._set_pending_db_confirm(
                            session_id,
                            {
                                "pending_action": result.get("pending_action", "delete_all"),
                                "tasks": pending_tasks,
                            },
                        )
                        log(
                            "user_comm_agent",
                            LogLevel.WARNING,
                            "âš ï¸ åˆ é™¤å…¨éƒ¨æ•°æ®éœ€è¦ç¡®è®¤ï¼Œè¯·å›å¤ç¡®è®¤åç»§ç»­æ‰§è¡Œã€‚",
                        )
            else:
                log(
                    "user_comm_agent",
                    LogLevel.INFO,
                    f"ğŸ“¨ è½¬å‘ db_tasks â†’ {data_manage_agent.agent_id} (count=0)",
                )
                await self.dispatch_message(
                    receiver=data_manage_agent.agent_id,
                    content={
                        "requirement": {
                            "db_tasks": [],
                            "raw_text": raw_user_message,
                            "forced_tasks": forced_tasks or [],
                        },
                        "session_id": session_id,
                    },
                    message_type="user_requirement",
                )
        except Exception as e:
            log("user_comm_agent", LogLevel.ERROR, f"âŒ å¤„ç†æ•°æ®åº“ä»»åŠ¡å¤±è´¥: {e}")
            log("user_comm_agent", LogLevel.INFO, f"âš ï¸ æ•°æ®åº“ç›¸å…³æ“ä½œæš‚æ—¶ä¸å¯ç”¨: {e}")
    
    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        return datetime.datetime.now().isoformat()
    
    # === AIæ ¸å¿ƒæ–¹æ³• ===
    
    async def _generate_ai_response(
        self,
        prompt: str = None,
        system_prompt: Optional[str] = None,
        user_content: Optional[str] = None,
    ) -> str:
        """ä½¿ç”¨Qwen1.5-7Bæ¨¡å‹ç”Ÿæˆå›åº”(æ”¹è¿›: ä½¿ç”¨chatæ¨¡æ¿å’Œä¼šè¯ç»“æ„)
        
        å‚æ•°:
            prompt: å…¼å®¹æ—§pipelineçš„å•å­—ç¬¦ä¸²prompt
            system_prompt: chatæ¨¡å‹ä½¿ç”¨çš„ç³»ç»Ÿè§’è‰²ä¸åè®®è¯´æ˜
            user_content: chatæ¨¡å‹ä½¿ç”¨çš„ç”¨æˆ·è¾“å…¥ä¸ä¼šè¯å†…å®¹
        """
        try:
            if not self.ai_enabled or not self.conversation_model:
                raise Exception("AIæ¨¡å‹æœªåˆå§‹åŒ–")
            
            # å¦‚æœæ”¯æŒchatæ¨¡æ¿å¹¶ä¸”æ˜¯Qwenæ¨¡å‹,ä¼˜å…ˆèµ°chatå½¢å¼
            if (
                self.tokenizer
                and hasattr(self.tokenizer, "apply_chat_template")
                and isinstance(self.model_name, str)
                and self.model_name.startswith("Qwen/")
                and system_prompt
                and user_content
            ):
                messages = [
                    {
                        "role": "system",
                        "content": system_prompt,
                    },
                    {
                        "role": "user",
                        "content": user_content,
                    },
                ]
                input_ids = self.tokenizer.apply_chat_template(
                    messages,
                    add_generation_prompt=True,
                    return_tensors="pt",
                )
                if self.used_device == "gpu":
                    input_ids = input_ids.to("cuda")
                outputs = self.conversation_model.model.generate(
                    input_ids,
                    max_new_tokens=300,
                    temperature=0.85,
                    top_p=0.9,
                    do_sample=True,
                    repetition_penalty=1.05,
                    pad_token_id=self.tokenizer.eos_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                )
                generated_text = self.tokenizer.decode(
                    outputs[0][input_ids.shape[1] :], skip_special_tokens=True
                )
                ai_response = generated_text.strip()
                # ç®€å•å»é™¤é‡å¤å¼€åœº
                repeats = [
                    "æˆ‘æ˜¯MASä»£ç åˆ†æåŠ©æ‰‹",
                    "æˆ‘å¯ä»¥å¸®æ‚¨",
                    "æ‚¨å¥½ï¼æˆ‘æ˜¯MASä»£ç åˆ†æåŠ©æ‰‹",
                ]
                for r in repeats:
                    if ai_response.startswith(r):
                        ai_response = ai_response[len(r) :].lstrip(": ï¼š,ï¼Œ")
                if len(ai_response) < 5:
                    # é€€å›æ—§pipelineæ–¹å¼
                    fallback_prompt = (
                        prompt
                        if prompt
                        else f"{system_prompt}\n\n{user_content}".strip()
                    )
                    result = self.conversation_model(
                        fallback_prompt,
                        max_new_tokens=200,
                        temperature=0.9,
                        do_sample=True,
                    )
                    ai_response = self._clean_ai_response(
                        result[0]["generated_text"], fallback_prompt
                    )
                return ai_response
            
            # å›é€€: ä½¿ç”¨åŸpipeline
            if not prompt:
                # å¦‚æœæœªæä¾›prompt,ä½†æœ‰system+user,åˆ™æ‹¼æ¥æˆæ–‡æœ¬prompt
                if system_prompt or user_content:
                    prompt_parts = []
                    if system_prompt:
                        prompt_parts.append(system_prompt)
                    if user_content:
                        prompt_parts.append(user_content)
                    prompt = "\n\n".join(prompt_parts)
                else:
                    raise Exception("ç¼ºå°‘æœ‰æ•ˆçš„ç”Ÿæˆpromptå‚æ•°")

            result = self.conversation_model(
                prompt,
                max_new_tokens=150,
                temperature=0.85,
                do_sample=True,
                repetition_penalty=1.1,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id
            )
            if result and len(result) > 0:
                raw_text = result[0]["generated_text"]
                ai_response = self._clean_ai_response(raw_text, prompt)
                if not ai_response or len(ai_response.strip()) < 5:
                    ai_response = raw_text[-300:].strip()
                return ai_response
            raise Exception("æ¨¡å‹è¿”å›ç©ºç»“æœ")
        except Exception as e:
            log("user_comm_agent", LogLevel.ERROR, f"âŒ AIç”Ÿæˆå¤±è´¥: {e}")
            raise
    
    def _clean_ai_response(self, raw_text: str, prompt: str) -> str:
        """æ¸…ç†AIç”Ÿæˆçš„å›åº”"""
        # ç§»é™¤promptéƒ¨åˆ†ï¼Œåªä¿ç•™æ–°ç”Ÿæˆçš„å†…å®¹
        if prompt in raw_text:
            ai_response = raw_text.replace(prompt, "").strip()
        else:
            ai_response = raw_text.strip()
        
        # åªæ¸…ç†æ˜æ˜¾çš„å‰ç¼€ï¼Œä¿ç•™å®é™…å†…å®¹
        cleanup_patterns = [
            r'^åŠ©æ‰‹:\s*',
            r'^AIåŠ©æ‰‹:\s*',
            r'^å›ç­”:\s*',
        ]
        
        for pattern in cleanup_patterns:
            ai_response = re.sub(pattern, '', ai_response, flags=re.IGNORECASE)
        
        ai_response = ai_response.strip()
        
        # å¦‚æœç»“æœä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œè¿”å›åŸå§‹æ–‡æœ¬ï¼ˆå»æ‰promptï¼‰
        if len(ai_response) < 5:
            # å°è¯•ä»åŸå§‹æ–‡æœ¬ä¸­æå–æœ‰ç”¨å†…å®¹
            lines = raw_text.strip().split('\n')
            for line in lines:
                if line.strip() and not line.strip().startswith('ç”¨æˆ·:') and len(line.strip()) > 5:
                    return line.strip()
            # å¦‚æœæ‰¾ä¸åˆ°åˆé€‚çš„å†…å®¹ï¼Œè¿”å›ä¸€ä¸ªé»˜è®¤å›åº”
            return "æˆ‘æ˜ç™½äº†ï¼Œæœ‰ä»€ä¹ˆå¯ä»¥å¸®åŠ©æ‚¨çš„å—ï¼Ÿ"
        
        return ai_response
    
    # === å…¶ä»–å¿…è¦æ–¹æ³•çš„ç®€åŒ–å®ç° ===
    
    async def _process_system_feedback(self, content: Dict[str, Any]):
        """å¤„ç†ç³»ç»Ÿåé¦ˆ"""
        feedback_type = content.get("type", "unknown")
        feedback_message = content.get("message", "")
        log("user_comm_agent", LogLevel.INFO, f"ğŸ“Š ç³»ç»Ÿåé¦ˆ: {feedback_message}")
    
    async def _process_analysis_result(self, content: Dict[str, Any]):
        """å¤„ç†åˆ†æç»“æœ"""
        agent_type = content.get("agent_type")
        requirement_id = content.get("requirement_id")
        log("user_comm_agent", LogLevel.INFO, f"ğŸ“Š æ”¶åˆ° {agent_type} åˆ†æç»“æœ (ä»»åŠ¡ID: {requirement_id})")
    
    async def _start_code_analysis(self, extracted_info: Dict[str, Any], session_id: str):
        """å¯åŠ¨ä»£ç åˆ†æ"""
        # ä»ä¼šè¯ä¸­è·å–ç›®å½•è·¯å¾„
        session = self.session_memory.get(session_id, {})
        target_directory = session.get("target_directory")
        
        # å°è¯•ä»ç”¨æˆ·æ¶ˆæ¯ä¸­æå–è·¯å¾„
        if not target_directory:
            messages = session.get("messages", [])
            for msg in reversed(messages):
                if msg.get("type") == "user":
                    content = msg.get("content", "")
                    # æŸ¥æ‰¾è·¯å¾„æ¨¡å¼
                    import re
                    path_patterns = [
                        r'/[a-zA-Z0-9/_.-]+',  # Unixè·¯å¾„
                        r'[A-Z]:\\[a-zA-Z0-9\\._-]+',  # Windowsè·¯å¾„
                    ]
                    for pattern in path_patterns:
                        matches = re.findall(pattern, content)
                        if matches:
                            target_directory = matches[0]
                            break
                    if target_directory:
                        break
        
        if target_directory:
            log("user_comm_agent", LogLevel.INFO, f"ğŸš€ å¯åŠ¨ä»£ç åˆ†æï¼Œç›®æ ‡ç›®å½•: {target_directory}")
            
            # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
            import os
            if os.path.exists(target_directory):
                try:
                    # å¯åŠ¨MASåˆ†ææµç¨‹
                    if self.agent_integration:
                        log("user_comm_agent", LogLevel.INFO, "ğŸ“Š è°ƒç”¨å¤šæ™ºèƒ½ä½“åˆ†æç³»ç»Ÿ...")
                        await self._trigger_mas_analysis(target_directory, session_id)
                    else:
                        log("user_comm_agent", LogLevel.INFO, "ğŸ“Š å¼€å§‹åˆ†æä»£ç ç›®å½•ç»“æ„...")
                        await self._analyze_directory_structure(target_directory, session_id)
                except Exception as e:
                    log("user_comm_agent", LogLevel.ERROR, f"âŒ ä»£ç åˆ†æå¯åŠ¨å¤±è´¥: {e}")
            else:
                log("user_comm_agent", LogLevel.ERROR, f"âŒ ç›®å½•ä¸å­˜åœ¨: {target_directory}")
        else:
            log("user_comm_agent", LogLevel.ERROR, "âŒ æ— æ³•æ‰¾åˆ°æœ‰æ•ˆçš„ä»£ç ç›®å½•è·¯å¾„")
    
    async def _trigger_mas_analysis(self, target_directory: str, session_id: str):
        """
        è§¦å‘MASå¤šæ™ºèƒ½ä½“åˆ†æ(å¢å¼º: è°ƒç”¨é›†æˆå™¨analyze_directory å¹¶ç­‰å¾…ç»“æœç”Ÿæˆ)
        å¢åŠ ç­‰å¾…è¶…æ—¶æœºåˆ¶: é»˜è®¤1å°æ—¶, æ¯10ç§’åˆ·æ–°ä¸€æ¬¡è¿›åº¦, ç›´åˆ°ç”Ÿæˆ run_summary æˆ–è¶…æ—¶ã€‚
        """
        try:
            if self.agent_integration and hasattr(self.agent_integration, 'analyze_directory'):
                result = await self.agent_integration.analyze_directory(target_directory)
                status = result.get('status')
                if status == 'dispatched':
                    path = result.get('report_path')
                    run_id = result.get('run_id')
                    total_files = result.get('total_files')
                    log("user_comm_agent", LogLevel.INFO, f"âœ… åˆ†æä»»åŠ¡å·²æ´¾å‘ï¼Œå…± {total_files} ä¸ªæ–‡ä»¶ï¼ŒdispatchæŠ¥å‘Š: {path}")
                    # å¯åŠ¨ç­‰å¾…æµç¨‹
                    await self._wait_for_run_completion(run_id, total_files)
                elif status == 'empty':
                    log("user_comm_agent", LogLevel.WARNING, "âš ï¸ ç›®å½•ä¸­æœªæ‰¾åˆ°å¯åˆ†æçš„Pythonæ–‡ä»¶ï¼Œåˆ†ææœªæ‰§è¡Œ")
                else:
                    log("user_comm_agent", LogLevel.ERROR, f"âŒ åˆ†æå¤±è´¥: {result.get('message','æœªçŸ¥é”™è¯¯')}")
            else:
                log("user_comm_agent", LogLevel.ERROR, "âŒ é›†æˆå™¨ä¸å¯ç”¨ï¼Œæ— æ³•æ‰§è¡Œå¤šæ™ºèƒ½ä½“åˆ†æ")
        except Exception as e:
            log("user_comm_agent", LogLevel.ERROR, f"âŒ MASåˆ†æå¯åŠ¨å¼‚å¸¸: {e}")

    async def _wait_for_run_completion(self, run_id: str, total_files: int, timeout: int = None, poll_interval: int = None):
        """ç­‰å¾…MASè¿è¡Œå®Œæˆï¼Œé¿å…é¢‘ç¹æ‰“å°è¿›åº¦ã€‚"""
        if timeout is None:
            timeout = self.agent_config.get("analysis_wait_timeout", 1200)
        if poll_interval is None:
            poll_interval = self.agent_config.get("analysis_poll_interval", 60)
        
        analysis_dir = Path(__file__).parent.parent.parent / 'reports' / 'analysis'
        run_dir = analysis_dir / run_id
        consolidated_dir = run_dir / 'consolidated'
        start_time = asyncio.get_event_loop().time()
        
        log("user_comm_agent", LogLevel.INFO,
            f"WaitLoop start run_id={run_id} timeout={timeout}s poll_interval={poll_interval}s total_files={total_files}"
        )
        
        while True:
            elapsed = int(asyncio.get_event_loop().time() - start_time)
            if elapsed >= timeout:
                log("user_comm_agent", LogLevel.ERROR, f"â±ï¸ åˆ†æè¶…æ—¶ ({timeout} ç§’)")
                return
            
            if not run_dir.exists():
                await asyncio.sleep(poll_interval)
                continue
            
            consolidated_files = []
            if consolidated_dir.exists():
                consolidated_files = list(consolidated_dir.glob("consolidated_*.json"))
            
            severity_agg = {"critical": 0, "high": 0, "medium": 0, "low": 0, "info": 0}
            total_issues = 0
            for report_path in consolidated_files:
                try:
                    data = json.loads(report_path.read_text(encoding='utf-8'))
                except Exception as exc:
                    log("user_comm_agent", LogLevel.WARNING, f"è¯»å–æŠ¥å‘Šå¤±è´¥ {report_path.name}: {exc}")
                    continue
                
                for level, count in data.get('severity_stats', {}).items():
                    if level in severity_agg:
                        severity_agg[level] += count
                total_issues += data.get('issue_count', 0)
            
            summary_candidates = list(run_dir.glob("run_summary*.json"))
            summary_file = summary_candidates[0] if summary_candidates else None
            
            if summary_file:
                try:
                    summary_data = json.loads(summary_file.read_text(encoding='utf-8'))
                except Exception:
                    summary_data = {}
                
                log("user_comm_agent", LogLevel.INFO, "âœ… MAS åˆ†æå®Œæˆã€‚")
                log("user_comm_agent", LogLevel.INFO, f"è¿è¡Œçº§æ±‡æ€»æŠ¥å‘Š: {summary_file}")
                stats = summary_data.get('severity_stats') or severity_agg
                if stats:
                    log("user_comm_agent", LogLevel.INFO, f"æ€»ä½“é—®é¢˜ç»Ÿè®¡: {stats}")
                return
            
            if total_files and len(consolidated_files) >= total_files:
                log("user_comm_agent", LogLevel.INFO, "âœ… MAS åˆ†æå®Œæˆã€‚")
                return
            
            await asyncio.sleep(poll_interval)
    
    async def _execute_task_impl(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç”¨æˆ·æ²Ÿé€šä»»åŠ¡"""
        return {"status": "user_communication_ready", "timestamp": self._get_current_time()}
    
    def generate_conversation_report(self, session_data: Dict[str, Any]) -> Optional[str]:
        """ç”Ÿæˆå¯¹è¯ä¼šè¯æŠ¥å‘Š"""
        if not report_manager:
            return None
        
        try:
            report_data = {
                "session_id": session_data.get("session_id", "unknown"),
                "start_time": session_data.get("start_time"),
                "end_time": datetime.datetime.now().isoformat(),
                "total_messages": len(session_data.get("messages", [])),
                "user_requests": session_data.get("user_requests", []),
                "ai_responses": session_data.get("ai_responses", []),
                "analysis_triggered": session_data.get("analysis_triggered", False),
                "code_paths_analyzed": session_data.get("code_paths", [])
            }
            
            report_path = report_manager.generate_analysis_report(
                report_data, 
                f"conversation_session_{session_data.get('session_id', 'unknown')}.json"
            )
            return str(report_path)
            
        except Exception as e:
            log("user_comm_agent", LogLevel.ERROR, f"ç”Ÿæˆå¯¹è¯æŠ¥å‘Šæ—¶å‡ºç°é”™è¯¯: {e}")
            return None