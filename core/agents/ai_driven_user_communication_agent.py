"""
AIé©±åŠ¨çš„ç”¨æˆ·æ²Ÿé€šä»£ç† - å®Œå…¨åŸºäºAIæ¨¡å‹é©±åŠ¨
ä½¿ç”¨çœŸå®çš„AIæ¨¡å‹è¿›è¡Œè‡ªç„¶è¯­è¨€ç†è§£å’Œå¯¹è¯ç”Ÿæˆ
"""
import os
import re
import json
import logging
import datetime
import asyncio
from typing import Dict, Any, Optional, List, Tuple
from .base_agent import BaseAgent, Message

try:
    from infrastructure.config.prompts import get_prompt
except ImportError:
    try:
        import sys
        import os
        sys.path.append(os.path.join(os.path.dirname(__file__), '../../'))
        from infrastructure.config.prompts import get_prompt
    except ImportError:
        # æœ€åçš„é™çº§æ–¹æ¡ˆ,å®šä¹‰ä¸€ä¸ªç®€å•çš„get_promptå‡½æ•°
        def get_prompt(task_type, model_name=None, **kwargs):
            if task_type == "conversation":
                user_message = kwargs.get("user_message", "")
                return f"ç”¨æˆ·: {user_message}\nAIåŠ©æ‰‹:"
            return "AIåŠ©æ‰‹:"

# è®¾ç½®ç”¨æˆ·æ²Ÿé€šæ™ºèƒ½ä½“çš„æ—¥å¿—ä¸ºè­¦å‘Šçº§åˆ«,å‡å°‘éå¿…è¦è¾“å‡º
logger = logging.getLogger(__name__)
logger.setLevel(logging.WARNING)

class AIDrivenUserCommunicationAgent(BaseAgent):
    """AIé©±åŠ¨ç”¨æˆ·æ²Ÿé€šæ™ºèƒ½ä½“ - å®Œå…¨åŸºäºçœŸå®AIæ¨¡å‹
    
    æ ¸å¿ƒåŠŸèƒ½:
    1. çœŸæ­£çš„AIå¯¹è¯æ¨¡å‹ - ä½¿ç”¨transformersæ¨¡å‹è¿›è¡Œè‡ªç„¶è¯­è¨€ç†è§£
    2. AIé©±åŠ¨çš„æ„å›¾è¯†åˆ« - é€šè¿‡prompt engineeringå®ç°æ™ºèƒ½ç†è§£
    3. ä¸Šä¸‹æ–‡æ„ŸçŸ¥å¯¹è¯ - ç»´æŠ¤ä¼šè¯çŠ¶æ€å’Œè®°å¿†
    4. æ™ºèƒ½ä»»åŠ¡åˆ†æ´¾ - AIå†³ç­–ä½•æ—¶å¯åŠ¨ä»£ç åˆ†æ
    """
    
    def __init__(self):
        super().__init__("user_comm_agent", "AIé©±åŠ¨ç”¨æˆ·æ²Ÿé€šæ™ºèƒ½ä½“")
        
        # AIæ¨¡å‹ç»„ä»¶
        self.conversation_model = None
        self.tokenizer = None
        
        # ä¼šè¯ç®¡ç†
        self.session_memory = {}
        self.agent_integration = None
        
        # æ¨¡å‹é…ç½® - ä½¿ç”¨è½»é‡çº§ä¸­è‹±æ–‡åŒè¯­æ¨¡å‹
        # ChatGLM2-6B: è½»é‡çº§(6Bå‚æ•°)ï¼Œä¸­è‹±æ–‡æ”¯æŒä¼˜ç§€ï¼Œä¸“ä¸ºå¯¹è¯ä¼˜åŒ–
        self.model_name = "THUDM/chatglm2-6b"  # ä¸»è¦æ¨¡å‹
        
        # å¤‡ç”¨æ¨¡å‹ï¼ˆå¦‚æœChatGLM2ä¸å¯ç”¨ï¼‰
        # self.model_name = "microsoft/DialoGPT-small"  # è‹±æ–‡ä¸ºä¸»ï¼Œä¸­æ–‡æ”¯æŒæœ‰é™
        
        # ç¡¬ä»¶è¦æ±‚ï¼šChatGLM2-6B çº¦éœ€è¦ 12GB å†…å­˜
        
        # æ•°æ®åº“é…ç½®
        self._mock_db = True
        self._mock_requirement_id = 1000
        
        # AIæ¨¡å‹çŠ¶æ€
        self.ai_enabled = False
        
        # åˆ†æç»“æœå­˜å‚¨
        self.analysis_results = {}
    
    def set_model(self, model_name: str):
        """åŠ¨æ€è®¾ç½®AIæ¨¡å‹"""
        supported_models = ["THUDM/chatglm2-6b", "microsoft/DialoGPT-small"]
        
        if model_name not in supported_models:
            print(f"âš ï¸ æ¨¡å‹ {model_name} æš‚ä¸æ”¯æŒ")
            print(f"ğŸ“‹ å½“å‰æ”¯æŒçš„æ¨¡å‹: {', '.join(supported_models)}")
            return
        
        self.model_name = model_name
        print(f"ğŸ”„ å·²åˆ‡æ¢åˆ°æ¨¡å‹: {model_name}")
        
        # å¦‚æœAIå·²ç»åˆå§‹åŒ–ï¼Œéœ€è¦é‡æ–°åˆå§‹åŒ–
        if self.ai_enabled:
            print("â™»ï¸ æ£€æµ‹åˆ°æ¨¡å‹å·²åˆå§‹åŒ–ï¼Œå°†é‡æ–°åŠ è½½...")
            self.ai_enabled = False
            self.conversation_model = None
            self.tokenizer = None
    
    def get_supported_models(self) -> list:
        """è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨"""
        return [
            "THUDM/chatglm2-6b",        # ä¸»è¦æ¨¡å‹ï¼šè½»é‡çº§ä¸­è‹±æ–‡
            "microsoft/DialoGPT-small"  # å¤‡ç”¨æ¨¡å‹ï¼šè‹±æ–‡ä¸ºä¸»
        ]
    
    async def initialize(self, agent_integration=None):
        """åˆå§‹åŒ–AIæ¨¡å‹å’Œä»£ç†é›†æˆ"""
        self.agent_integration = agent_integration
        await self._initialize_ai_models()
        logger.info("âœ… AIç”¨æˆ·æ²Ÿé€šä»£ç†åˆå§‹åŒ–å®Œæˆ")
    
    async def initialize_ai_communication(self):
        """åˆå§‹åŒ–AIç”¨æˆ·äº¤æµèƒ½åŠ› - å‘åå…¼å®¹æ–¹æ³•"""
        try:
            logger.info("åˆå§‹åŒ–æ™ºèƒ½å¯¹è¯AI...")
            await self._initialize_ai_models()
            logger.info("æ™ºèƒ½å¯¹è¯AIåˆå§‹åŒ–æˆåŠŸ")
            return True
        except Exception as e:
            logger.error(f"AIäº¤æµåˆå§‹åŒ–é”™è¯¯: {e}")
            return False

    async def _initialize_ai_models(self):
        """åˆå§‹åŒ–çœŸå®AIæ¨¡å‹"""
        try:
            from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM
            
            print("ğŸ”§ å¼€å§‹åˆå§‹åŒ–AIå¯¹è¯æ¨¡å‹...")
            
            model_name = self.model_name
            print(f"ğŸ“¦ æ­£åœ¨åŠ è½½æ¨¡å‹: {model_name}")
            
            # æ¨¡å‹å…¼å®¹æ€§æ£€æŸ¥
            is_chatglm = "chatglm" in model_name.lower()
            is_dialogpt = "DialoGPT" in model_name
            
            print(f"ğŸ” æ¨¡å‹ç±»å‹æ£€æµ‹: ChatGLM={is_chatglm}, DialoGPT={is_dialogpt}")
            
            # åˆå§‹åŒ–tokenizer
            try:
                if is_chatglm:
                    print("ğŸ”§ ä½¿ç”¨ChatGLMé…ç½®åŠ è½½tokenizer...")
                    self.tokenizer = AutoTokenizer.from_pretrained(
                        model_name, 
                        trust_remote_code=True
                    )
                else:
                    self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                print("âœ… TokenizeråŠ è½½æˆåŠŸ")
            except Exception as e:
                print(f"âŒ TokenizeråŠ è½½å¤±è´¥: {e}")
                if is_chatglm:
                    print("ğŸ”„ ChatGLMåŠ è½½å¤±è´¥ï¼Œå°è¯•é™çº§åˆ°DialoGPT...")
                    model_name = "microsoft/DialoGPT-small"
                    self.model_name = model_name
                    is_chatglm = False
                    is_dialogpt = True
                    self.tokenizer = AutoTokenizer.from_pretrained(model_name)
                    print("âœ… é™çº§TokenizeråŠ è½½æˆåŠŸ")
                else:
                    raise
            
            # é…ç½®tokenizer
            if self.tokenizer.pad_token is None:
                self.tokenizer.pad_token = self.tokenizer.eos_token
                print("ğŸ”§ å·²è®¾ç½®pad_token")
            
            # åˆå§‹åŒ–å¯¹è¯ç”Ÿæˆpipeline
            device = "cuda" if self._has_gpu() else "cpu"
            print(f"ğŸ–¥ï¸ ä½¿ç”¨è®¾å¤‡: {device}")
            
            # åªå¯¹éChatGLMæ¨¡å‹è®¾ç½®padding_side
            if not is_chatglm:
                self.tokenizer.padding_side = "left"
                print("ğŸ”§ å·²è®¾ç½®padding_side")
            
            print("ğŸš€ æ­£åœ¨åˆ›å»ºå¯¹è¯ç”Ÿæˆpipeline...")
            
            # æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨ä¸åŒçš„pipelineé…ç½®
            if is_chatglm:
                print("ğŸ”§ ä½¿ç”¨ChatGLMä¸“ç”¨é…ç½®...")
                # ChatGLMä½¿ç”¨æ›´ç®€åŒ–çš„é…ç½®
                self.conversation_model = pipeline(
                    "text-generation",
                    model=model_name,
                    tokenizer=self.tokenizer,
                    device_map="auto" if self._has_gpu() else None,
                    trust_remote_code=True
                )
            else:
                print("ğŸ”§ ä½¿ç”¨DialoGPTé…ç½®...")
                self.conversation_model = pipeline(
                    "text-generation",
                    model=model_name,
                    tokenizer=self.tokenizer,
                    device_map="auto" if self._has_gpu() else None,
                    return_full_text=False,
                    truncation=True,
                    max_length=1024
                )
            print("âœ… Pipelineåˆ›å»ºæˆåŠŸ")
            
            # é¢„çƒ­æ¨¡å‹
            try:
                print("ğŸ”¥ é¢„çƒ­AIæ¨¡å‹...")
                
                if is_chatglm:
                    print("ğŸ§ª ChatGLM2æµ‹è¯•æ ¼å¼")
                    test_inputs = ["ä½ å¥½", "Hello", "ç”¨æˆ·: ä½ å¥½\nåŠ©æ‰‹:"]
                else:
                    print("ğŸ§ª DialoGPTæµ‹è¯•æ ¼å¼")
                    test_inputs = ["Hello", "ä½ å¥½", "User: Hello\nBot:"]
                
                for i, test_input in enumerate(test_inputs):
                    try:
                        print(f"ğŸ§ª æµ‹è¯•{i+1}: '{test_input}'")
                        test_result = self.conversation_model(test_input, max_new_tokens=10, do_sample=False)
                        print(f"   ç»“æœ: {test_result}")
                        
                        if test_result and len(test_result) > 0:
                            generated = test_result[0].get("generated_text", "")
                            if len(generated.strip()) > len(test_input.strip()):
                                print(f"   âœ… æ ¼å¼{i+1}ç”Ÿæˆæœ‰æ•ˆå†…å®¹")
                                break
                    except Exception as test_error:
                        print(f"   âŒ æ ¼å¼{i+1}æµ‹è¯•å¤±è´¥: {test_error}")
                        continue
                
                print("âœ… æ¨¡å‹é¢„çƒ­æˆåŠŸ")
            except Exception as e:
                print(f"âš ï¸ æ¨¡å‹é¢„çƒ­å¤±è´¥: {e}")
                # é¢„çƒ­å¤±è´¥ä¸å½±å“æ•´ä½“åˆå§‹åŒ–
            
            self.ai_enabled = True
            print("ğŸ‰ AIå¯¹è¯æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")
            
        except ImportError:
            error_msg = "transformersåº“æœªå®‰è£…,AIåŠŸèƒ½æ— æ³•ä½¿ç”¨"
            logger.error(error_msg)
            print(f"âŒ {error_msg}")
            print("ğŸ’¡ è¯·å®‰è£…: pip install transformers torch")
            self.ai_enabled = False
        except Exception as e:
            error_msg = f"AIæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}"
            logger.error(error_msg)
            print(f"âŒ {error_msg}")
            import traceback
            print(f"ğŸ› è¯¦ç»†é”™è¯¯: {traceback.format_exc()}")
            self.ai_enabled = False
    
    def _has_gpu(self) -> bool:
        """æ£€æµ‹æ˜¯å¦æœ‰GPUå¯ç”¨"""
        try:
            import torch
            return torch.cuda.is_available()
        except:
            return False

    async def handle_message(self, message: Message):
        """å¤„ç†ç”¨æˆ·è¾“å…¥æ¶ˆæ¯"""
        try:
            if message.message_type == "user_input":
                await self._process_user_input(message.content)
            elif message.message_type == "system_feedback":
                await self._process_system_feedback(message.content)
            elif message.message_type == "analysis_result":
                await self._process_analysis_result(message.content)
            else:
                logger.warning(f"æœªçŸ¥æ¶ˆæ¯ç±»å‹: {message.message_type}")
                print(f"âŒ ç³»ç»Ÿé”™è¯¯: æ”¶åˆ°æœªçŸ¥æ¶ˆæ¯ç±»å‹: {message.message_type}")
        except Exception as e:
            logger.error(f"å¤„ç†æ¶ˆæ¯æ—¶å‡ºé”™: {e}")
            print(f"âŒ ç³»ç»Ÿé”™è¯¯: æ¶ˆæ¯å¤„ç†å¼‚å¸¸ ({str(e)})")
            raise

    async def _process_user_input(self, content: Dict[str, Any]):
        """å¤„ç†ç”¨æˆ·è¾“å…¥ - AIé©±åŠ¨çš„å¯¹è¯å¼•æ“"""
        user_message = content.get("message", "")
        session_id = content.get("session_id", "default")
        target_directory = content.get("target_directory")
        
        logger.info(f"å¤„ç†ç”¨æˆ·è¾“å…¥: {user_message[:50]}...")
        
        # ä½¿ç”¨AIé©±åŠ¨çš„å¯¹è¯å¤„ç†
        if self.ai_enabled and self.conversation_model:
            try:
                logger.info("å¼€å§‹AIå¯¹è¯å¤„ç†...")
                response, actions = await self.process_ai_conversation(
                    user_message, session_id, target_directory
                )
                
                if response:
                    logger.info(f"AIå›åº”ç”ŸæˆæˆåŠŸ: {len(response)} å­—ç¬¦")
                    print(response)
                    
                    await self._execute_ai_actions(actions, session_id)
                    return
                else:
                    logger.error("AIå“åº”ç”Ÿæˆå¤±è´¥,æ— æ³•è·å–æœ‰æ•ˆå›å¤")
                    print("âŒ ç³»ç»Ÿé”™è¯¯: AIå“åº”ç”Ÿæˆå¤±è´¥")
                    return
                
            except Exception as e:
                logger.error(f"AIå¯¹è¯å¤„ç†å¤±è´¥: {e}")
                print(f"âŒ ç³»ç»Ÿé”™è¯¯: AIå¤„ç†å¼‚å¸¸ ({str(e)})")
                return
        
        # AIæ¨¡å‹æœªå¯ç”¨
        logger.error("AIæ¨¡å‹æœªå¯ç”¨,æ— æ³•å¤„ç†ç”¨æˆ·è¾“å…¥")
        print("âŒ ç³»ç»Ÿé”™è¯¯: AIæ¨¡å‹æœªå¯ç”¨æˆ–åˆå§‹åŒ–å¤±è´¥")

    async def process_ai_conversation(self, user_message: str, session_id: str, target_directory: str = None):
        """AIé©±åŠ¨çš„å¯¹è¯å¤„ç†"""
        try:
            logger.info("å¼€å§‹AIå¯¹è¯å¤„ç†æµç¨‹...")
            
            # 1. æ›´æ–°ä¼šè¯ä¸Šä¸‹æ–‡
            self._update_session_context(user_message, session_id, target_directory)
            
            # 2. å‡†å¤‡AIå¯¹è¯ä¸Šä¸‹æ–‡
            conversation_history = self._format_conversation_history(session_id)
            
            # 3. æ„å»ºAI prompt
            try:
                ai_prompt = get_prompt(
                    task_type="conversation",
                    model_name=self.model_name,
                    user_message=user_message,
                    conversation_history=conversation_history
                )
            except (ValueError, KeyError) as e:
                logger.warning(f"è·å–Promptå¤±è´¥,ä½¿ç”¨ç®€åŒ–æ ¼å¼: {e}")
                if "chatglm" in self.model_name.lower():
                    ai_prompt = f"ç”¨æˆ·: {user_message}\nåŠ©æ‰‹:"
                else:
                    ai_prompt = user_message
            
            # 4. ä½¿ç”¨AIæ¨¡å‹ç”Ÿæˆå›åº”
            ai_response = await self._generate_ai_response(ai_prompt)
            
            if not ai_response:
                logger.error("AIå›åº”ç”Ÿæˆå¤±è´¥")
                return None, {"next_action": "continue_conversation"}
                
            logger.info(f"AIå›åº”ç”ŸæˆæˆåŠŸ: {len(ai_response)} å­—ç¬¦")
            
            # 5. æ›´æ–°ä¼šè¯è®°å¿†
            self._update_session_memory_simple(session_id, ai_response, user_message)
            
            # 6. ç®€å•çš„æ„å›¾æ£€æµ‹
            next_action = self._detect_simple_intent(user_message, ai_response)
            
            return ai_response, {
                "intent": "conversation",
                "next_action": next_action,
                "extracted_info": {},
                "confidence": 1.0
            }
            
        except Exception as e:
            logger.error(f"AIå¯¹è¯å¤„ç†å¤±è´¥: {e}")
            print(f"âŒ ç³»ç»Ÿé”™è¯¯: {e}")
            return None, {"next_action": "continue_conversation"}
    
    # === ä¼šè¯ç®¡ç†æ–¹æ³• ===
    
    def _update_session_context(self, message: str, session_id: str, target_directory: str = None):
        """æ›´æ–°ä¼šè¯ä¸Šä¸‹æ–‡"""
        if session_id not in self.session_memory:
            self.session_memory[session_id] = {
                "messages": [],
                "collected_info": {},
                "last_active": self._get_current_time()
            }
        
        session = self.session_memory[session_id]
        session["messages"].append({
            "content": message,
            "timestamp": self._get_current_time(),
            "type": "user"
        })
        session["last_active"] = self._get_current_time()
        
        if target_directory:
            session["target_directory"] = target_directory
    
    def _format_conversation_history(self, session_id: str) -> str:
        """æ ¼å¼åŒ–å¯¹è¯å†å²"""
        session = self.session_memory.get(session_id, {})
        messages = session.get("messages", [])
        
        if not messages:
            return "é¦–æ¬¡å¯¹è¯"
        
        # è·å–æœ€è¿‘çš„3-5æ¡æ¶ˆæ¯
        recent_messages = messages[-5:]
        formatted = []
        
        for msg in recent_messages:
            role = "ç”¨æˆ·" if msg.get("type") == "user" else "AI"
            content = msg.get("content", "")[:100]  # é™åˆ¶é•¿åº¦
            formatted.append(f"{role}: {content}")
        
        return "\n".join(formatted)
    
    def _update_session_memory_simple(self, session_id: str, ai_response: str, user_message: str):
        """ç®€åŒ–çš„ä¼šè¯è®°å¿†æ›´æ–°"""
        if session_id not in self.session_memory:
            self.session_memory[session_id] = {
                "messages": [],
                "last_active": self._get_current_time()
            }
        
        session = self.session_memory[session_id]
        
        # æ·»åŠ AIå›åº”åˆ°å¯¹è¯å†å²
        session["messages"].append({
            "content": ai_response,
            "timestamp": self._get_current_time(),
            "type": "ai"
        })
        
        session["last_active"] = self._get_current_time()
        
        # ä¿æŒå¯¹è¯å†å²åœ¨åˆç†èŒƒå›´å†…
        if len(session["messages"]) > 20:
            session["messages"] = session["messages"][-15:]
    
    def _detect_simple_intent(self, user_message: str, ai_response: str) -> str:
        """åŸºäºå…³é”®è¯çš„ç®€å•æ„å›¾æ£€æµ‹"""
        user_lower = user_message.lower()
        
        # æ£€æµ‹ä»£ç åˆ†æç›¸å…³å…³é”®è¯
        analysis_keywords = ["åˆ†æ", "æ£€æŸ¥", "å®¡æŸ¥", "æ‰«æ", "analysis", "scan", "check", "review"]
        path_keywords = ["è·¯å¾„", "ç›®å½•", "æ–‡ä»¶å¤¹", "ä»£ç ", "é¡¹ç›®", "path", "directory", "folder", "code"]
        
        if any(keyword in user_lower for keyword in analysis_keywords):
            if any(keyword in user_lower for keyword in path_keywords):
                return "start_analysis"
            else:
                return "collect_info"
        
        return "continue_conversation"
    
    async def _execute_ai_actions(self, actions: Dict[str, Any], session_id: str):
        """æ‰§è¡ŒAIå»ºè®®çš„æ“ä½œ"""
        next_action = actions.get("next_action")
        
        if next_action == "start_analysis":
            extracted_info = actions.get("extracted_info", {})
            await self._start_code_analysis(extracted_info, session_id)
        elif next_action == "collect_info":
            # ç»§ç»­ä¿¡æ¯æ”¶é›†
            pass
        else:
            # ç»§ç»­å¯¹è¯
            pass
    
    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        return datetime.datetime.now().isoformat()
    
    # === AIæ ¸å¿ƒæ–¹æ³• ===
    
    async def _generate_ai_response(self, prompt: str) -> str:
        """ä½¿ç”¨AIæ¨¡å‹ç”Ÿæˆå›åº”"""
        try:
            if not self.ai_enabled or not self.conversation_model:
                logger.error("AIæ¨¡å‹çŠ¶æ€æ£€æŸ¥å¤±è´¥")
                return None
            
            print(f"ğŸ§  å¼€å§‹AIå›åº”ç”Ÿæˆ...")
            print(f"ğŸ“ è¾“å…¥prompt: '{prompt[:100]}{'...' if len(prompt) > 100 else ''}'")
            
            is_chatglm = "chatglm" in self.model_name.lower()
            
            # æ ¹æ®æ¨¡å‹ç±»å‹ä½¿ç”¨ä¸åŒçš„ç”Ÿæˆç­–ç•¥
            try:
                if is_chatglm:
                    print("ğŸ¤– ä½¿ç”¨ChatGLM2ç”Ÿæˆç­–ç•¥...")
                    # ChatGLMä½¿ç”¨æœ€ç®€åŒ–å‚æ•°ï¼Œé¿å…tokenizerå…¼å®¹æ€§é—®é¢˜
                    result = self.conversation_model(
                        prompt,
                        max_length=len(prompt) + 50,  # ä½¿ç”¨max_lengthè€Œä¸æ˜¯max_new_tokens
                        do_sample=False  # ç¦ç”¨é‡‡æ ·é¿å…å…¼å®¹æ€§é—®é¢˜
                    )
                else:
                    print("ğŸ¤– ä½¿ç”¨DialoGPTç”Ÿæˆç­–ç•¥...")
                    result = self.conversation_model(
                        prompt,
                        max_new_tokens=50,
                        temperature=0.8,
                        do_sample=True,
                        repetition_penalty=1.2,
                        pad_token_id=self.tokenizer.pad_token_id,
                        eos_token_id=self.tokenizer.eos_token_id
                    )
                
                print(f"âœ… æ¨¡å‹è°ƒç”¨å®Œæˆ")
                
                if result and len(result) > 0:
                    raw_text = result[0]["generated_text"]
                    print(f"ğŸ“„ åŸå§‹ç”Ÿæˆæ–‡æœ¬: '{raw_text}'")
                    
                    # æ¸…ç†å’Œæå–å›åº”
                    ai_response = self._clean_ai_response(raw_text, prompt)
                    
                    if ai_response and len(ai_response.strip()) >= 2:
                        print(f"ğŸ‰ AIå›åº”ç”ŸæˆæˆåŠŸ: '{ai_response}'")
                        return ai_response
                    else:
                        print(f"âš ï¸ AIç”Ÿæˆçš„å›åº”è¿‡çŸ­æˆ–æ— æ•ˆ")
                        return "ä½ å¥½ï¼æˆ‘æ˜¯MASä»£ç åˆ†æåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ã€‚"
                else:
                    print("âŒ AIæ¨¡å‹è¿”å›ç©ºç»“æœ")
                    return "æŠ±æ­‰ï¼Œæˆ‘æš‚æ—¶æ— æ³•ç”Ÿæˆå›åº”ã€‚è¯·ç¨åå†è¯•ã€‚"
                    
            except Exception as model_error:
                print(f"âŒ æ¨¡å‹è°ƒç”¨è¿‡ç¨‹å‡ºé”™: {model_error}")
                return None
            
        except Exception as e:
            logger.error(f"AIæ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")
            return None
    
    def _clean_ai_response(self, raw_text: str, prompt: str) -> str:
        """æ¸…ç†AIç”Ÿæˆçš„å›åº”"""
        # ç§»é™¤promptéƒ¨åˆ†ï¼Œåªä¿ç•™æ–°ç”Ÿæˆçš„å†…å®¹
        if prompt in raw_text:
            ai_response = raw_text.replace(prompt, "").strip()
        else:
            ai_response = raw_text.strip()
        
        # æ¸…ç†å¸¸è§çš„æ¨¡å‹è¾“å‡ºå‰ç¼€/åç¼€
        cleanup_patterns = [
            r'^[Bb]ot:\s*',
            r'^[Aa][Ii]:\s*',
            r'^[Aa]ssistant:\s*',
            r'^åŠ©æ‰‹:\s*',
            r'^\s*[:ï¼š]\s*',
        ]
        
        for pattern in cleanup_patterns:
            ai_response = re.sub(pattern, '', ai_response, flags=re.IGNORECASE)
        
        ai_response = ai_response.strip()
        
        # å¦‚æœå›åº”ä»ç„¶è¿‡çŸ­ï¼Œå°è¯•æå–æœ€åä¸€è¡Œ
        if len(ai_response) < 3:
            lines = raw_text.strip().split('\n')
            if len(lines) > 1:
                last_line = lines[-1].strip()
                if len(last_line) > len(ai_response):
                    ai_response = last_line
        
        return ai_response
    
    # === å…¶ä»–å¿…è¦æ–¹æ³•çš„ç®€åŒ–å®ç° ===
    
    async def _process_system_feedback(self, content: Dict[str, Any]):
        """å¤„ç†ç³»ç»Ÿåé¦ˆ"""
        feedback_type = content.get("type", "unknown")
        feedback_message = content.get("message", "")
        print(f"ğŸ“Š ç³»ç»Ÿåé¦ˆ: {feedback_message}")
    
    async def _process_analysis_result(self, content: Dict[str, Any]):
        """å¤„ç†åˆ†æç»“æœ"""
        agent_type = content.get("agent_type")
        requirement_id = content.get("requirement_id")
        print(f"ğŸ“Š æ”¶åˆ° {agent_type} åˆ†æç»“æœ (ä»»åŠ¡ID: {requirement_id})")
    
    async def _start_code_analysis(self, extracted_info: Dict[str, Any], session_id: str):
        """å¯åŠ¨ä»£ç åˆ†æ"""
        print("ğŸš€ å¯åŠ¨ä»£ç åˆ†æ...")
    
    async def _execute_task_impl(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡Œç”¨æˆ·æ²Ÿé€šä»»åŠ¡"""
        return {"status": "user_communication_ready", "timestamp": self._get_current_time()}