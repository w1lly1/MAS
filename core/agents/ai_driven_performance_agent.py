import os
import torch
import asyncio
import ast
import time
from transformers import pipeline, AutoTokenizer, AutoModelForSequenceClassification, AutoModelForCausalLM
from typing import Dict, Any, List, Tuple
from .base_agent import BaseAgent, Message
from infrastructure.database.sqlite.service import DatabaseService
from infrastructure.config.settings import HUGGINGFACE_CONFIG
from infrastructure.config.ai_agents import get_ai_agent_config
from infrastructure.config.prompts import get_prompt
from infrastructure.reports import report_manager
from utils import log, LogLevel

class AIDrivenPerformanceAgent(BaseAgent):
    """AIé©±åŠ¨çš„æ€§èƒ½åˆ†ææ™ºèƒ½ä½“ - åŸºäºæ·±åº¦å­¦ä¹ å’Œpromptå·¥ç¨‹"""
    
    def __init__(self):
        super().__init__("ai_performance_agent", "AI Performance Analysis Agent")
        self.db_service = DatabaseService()
        self.used_device = "gpu"
        self.used_device_map = None  # æ·»åŠ è®¾å¤‡æ˜ å°„å‚æ•°
        # ä»ç»Ÿä¸€é…ç½®è·å–
        self.agent_config = get_ai_agent_config().get_performance_agent_config()
        self.model_config = HUGGINGFACE_CONFIG["models"]["performance"]
        self.performance_model = None
        self.complexity_analyzer = None
        self.optimization_advisor = None

    async def _initialize_models(self):
        """åˆå§‹åŒ–AIæ¨¡å‹ - æ”¯æŒ CPU/GPU åŠ¨æ€é€‰æ‹©"""
        try:
            # éªŒè¯ used_device å‚æ•°
            if self.used_device not in ["cpu", "gpu"]:
                log("ai_performance_agent", LogLevel.INFO, f"âš ï¸ [ai_performance_agent] æ— æ•ˆçš„è®¾å¤‡å‚æ•°: {self.used_device}ï¼Œå›é€€åˆ°CPU")
                self.used_device = "cpu"
            
            device_mode = "CPU" if self.used_device == "cpu" else "GPU"
            log("ai_performance_agent", LogLevel.INFO, f"ğŸ”§ [ai_performance_agent] åˆå§‹åŒ–æ€§èƒ½åˆ†æAIæ¨¡å‹ ({device_mode}æ¨¡å¼)...")
            
            # ä¼˜å…ˆä½¿ç”¨agentä¸“å±é…ç½®ï¼Œå›é€€åˆ°HUGGINGFACE_CONFIG
            model_name = self.agent_config.get("model_name", "microsoft/codebert-base")
            cache_dir = HUGGINGFACE_CONFIG.get("cache_dir", "./model_cache/")
            device = -1 if self.used_device == "cpu" else 0
            
            # ä»…åœ¨CPUæ¨¡å¼ä¸‹è®¾ç½®çº¿ç¨‹æ•°
            if self.used_device == "cpu":
                cpu_threads = self.agent_config.get("cpu_threads", 4)
                torch.set_num_threads(cpu_threads)
            
            log("ai_performance_agent", LogLevel.INFO, f"ğŸ¤– [ai_performance_agent] æ­£åœ¨åŠ è½½æ€§èƒ½åˆ†ææ¨¡å‹ ({device_mode}æ¨¡å¼): {model_name}")
            log("ai_performance_agent", LogLevel.INFO, f"ğŸ’¾ [ai_performance_agent] ç¼“å­˜ç›®å½•: {cache_dir}")
            
            try:
                # å…ˆå°è¯•ä»æœ¬åœ°ç¼“å­˜åŠ è½½ï¼Œå¦‚æœå¤±è´¥åˆ™å…è®¸è”ç½‘ä¸‹è½½å¹¶ç¼“å­˜
                try:
                    tokenizer = AutoTokenizer.from_pretrained(
                        model_name,
                        cache_dir=cache_dir,
                        local_files_only=True,
                        trust_remote_code=False
                    )
                    model = AutoModelForSequenceClassification.from_pretrained(
                        model_name,
                        cache_dir=cache_dir,
                        local_files_only=True,
                        torch_dtype=getattr(torch, self.agent_config.get("torch_dtype", "float32")),
                        low_cpu_mem_usage=self.agent_config.get("low_cpu_mem_usage", True)
                    )
                    log("ai_performance_agent", LogLevel.INFO, f"âœ… [ai_performance_agent] {model_name} æ€§èƒ½æ¨¡å‹(æœ¬åœ°ç¼“å­˜)åˆå§‹åŒ–æˆåŠŸ")
                except Exception as local_err:
                    log("ai_performance_agent", LogLevel.INFO, f"âš ï¸ [ai_performance_agent] æœ¬åœ°ç¼“å­˜æœªå°±ç»ªï¼Œå°è¯•è”ç½‘ä¸‹è½½: {local_err}")
                    tokenizer = AutoTokenizer.from_pretrained(
                        model_name,
                        cache_dir=cache_dir,
                        local_files_only=False,
                        trust_remote_code=False
                    )
                    model = AutoModelForSequenceClassification.from_pretrained(
                        model_name,
                        cache_dir=cache_dir,
                        local_files_only=False,
                        torch_dtype=getattr(torch, self.agent_config.get("torch_dtype", "float32")),
                        low_cpu_mem_usage=self.agent_config.get("low_cpu_mem_usage", True)
                    )
                    log("ai_performance_agent", LogLevel.INFO, f"âœ… [ai_performance_agent] {model_name} æ€§èƒ½æ¨¡å‹(è”ç½‘ä¸‹è½½å¹¶ç¼“å­˜)åˆå§‹åŒ–æˆåŠŸ")

                self.performance_model = pipeline(
                    "text-classification",
                    model=model,
                    tokenizer=tokenizer,
                    device=device
                )
            except Exception as model_error:
                log("ai_performance_agent", LogLevel.INFO, f"âš ï¸ [ai_performance_agent] ä¸»æ¨¡å‹åŠ è½½å¤±è´¥,å°è¯•å¤‡ç”¨æ¨¡å‹: {model_error}")
                fallback_model = self.agent_config.get("fallback_model", "distilbert-base-uncased")
                try:
                    tokenizer = AutoTokenizer.from_pretrained(
                        fallback_model,
                        cache_dir=cache_dir,
                        local_files_only=True,
                        trust_remote_code=False
                    )
                    model = AutoModelForSequenceClassification.from_pretrained(
                        fallback_model,
                        cache_dir=cache_dir,
                        local_files_only=True,
                        low_cpu_mem_usage=True
                    )
                    log("ai_performance_agent", LogLevel.INFO, f"âœ… [ai_performance_agent] å¤‡ç”¨æ¨¡å‹(æœ¬åœ°ç¼“å­˜)åŠ è½½æˆåŠŸ: {fallback_model}")
                except Exception as fb_local_err:
                    log("ai_performance_agent", LogLevel.INFO, f"âš ï¸ [ai_performance_agent] å¤‡ç”¨æ¨¡å‹æœ¬åœ°ç¼“å­˜æœªå°±ç»ªï¼Œå°è¯•è”ç½‘ä¸‹è½½: {fb_local_err}")
                    tokenizer = AutoTokenizer.from_pretrained(
                        fallback_model,
                        cache_dir=cache_dir,
                        local_files_only=False,
                        trust_remote_code=False
                    )
                    model = AutoModelForSequenceClassification.from_pretrained(
                        fallback_model,
                        cache_dir=cache_dir,
                        local_files_only=False,
                        low_cpu_mem_usage=True
                    )
                    log("ai_performance_agent", LogLevel.INFO, f"âœ… [ai_performance_agent] å¤‡ç”¨æ¨¡å‹(è”ç½‘ä¸‹è½½å¹¶ç¼“å­˜)åŠ è½½æˆåŠŸ: {fallback_model}")

                self.performance_model = pipeline(
                    "text-classification",
                    model=model,
                    tokenizer=tokenizer,
                    device=device
                )

            try:
                # ä¸º text-generation ä¹Ÿä¼˜å…ˆä½¿ç”¨æœ¬åœ°ç¼“å­˜ï¼Œå¦‚æœå¤±è´¥åˆ™è”ç½‘ä¸‹è½½
                text_gen_model = self.agent_config.get("text_generator_model", "gpt2")
                try:
                    tokenizer_gen = AutoTokenizer.from_pretrained(
                        text_gen_model,
                        cache_dir=cache_dir,
                        local_files_only=True,
                        trust_remote_code=False
                    )
                    model_gen = AutoModelForCausalLM.from_pretrained(
                        text_gen_model,
                        cache_dir=cache_dir,
                        local_files_only=True,
                        low_cpu_mem_usage=True
                    )
                    log("ai_performance_agent", LogLevel.INFO, f"âœ… [ai_performance_agent] {text_gen_model} ä¼˜åŒ–å»ºè®®æ¨¡å‹(æœ¬åœ°ç¼“å­˜)åŠ è½½æˆåŠŸ")
                except Exception as tg_local_err:
                    log("ai_performance_agent", LogLevel.INFO, f"âš ï¸ [ai_performance_agent] æ–‡æœ¬ç”Ÿæˆæ¨¡å‹æœ¬åœ°ç¼“å­˜æœªå°±ç»ªï¼Œå°è¯•è”ç½‘ä¸‹è½½: {tg_local_err}")
                    tokenizer_gen = AutoTokenizer.from_pretrained(
                        text_gen_model,
                        cache_dir=cache_dir,
                        local_files_only=False,
                        trust_remote_code=False
                    )
                    model_gen = AutoModelForCausalLM.from_pretrained(
                        text_gen_model,
                        cache_dir=cache_dir,
                        local_files_only=False,
                        low_cpu_mem_usage=True
                    )
                    log("ai_performance_agent", LogLevel.INFO, f"âœ… [ai_performance_agent] {text_gen_model} ä¼˜åŒ–å»ºè®®æ¨¡å‹(è”ç½‘ä¸‹è½½å¹¶ç¼“å­˜)åŠ è½½æˆåŠŸ")

                self.optimization_generator = pipeline(
                    "text-generation",
                    model=model_gen,
                    tokenizer=tokenizer_gen,
                    device=device
                )
                if self.optimization_generator.tokenizer.pad_token is None:
                    self.optimization_generator.tokenizer.pad_token = self.optimization_generator.tokenizer.eos_token
            except Exception as gen_error:
                log("ai_performance_agent", LogLevel.INFO, f"âš ï¸ [ai_performance_agent] ä¼˜åŒ–å»ºè®®æ¨¡å‹åŠ è½½å¤±è´¥: {gen_error}")
                self.optimization_generator = None
                
            self.models_loaded = True
            log("ai_performance_agent", LogLevel.INFO, f"âœ… [ai_performance_agent] æ€§èƒ½åˆ†æAIæ¨¡å‹åˆå§‹åŒ–å®Œæˆ ({device_mode}æ¨¡å¼)")
            
        except Exception as e:
            log("ai_performance_agent", LogLevel.INFO, f"âŒ [ai_performance_agent] æ€§èƒ½åˆ†æAIæ¨¡å‹åˆå§‹åŒ–å¤±è´¥: {e}")
            self.models_loaded = False
            # è®¾ç½®å¤‡ç”¨çŠ¶æ€
            self.performance_model = None
            self.optimization_generator = None

    async def handle_message(self, message: Message):
        """å¤„ç†æ€§èƒ½åˆ†æè¯·æ±‚"""
        if message.message_type == "performance_analysis_request":
            requirement_id = message.content.get("requirement_id")
            code_content = message.content.get("code_content", "")
            code_directory = message.content.get("code_directory", "")
            file_path = message.content.get("file_path")
            run_id = message.content.get('run_id')
            log("ai_performance_agent", LogLevel.INFO, f"âš¡ AIæ€§èƒ½åˆ†æå¼€å§‹ - éœ€æ±‚ID: {requirement_id} run_id={run_id}")
            if not self.performance_model:
                log("ai_performance_agent", LogLevel.INFO, f"âš ï¸ [ai_performance_agent] æ€§èƒ½åˆ†ææ¨¡å‹æœªåŠ è½½ï¼Œå°è¯•åˆå§‹åŒ–")
                await self._initialize_models()
            result = await self._ai_driven_performance_analysis(code_content, code_directory)
            # é¢å¤–: ç”Ÿæˆè¯¥Agentå•ç‹¬æŠ¥å‘Š (æŒ‰ run_id/agents/performance )
            if run_id:
                try:
                    per_agent_payload = {
                        "requirement_id": requirement_id,
                        "file_path": file_path,
                        "run_id": run_id,
                        "performance_result": result,
                        "generated_at": self._get_current_time()
                    }
                    report_manager.generate_run_scoped_report(run_id, per_agent_payload, f"performance_req_{requirement_id}.json", subdir="agents/performance")
                except Exception as e:
                    log("ai_performance_agent", LogLevel.INFO, f"âš ï¸ æ€§èƒ½Agentå•ç‹¬æŠ¥å‘Šç”Ÿæˆå¤±è´¥ requirement={requirement_id} run_id={run_id}: {e}")
            # å‘é€åˆ°ç”¨æˆ·äº¤äº’
            await self.dispatch_message(
                receiver="user_comm_agent",
                content={
                    "requirement_id": requirement_id,
                    "agent_type": "ai_performance",
                    "results": result,
                    "analysis_complete": True,
                    "file_path": file_path,
                    "run_id": run_id
                },
                message_type="analysis_result"
            )
            # å‘é€åˆ°æ±‡æ€»
            await self.dispatch_message(
                receiver="summary_agent",
                content={
                    "requirement_id": requirement_id,
                    "analysis_type": "performance_analysis",
                    "result": result,
                    "file_path": file_path,
                    "run_id": run_id
                },
                message_type="analysis_result"
            )
            log("ai_performance_agent", LogLevel.INFO, f"âœ… AIæ€§èƒ½åˆ†æå®Œæˆ - éœ€æ±‚ID: {requirement_id} run_id={run_id}")

    async def _ai_driven_performance_analysis(self, code_content: str, code_directory: str) -> Dict[str, Any]:
        """AIé©±åŠ¨çš„å…¨é¢æ€§èƒ½åˆ†æ"""
        
        try:
            log("ai_performance_agent", LogLevel.INFO, "ğŸ” AIæ­£åœ¨è¿›è¡Œæ·±åº¦æ€§èƒ½åˆ†æ...")
            
            # 1. ä»£ç ç»“æ„å’Œç¯å¢ƒåˆ†æ
            code_structure = await self._analyze_code_structure(code_content, code_directory)
            
            # 2. AIç®—æ³•å¤æ‚åº¦åˆ†æ
            complexity_analysis = await self._ai_complexity_analysis(code_content)
            
            # 3. AIæ€§èƒ½ç“¶é¢ˆæ£€æµ‹
            bottlenecks = await self._ai_bottleneck_detection(code_content, code_structure)
            
            # 4. AIæ€§èƒ½è¯„åˆ†
            performance_score = await self._ai_performance_scoring(complexity_analysis, bottlenecks)
            
            # 5. AIä¼˜åŒ–å»ºè®®ç”Ÿæˆ
            optimization_plan = await self._ai_optimization_planning(
                code_content, bottlenecks, complexity_analysis
            )
            
            # 6. AIæ€§èƒ½æµ‹è¯•å»ºè®®
            testing_recommendations = await self._ai_testing_recommendations(code_structure)
            
            log("ai_performance_agent", LogLevel.INFO, "ğŸš€ AIæ€§èƒ½åˆ†æå®Œæˆ,ç”Ÿæˆä¼˜åŒ–æŠ¥å‘Š")
            
            return {
                "ai_performance_analysis": {
                    "overall_performance_score": performance_score,
                    "code_structure_analysis": code_structure,
                    "complexity_analysis": complexity_analysis,
                    "performance_bottlenecks": bottlenecks,
                    "optimization_plan": optimization_plan,
                    "testing_recommendations": testing_recommendations,
                    "ai_confidence": 0.88,
                    "model_used": self.model_config["name"],
                    "analysis_timestamp": self._get_current_time()
                },
                "analysis_status": "completed"
            }
            
        except Exception as e:
            log("ai_performance_agent", LogLevel.INFO, f"âŒ AIæ€§èƒ½åˆ†æè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            return {
                "ai_performance_analysis": {"error": str(e)},
                "analysis_status": "failed"
            }

    async def _analyze_code_structure(self, code_content: str, code_directory: str) -> Dict[str, Any]:
        """åˆ†æä»£ç ç»“æ„å’Œæ‰§è¡Œç¯å¢ƒ"""
        structure = {
            "language": "unknown",
            "framework": "unknown",
            "code_size": len(code_content),
            "function_count": 0,
            "class_count": 0,
            "loop_count": 0,
            "recursive_functions": [],
            "async_patterns": False,
            "database_operations": False,
            "file_operations": False,
            "network_operations": False
        }
        
        try:
            # åŸºç¡€ä»£ç åˆ†æ
            if "def " in code_content:
                structure["language"] = "python"
                structure["function_count"] = code_content.count("def ")
                structure["class_count"] = code_content.count("class ")
                structure["async_patterns"] = "async " in code_content or "await " in code_content
            
            # å¾ªç¯æ£€æµ‹
            structure["loop_count"] = (
                code_content.count("for ") + 
                code_content.count("while ") + 
                code_content.count("forEach")
            )
            
            # æ“ä½œç±»å‹æ£€æµ‹
            structure["database_operations"] = any(keyword in code_content.lower() for keyword in 
                ["sql", "select", "insert", "update", "delete", "database", "query"])
            
            structure["file_operations"] = any(keyword in code_content.lower() for keyword in
                ["open(", "read(", "write(", "file", "io"])
            
            structure["network_operations"] = any(keyword in code_content.lower() for keyword in
                ["requests", "http", "socket", "urllib", "fetch"])
            
            # AIå¢å¼ºåˆ†æ
            if self.complexity_analyzer:
                ai_structure_analysis = await self._ai_structural_analysis(code_content)
                structure.update(ai_structure_analysis)
            
        except Exception as e:
            structure["analysis_error"] = str(e)
        
        return structure

    async def _ai_complexity_analysis(self, code_content: str) -> Dict[str, Any]:
        """AIé©±åŠ¨çš„ç®—æ³•å¤æ‚åº¦åˆ†æ"""
        try:
            code_functions = self._extract_functions(code_content)
            complexity_results = []
            for i, func_code in enumerate(code_functions[:5]):
                analysis_prompt = get_prompt(
                    task_type="performance",
                    variant="algorithmic_analysis",
                    code_snippet=func_code
                )
                
                # AIå¤æ‚åº¦åˆ†ç±»
                if self.complexity_analyzer:
                    complexity_classification = self.complexity_analyzer(
                        f"Algorithm complexity analysis: {func_code[:300]}"
                    )
                    
                    # è§£æåˆ†ç±»ç»“æœ
                    complexity_data = await self._parse_complexity_result(
                        complexity_classification, func_code, i
                    )
                    
                    if complexity_data:
                        complexity_results.append(complexity_data)
                
                # AIç”Ÿæˆè¯¦ç»†åˆ†æ
                if self.optimization_advisor and len(complexity_results) < 3:
                    detailed_analysis = self.optimization_advisor(
                        analysis_prompt,
                        max_length=250,
                        temperature=0.3
                    )
                    
                    detailed_complexity = await self._extract_complexity_details(
                        detailed_analysis, func_code, i
                    )
                    
                    if detailed_complexity:
                        complexity_results.append(detailed_complexity)
            
            # ç»¼åˆå¤æ‚åº¦è¯„ä¼°
            overall_complexity = await self._calculate_overall_complexity(complexity_results)
            
            return {
                "function_complexities": complexity_results,
                "overall_complexity": overall_complexity,
                "complexity_distribution": self._analyze_complexity_distribution(complexity_results),
                "optimization_priority": self._prioritize_optimizations(complexity_results)
            }
            
        except Exception as e:
            return {"error": f"å¤æ‚åº¦åˆ†æå¤±è´¥: {e}"}

    async def _ai_bottleneck_detection(self, code_content: str, code_structure: Dict[str, Any]) -> List[Dict[str, Any]]:
        """AIé©±åŠ¨çš„æ€§èƒ½ç“¶é¢ˆæ£€æµ‹"""
        bottlenecks = []
        
        try:
            # 1. åµŒå¥—å¾ªç¯æ£€æµ‹
            nested_loops = self._detect_nested_loops(code_content)
            for loop_info in nested_loops:
                bottleneck = await self._ai_analyze_loop_bottleneck(loop_info)
                if bottleneck:
                    bottlenecks.append(bottleneck)
            
            # 2. é€’å½’å‡½æ•°åˆ†æ
            recursive_functions = self._detect_recursive_functions(code_content)
            for func_info in recursive_functions:
                bottleneck = await self._ai_analyze_recursion_bottleneck(func_info)
                if bottleneck:
                    bottlenecks.append(bottleneck)
            
            # 3. I/Oæ“ä½œåˆ†æ
            io_operations = self._detect_io_operations(code_content)
            for io_info in io_operations:
                bottleneck = await self._ai_analyze_io_bottleneck(io_info)
                if bottleneck:
                    bottlenecks.append(bottleneck)
            
            # 4. AIæ¨¡å¼è¯†åˆ«
            if self.complexity_analyzer:
                ai_bottlenecks = await self._ai_pattern_based_bottleneck_detection(code_content)
                bottlenecks.extend(ai_bottlenecks)
            
            # æŒ‰ä¸¥é‡ç¨‹åº¦æ’åº
            bottlenecks = sorted(bottlenecks, key=lambda x: x.get("severity_score", 0), reverse=True)
            
        except Exception as e:
            bottlenecks.append({
                "bottleneck_id": "DETECTION_ERROR",
                "type": "analysis_error", 
                "description": f"ç“¶é¢ˆæ£€æµ‹è¿‡ç¨‹å‡ºé”™: {e}",
                "severity": "info"
            })
        
        max_bottlenecks = self.agent_config.get("max_bottlenecks_reported", 10)
        return bottlenecks[:max_bottlenecks]  # é™åˆ¶è¿”å›æ•°é‡

    async def _ai_performance_scoring(self, complexity_analysis: Dict[str, Any], 
                                     bottlenecks: List[Dict[str, Any]]) -> Dict[str, Any]:
        """AIé©±åŠ¨çš„æ€§èƒ½è¯„åˆ†"""
        try:
            # åŸºç¡€è¯„åˆ†
            base_score = 8.0
            
            # æ ¹æ®å¤æ‚åº¦è°ƒæ•´åˆ†æ•°
            overall_complexity = complexity_analysis.get("overall_complexity", {})
            complexity_score = overall_complexity.get("average_score", 5.0)
            
            # æ ¹æ®ç“¶é¢ˆæ•°é‡å’Œä¸¥é‡ç¨‹åº¦è°ƒæ•´åˆ†æ•°
            bottleneck_penalty = 0
            for bottleneck in bottlenecks:
                severity = bottleneck.get("severity", "low")
                if severity == "critical":
                    bottleneck_penalty += 2.0
                elif severity == "high":
                    bottleneck_penalty += 1.5
                elif severity == "medium":
                    bottleneck_penalty += 1.0
                elif severity == "low":
                    bottleneck_penalty += 0.5
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            final_score = max(0.0, min(10.0, base_score - bottleneck_penalty + (complexity_score - 5.0)))
            
            # AIç”Ÿæˆè¯„åˆ†è§£é‡Š
            scoring_explanation = await self._generate_scoring_explanation(
                final_score, complexity_analysis, bottlenecks
            )
            
            return {
                "performance_score": final_score,
                "performance_grade": self._score_to_grade(final_score),
                "complexity_contribution": complexity_score,
                "bottleneck_impact": bottleneck_penalty,
                "explanation": scoring_explanation,
                "improvement_potential": 10.0 - final_score
            }
            
        except Exception as e:
            return {"error": f"æ€§èƒ½è¯„åˆ†å¤±è´¥: {e}"}

    async def _ai_optimization_planning(self, code_content: str, bottlenecks: List[Dict[str, Any]], complexity_analysis: Dict[str, Any]) -> Dict[str, Any]:
        """AIç”Ÿæˆä¼˜åŒ–è®¡åˆ’"""
        try:
            optimization_plan = {
                "immediate_optimizations": [],
                "algorithmic_improvements": [],
                "architectural_changes": [],
                "monitoring_setup": [],
                "estimated_performance_gain": "unknown"
            }
            
            # åŸºäºç“¶é¢ˆç”Ÿæˆä¼˜åŒ–å»ºè®®
            for bottleneck in bottlenecks[:5]:  # å¤„ç†å‰5ä¸ªç“¶é¢ˆ
                optimization = await self._generate_bottleneck_optimization(bottleneck)
                
                severity = bottleneck.get("severity", "low")
                if severity in ["critical", "high"]:
                    optimization_plan["immediate_optimizations"].append(optimization)
                else:
                    optimization_plan["algorithmic_improvements"].append(optimization)
            
            # åŸºäºå¤æ‚åº¦åˆ†æç”Ÿæˆå»ºè®®
            complexity_optimizations = await self._generate_complexity_optimizations(complexity_analysis)
            optimization_plan["algorithmic_improvements"].extend(complexity_optimizations)
            
            # AIç”Ÿæˆæ¶æ„çº§ä¼˜åŒ–å»ºè®®
            if self.optimization_advisor:
                architectural_suggestions = await self._generate_architectural_optimizations(
                    code_content, bottlenecks
                )
                optimization_plan["architectural_changes"] = architectural_suggestions
            
            # ä¼°ç®—æ€§èƒ½æå‡
            optimization_plan["estimated_performance_gain"] = await self._estimate_performance_gain(
                bottlenecks, complexity_analysis
            )
            
            return optimization_plan
            
        except Exception as e:
            return {"error": f"ä¼˜åŒ–è®¡åˆ’ç”Ÿæˆå¤±è´¥: {e}"}

    async def _ai_testing_recommendations(self, code_structure: Dict[str, Any]) -> List[Dict[str, Any]]:
        """AIç”Ÿæˆæ€§èƒ½æµ‹è¯•å»ºè®®"""
        recommendations = []
        
        try:
            # åŸºäºä»£ç ç»“æ„ç”Ÿæˆæµ‹è¯•å»ºè®®
            if code_structure.get("database_operations"):
                recommendations.append({
                    "test_type": "æ•°æ®åº“æ€§èƒ½æµ‹è¯•",
                    "description": "æµ‹è¯•æ•°æ®åº“æŸ¥è¯¢æ€§èƒ½å’Œè¿æ¥æ± æ•ˆç‡",
                    "tools": ["pytest-benchmark", "SQLAlchemy profiling"],
                    "priority": "high"
                })
            
            if code_structure.get("network_operations"):
                recommendations.append({
                    "test_type": "ç½‘ç»œæ€§èƒ½æµ‹è¯•",
                    "description": "æµ‹è¯•ç½‘ç»œè¯·æ±‚å»¶è¿Ÿå’Œå¹¶å‘å¤„ç†èƒ½åŠ›",
                    "tools": ["aiohttp testing", "requests-mock"],
                    "priority": "medium"
                })
            
            if code_structure.get("async_patterns"):
                recommendations.append({
                    "test_type": "å¼‚æ­¥æ€§èƒ½æµ‹è¯•",
                    "description": "æµ‹è¯•å¼‚æ­¥æ“ä½œçš„å¹¶å‘æ€§èƒ½",
                    "tools": ["pytest-asyncio", "asyncio profiling"],
                    "priority": "high"
                })
            
            # AIç”Ÿæˆè‡ªå®šä¹‰æµ‹è¯•å»ºè®®
            if self.optimization_advisor:
                custom_tests = await self._generate_custom_performance_tests(code_structure)
                recommendations.extend(custom_tests)
            
        except Exception as e:
            recommendations.append({
                "test_type": "é”™è¯¯",
                "description": f"æµ‹è¯•å»ºè®®ç”Ÿæˆå¤±è´¥: {e}",
                "priority": "info"
            })
        
        return recommendations

    # è¾…åŠ©æ–¹æ³•
    def _extract_functions(self, code_content: str) -> List[str]:
        """æå–ä»£ç ä¸­çš„å‡½æ•°"""
        functions = []
        try:
            # ç®€å•çš„å‡½æ•°æå–,å®é™…åº”è¯¥ä½¿ç”¨AST
            lines = code_content.split('\n')
            current_function = []
            in_function = False
            indent_level = 0
            
            for line in lines:
                if line.strip().startswith('def '):
                    if current_function:
                        functions.append('\n'.join(current_function))
                    current_function = [line]
                    in_function = True
                    indent_level = len(line) - len(line.lstrip())
                elif in_function:
                    if line.strip() and len(line) - len(line.lstrip()) <= indent_level and not line.strip().startswith('#'):
                        if current_function:
                            functions.append('\n'.join(current_function))
                        current_function = []
                        in_function = False
                    else:
                        current_function.append(line)
            
            if current_function:
                functions.append('\n'.join(current_function))
                
        except Exception as e:
            log("ai_performance_agent", LogLevel.INFO, f"å‡½æ•°æå–å¤±è´¥: {e}")
        
        return functions[:10]  # é™åˆ¶å‡½æ•°æ•°é‡

    def _detect_nested_loops(self, code_content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹åµŒå¥—å¾ªç¯"""
        nested_loops = []
        lines = code_content.split('\n')
        
        for i, line in enumerate(lines):
            if 'for ' in line or 'while ' in line:
                # æ£€æŸ¥åç»­è¡Œä¸­æ˜¯å¦æœ‰åµŒå¥—å¾ªç¯
                indent = len(line) - len(line.lstrip())
                for j in range(i+1, min(i+20, len(lines))):
                    next_line = lines[j]
                    if next_line.strip() and len(next_line) - len(next_line.lstrip()) > indent:
                        if 'for ' in next_line or 'while ' in next_line:
                            nested_loops.append({
                                "line_number": i+1,
                                "outer_loop": line.strip(),
                                "inner_loop": next_line.strip(),
                                "nesting_level": 2  # ç®€åŒ–,åªæ£€æµ‹2å±‚
                            })
                            break
        
        return nested_loops

    def _detect_recursive_functions(self, code_content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹é€’å½’å‡½æ•°"""
        recursive_functions = []
        functions = self._extract_functions(code_content)
        
        for func in functions:
            lines = func.split('\n')
            func_name = None
            
            # æå–å‡½æ•°å
            for line in lines:
                if line.strip().startswith('def '):
                    func_name = line.split('def ')[1].split('(')[0].strip()
                    break
            
            # æ£€æŸ¥æ˜¯å¦è°ƒç”¨è‡ªèº«
            if func_name:
                for line in lines[1:]:  # è·³è¿‡å‡½æ•°å®šä¹‰è¡Œ
                    if func_name in line and '(' in line:
                        recursive_functions.append({
                            "function_name": func_name,
                            "function_code": func,
                            "recursive_call_line": line.strip()
                        })
                        break
        
        return recursive_functions

    def _detect_io_operations(self, code_content: str) -> List[Dict[str, Any]]:
        """æ£€æµ‹I/Oæ“ä½œ"""
        io_operations = []
        io_patterns = [
            ("file_io", ["open(", "read(", "write(", "close()"]),
            ("database_io", ["execute(", "query(", "select", "insert", "update"]),
            ("network_io", ["requests.", "urllib.", "socket.", "httpx."])
        ]
        
        lines = code_content.split('\n')
        for i, line in enumerate(lines):
            for io_type, patterns in io_patterns:
                for pattern in patterns:
                    if pattern.lower() in line.lower():
                        io_operations.append({
                            "line_number": i+1,
                            "io_type": io_type,
                            "operation": line.strip(),
                            "pattern_matched": pattern
                        })
                        break
        
        return io_operations

    def _score_to_grade(self, score: float) -> str:
        """å°†åˆ†æ•°è½¬æ¢ä¸ºç­‰çº§"""
        if score >= 9.0:
            return "Excellent"
        elif score >= 7.5:
            return "Good"
        elif score >= 6.0:
            return "Fair"
        elif score >= 4.0:
            return "Poor"
        else:
            return "Critical"

    # å ä½ç¬¦æ–¹æ³• - å®é™…å®ç°ä¸­éœ€è¦å®Œå–„
    async def _ai_structural_analysis(self, code_content):
        return {}
    
    async def _parse_complexity_result(self, result, code, index):
        if result and result[0].get("score", 0) > 0.6:
            return {
                "function_id": f"func_{index}",
                "estimated_complexity": "O(n)",
                "confidence": result[0]["score"]
            }
        return None
    
    async def _extract_complexity_details(self, analysis, code, index):
        return None
    
    async def _calculate_overall_complexity(self, results):
        return {"average_score": 6.5}
    
    def _analyze_complexity_distribution(self, results):
        return {"distribution": "normal"}
    
    def _prioritize_optimizations(self, results):
        return ["é«˜å¤æ‚åº¦å‡½æ•°ä¼˜åŒ–"]
    
    async def _ai_analyze_loop_bottleneck(self, loop_info):
        line_num = loop_info.get('line_number')
        outer_loop = loop_info.get('outer_loop', 'N/A')
        inner_loop = loop_info.get('inner_loop', 'N/A')
        nesting_level = loop_info.get('nesting_level', 2)
        
        description = (
            f"åµŒå¥—å¾ªç¯æ€§èƒ½ç“¶é¢ˆ (ç¬¬{line_num}è¡Œ)\n"
            f"  - åµŒå¥—å±‚çº§: {nesting_level}å±‚\n"
            f"  - å¤–å±‚å¾ªç¯: {outer_loop}\n"
            f"  - å†…å±‚å¾ªç¯: {inner_loop}\n"
            f"  - æ€§èƒ½å½±å“: æ—¶é—´å¤æ‚åº¦å¯èƒ½è¾¾åˆ° O(n^{nesting_level})\n"
            f"  - å»ºè®®: è€ƒè™‘ä½¿ç”¨ASTè§£æã€å­—å…¸æŸ¥æ‰¾æˆ–ç”Ÿæˆå™¨ä¼˜åŒ–"
        )
        
        return {
            "bottleneck_id": f"LOOP_{line_num}",
            "type": "nested_loop",
            "description": description,
            "line_number": line_num,
            "severity": "medium",
            "severity_score": 6.0,
            "details": {
                "outer_loop": outer_loop,
                "inner_loop": inner_loop,
                "nesting_level": nesting_level,
                "estimated_complexity": f"O(n^{nesting_level})"
            }
        }
    
    async def _ai_analyze_recursion_bottleneck(self, func_info):
        func_name = func_info.get('function_name', 'unknown')
        recursive_call = func_info.get('recursive_call_line', 'N/A')
        func_code_snippet = func_info.get('function_code', '')[:200]  # å‰200å­—ç¬¦
        
        # å°è¯•ä»å‡½æ•°ä»£ç ä¸­æ‰¾åˆ°å‡½æ•°å®šä¹‰è¡Œ
        lines = func_code_snippet.split('\n')
        func_line = None
        for i, line in enumerate(lines):
            if f'def {func_name}' in line:
                func_line = i + 1
                break
        
        description = (
            f"é€’å½’å‡½æ•°æ€§èƒ½é£é™© - {func_name}\n"
            f"  - å‡½æ•°åç§°: {func_name}\n"
            f"  - é€’å½’è°ƒç”¨: {recursive_call}\n"
            f"  - æ€§èƒ½å½±å“: å¯èƒ½å¯¼è‡´æ ˆæº¢å‡ºæˆ–æŒ‡æ•°çº§æ—¶é—´å¤æ‚åº¦\n"
            f"  - å»ºè®®: æ£€æŸ¥æ˜¯å¦æœ‰ç»ˆæ­¢æ¡ä»¶ã€è€ƒè™‘ä½¿ç”¨è¿­ä»£æˆ–å°¾é€’å½’ä¼˜åŒ–ã€æ·»åŠ ç¼“å­˜(memoization)"
        )
        
        return {
            "bottleneck_id": f"RECURSION_{func_name}",
            "type": "recursion",
            "description": description,
            "line_number": func_line,
            "severity": "medium",
            "severity_score": 5.0,
            "details": {
                "function_name": func_name,
                "recursive_call_line": recursive_call,
                "code_preview": func_code_snippet
            }
        }
    
    async def _ai_analyze_io_bottleneck(self, io_info):
        line_num = io_info.get('line_number')
        io_type = io_info.get('io_type', 'unknown')
        operation = io_info.get('operation', 'N/A')
        pattern = io_info.get('pattern_matched', '')
        
        # æ ¹æ®IOç±»å‹æä¾›ä¸åŒçš„ä¼˜åŒ–å»ºè®®
        io_type_descriptions = {
            "file_io": {
                "name": "æ–‡ä»¶I/Oæ“ä½œ",
                "impact": "å¯èƒ½å¯¼è‡´ç£ç›˜I/Oé˜»å¡",
                "suggestions": "ä½¿ç”¨å¼‚æ­¥æ–‡ä»¶æ“ä½œ(aiofiles)ã€æ‰¹é‡è¯»å†™ã€æ·»åŠ ç¼“å­˜æœºåˆ¶"
            },
            "database_io": {
                "name": "æ•°æ®åº“I/Oæ“ä½œ",
                "impact": "æ•°æ®åº“æŸ¥è¯¢å¯èƒ½æˆä¸ºæ€§èƒ½ç“¶é¢ˆ",
                "suggestions": "ä½¿ç”¨è¿æ¥æ± ã€æ‰¹é‡æŸ¥è¯¢ã€æ·»åŠ ç´¢å¼•ã€è€ƒè™‘ä½¿ç”¨ç¼“å­˜(Redis)ã€ä½¿ç”¨å¼‚æ­¥æ•°æ®åº“é©±åŠ¨"
            },
            "network_io": {
                "name": "ç½‘ç»œI/Oæ“ä½œ",
                "impact": "ç½‘ç»œå»¶è¿Ÿå¯èƒ½å½±å“å“åº”æ—¶é—´",
                "suggestions": "ä½¿ç”¨å¼‚æ­¥HTTPå®¢æˆ·ç«¯(aiohttp)ã€å®ç°è¶…æ—¶æ§åˆ¶ã€æ·»åŠ é‡è¯•æœºåˆ¶ã€è€ƒè™‘å¹¶å‘è¯·æ±‚"
            }
        }
        
        io_details = io_type_descriptions.get(io_type, {
            "name": "I/Oæ“ä½œ",
            "impact": "å¯èƒ½å½±å“æ€§èƒ½",
            "suggestions": "è€ƒè™‘ä½¿ç”¨å¼‚æ­¥æ“ä½œ"
        })
        
        description = (
            f"{io_details['name']} (ç¬¬{line_num}è¡Œ)\n"
            f"  - æ“ä½œä»£ç : {operation}\n"
            f"  - åŒ¹é…æ¨¡å¼: {pattern}\n"
            f"  - æ€§èƒ½å½±å“: {io_details['impact']}\n"
            f"  - ä¼˜åŒ–å»ºè®®: {io_details['suggestions']}"
        )
        
        return {
            "bottleneck_id": f"IO_{io_type.upper()}_{line_num}",
            "type": io_type,
            "description": description,
            "line_number": line_num,
            "severity": "low",
            "severity_score": 3.0,
            "details": {
                "io_type": io_type,
                "operation": operation,
                "pattern_matched": pattern,
                "optimization_priority": "medium" if io_type == "database_io" else "low"
            }
        }
    
    async def _ai_pattern_based_bottleneck_detection(self, code_content):
        return []
    
    async def _generate_scoring_explanation(self, score, complexity, bottlenecks):
        return f"åŸºäº{len(bottlenecks)}ä¸ªæ€§èƒ½ç“¶é¢ˆå’Œå¤æ‚åº¦åˆ†æçš„ç»¼åˆè¯„ä¼°"
    
    async def _generate_bottleneck_optimization(self, bottleneck):
        return {
            "optimization_id": bottleneck.get("bottleneck_id"),
            "description": f"ä¼˜åŒ–å»ºè®®:{bottleneck.get('description')}",
            "priority": bottleneck.get("severity")
        }
    
    async def _generate_complexity_optimizations(self, complexity_analysis):
        return []
    
    async def _generate_architectural_optimizations(self, code_content, bottlenecks):
        return []
    
    async def _estimate_performance_gain(self, bottlenecks, complexity_analysis):
        return "20-40%æå‡"
    
    async def _generate_custom_performance_tests(self, code_structure):
        return []

    def _get_current_time(self) -> str:
        """è·å–å½“å‰æ—¶é—´æˆ³"""
        import datetime
        return datetime.datetime.now().isoformat()

    async def _execute_task_impl(self, task_data: Dict[str, Any]) -> Dict[str, Any]:
        """æ‰§è¡ŒAIé©±åŠ¨çš„æ€§èƒ½åˆ†æä»»åŠ¡"""
        return await self._ai_driven_performance_analysis(
            task_data.get("code_content", ""),
            task_data.get("code_directory", "")
        )